schedule = {"vcalendar":[{"version":"2.0","prodid":"-//sebbo.net//ical-generator//EN","name":"ApacheCon 2020","x-wr-calname":"ApacheCon 2020","vevent":[{"uid":"acah2020-bigdata-1-T1615@apachecon.com","sequence":"1","dtstamp":"20200813T193041Z","dtstart":"20200929T161500Z","dtend":"20200929T165500Z","summary":"Apache Hadoop YARN: Past\\, Now and Future","location":"@home","x-alt-desc":"<strong>\\nSzilard Nemeth\\, Sunil Govindan\\n</strong>\\n<p>\\nApache Hadoop YARN is an integral part of on-premiss solutions and it will be for the foreseeable future. We also believe\\, it will have an important role in the Cloud as well allowing users to move their solutions both to private and public cloud as easily as possible. For this reason\\, the development of YARN took a new momentum in the last year. In this talk\\, we will talk about the latest updates from the community which will be released in Hadoop 3.3.x/3.4.x releases: For compute acceleration devices (including GPU/FPGA\\, etc.)\\, we will talk about better GPU support (including GPU hierarchy scheduling support\\, Nvidia-docker v2 support\\, etc.)\\; YARNs device plugin framework to allow developers easier add new compute-acceleration devices\\; FPGA support\\, etc. For scheduling related improvements\\, we will talk about new improvements of global scheduling in CapacityScheduler\\, new efforts to bring dynamic-queue-creation/absolute-resource into production\\, and one of the recent work\\, fs2cs\\, which allows user migrate from FairScheduler to CapacityScheduler. Apart from these\\, we will talk about new containerization improvements: runc support\\, improvements of log aggregation to better support cloud storage\\, etc. Audiences will get the latest development progress of Apache Hadoop YARN\\, and help them to make a decision when upgrading to Hadoop 3.x.\\n</p>\\n\\n<p><em>\\nSzilard Nemeth:<br />\\nSzilard Nemeth has many years of development experience mainly in Java. He joined Clouderas YARN team late 2017 and has been key to transfer YARN knowledge from Palo Alto to Budapest\\, Hungary. He has become a Hadoop committer in the Summer of 2019. Through out his career in YARN he mostly focused on Custom Resource Types\\, GPU support and recently he has been involved in making YARN more flexible and ready to the Cloud. He lives in Budapest but when has the time\\, he loves traveling.<br />\\nSunil Govindan:<br />\\nEngineer Manager @Cloudera. Contributing to Apache Hadoop project since 2013 in various roles as Hadoop Contributor\\, Hadoop Committer\\, and a member Project Management Committee (PMC). Majorly working on YARN Scheduling improvements / Multiple Resource types support in YARN etc. He also served as Apache Submarine PMC member\\, Apache YuniKorn (incubating) PMC member.\\n</em></p>","categories":"Big Data","url":"https://apachecon.com/acah2020/tracks/bigdata-1.html#T1615"},{"uid":"acah2020-bigdata-1-T1655@apachecon.com","sequence":"1","dtstamp":"20200813T193041Z","dtstart":"20200929T165500Z","dtend":"20200929T173500Z","summary":"Hadoop Storage Reloaded: the 5 lessons Ozone learned from HDFS","location":"@home","x-alt-desc":"<strong>\\nMrton Elek\\n</strong>\\n<p>\\nApache (Hadoop) Ozone is a brand-new storage system for the Hadoop ecosystem. It provides Object Store semantics (like S3) and can handle billions of objects. Ozone doesn't depend on HDFS but it's the \\\"spiritual successor\\\" of it. The lessons learned during the 10+ years of HDFS helped to design a more scalable object store. This presentation explains the key challenges of a storage system and shows how the specific problems can be answered.\\n</p>\\n\\n<p><em>\\nMarton Elek is PMC in Apache Hadoop and Apache Ratis projects and working on the Apache Hadoop Ozone at Cloudera. Ozone is a new Hadoop sub-project which provides an S3 compatible Object Store for Hadoop on top of a new generalized binary storage layer. He is also working on the containerization of Hadoop and creating different solutions to run Apache Big Data projects in Kubernetes and other could native environments.\\n</em></p>","categories":"Big Data","url":"https://apachecon.com/acah2020/tracks/bigdata-1.html#T1655"},{"uid":"acah2020-bigdata-1-T1735@apachecon.com","sequence":"2","dtstamp":"20200813T193041Z","dtstart":"20200929T173500Z","dtend":"20200929T181500Z","summary":"Building efficient and reliable data lakes with Apache Iceberg","location":"@home","x-alt-desc":"<strong>\\nAnton Okolnychyi\\, Vishwanath Lakkundi\\n</strong>\\n<p>\\nApache Iceberg is a table format that allows data engineers and data scientists to build reliable and efficient data lakes with features that are normally present only in data warehouses. This talk will be a deep dive into key design principles of Apache Iceberg that enable the following features on top of data lakes: - ACID compliance on top of any object store or distributed file system - Flexible indexing capabilities which boost the performance of highly selective queries - Implicit partitioning using partition transforms - Reliable schema evolution - Time travel and rollback These advanced features let companies substantially simplify their current architectures as well as enable new use cases on top of data lakes.\\n</p>\\n\\n<p><em>\\nAnton is a committer and PMC member of Apache Iceberg as well as an Apache Spark contributor at Apple. At Apple\\, he is working on making data lakes efficient and reliable. Prior to joining Apple\\, he optimized and extended a proprietary Spark distribution at SAP. Anton holds a Masters degree in Computer Science from RWTH Aachen University.\\n<br />\\nVishwanath Lakkundi is the engineering lead for the team that\\nfocuses on Data Orchestration and Data Lake at Apple. This team is\\nresponsible for development of an elastic fully managed Apache Spark as\\na service\\, a Data Lake engine based on Apache Iceberg and a data\\npipelines product based on Apache Airflow. He has been working with\\nApple since the last seven years focusing on various analytics\\ninfrastructure and platform products.\\n</em></p>","categories":"Big Data","url":"https://apachecon.com/acah2020/tracks/bigdata-1.html#T1735"},{"uid":"acah2020-bigdata-1-T1815@apachecon.com","sequence":"1","dtstamp":"20200813T193041Z","dtstart":"20200929T181500Z","dtend":"20200929T185500Z","summary":"Accelerating distributed joins in Apache Hive: Runtime filtering enhancements","location":"@home","x-alt-desc":"<strong>\\nPanagiotis Garefalakis\\, Stamatis Zampetakis\\n</strong>\\n<p>\\nApache Hive is an open-source relational database system that is widely adopted by several organizations for big data analytic workloads. It combines traditional MPP (massively parallel processing) techniques with more recent cloud computing concepts to achieve the increased scalability and high performance needed by modern data intensive applications. Even though it was originally tailored towards long running data warehousing queries\\, its architecture recently changed with the introduction of LLAP (Live Long and Process) layer. Instead of regular containers\\, LLAP utilizes long-running executors to exploit data sharing and caching possibilities within and across queries. Executors eliminate unnecessary disk IO overhead and thus reduce the latency of interactive BI (business intelligence) queries by orders of magnitude. However\\, as container startup cost and IO overhead is now minimized\\, the need to effectively utilize memory and CPU resources across long-running executors in the cluster is becoming increasingly essential. For instance\\, in a variety of production workloads\\, we noticed that the memory bandwidth of early decoding all table columns for every row\\, even when this row is dropped later on\\, is starting to overwhelm the performance of single query execution. In this talk\\, we focus on some of the optimizations we introduced in Hive 4.0 to increase CPU efficiency and save memory allocations. In particular\\, we describe the lazy decoding (or row-level filtering) and composite bloom-filters optimizations that greatly improve the performance of queries containing broadcast joins\\, reducing their runtime by up to 50%. Over several production and synthetic workloads\\, we show the benefit of the newly introduced optimizations as part of Clouderas cloud-native Data Warehouse engine. At the same time\\, the community can directly benefit from the presented features as are they 100% open-source!\\n</p>\\n\\n<p><em>\\nPanagiotis Garefalakis:<br />\\nPanagiotis Garefalakis is a Software Engineer at Cloudera where he is part of the Data Warehousing team. He holds a Ph.D. in Computer Science from Imperial College London were he was affiliated with the Large-Scale Data & Systems (LSDS) group. His interests lie within the broad area of systems including large-scale distributed systems\\, cluster resource management\\, and big data processing.<br />\\nStamatis Zampetakis:<br />\\nStamatis Zampetakis is a Software Engineer at Cloudera working on the Data Warehousing product. He holds a PhD in Big Data management on massively parallel systems\\n</em></p>","categories":"Big Data","url":"https://apachecon.com/acah2020/tracks/bigdata-1.html#T1815"},{"uid":"acah2020-bigdata-1-T1935@apachecon.com","sequence":"1","dtstamp":"20200813T193041Z","dtstart":"20200929T193500Z","dtend":"20200929T201500Z","summary":"Big Data File Format Cost Efficiency - Millions of Dollars Deal","location":"@home","x-alt-desc":"<strong>\\nXinli Shang\\, Juncheng Ma\\n</strong>\\n<p>\\nReducing the size of data at rest and in transit is critical to many organizations\\, not only because it can save the cost of storage but also can improve the IO usage and traffic volume in the network. We will present how to translate hundreds of petabytes data compressed in GZIP in the data lake to a more efficient compression method - ZSTD which can reduce the data size by 10% and save millions of dollars. We will show the recent Apache Parquet format improvement that makes ZSTD compression to be easily set up (PARQUET-1866). The tech talk will also demonstrate how we solve the challenges of the compression translation speed by improving the throughput by 5X (PARQUET-1872). As a result\\, the translation time of large scale data sets can be reduced from months to days and save compute vCores correspondingly. The translation needs to be in a safe way to prevent data corruption and incompatibility. We will also show the technicals that are built into the compression translation tool to prevent them from happening. Another significant storage size reduction (up to 80%) can be done by 1) reordering the columns in Parquet to make it more friendly to encoding and compression\\, 2) encoding with BYTE_STREAM_SPLIT (PARQUET-1622) that is more efficient for floating type data\\, 3) reducing geolocation data precision to make RLE more efficient\\, 4) pruning unused columns (PARQUET-1800). We will show above every technique\\, the effectiveness of each and the reason behind it. We will also show the tools like Parquet column-size (PARQUET-1821) that can help users to identify the candidate tables to apply the above techniques.\\n</p>\\n\\n<p><em>\\nXinli Shang:<br />\\nXinli Shang is a tech lead on the Uber Data Infra team\\, Apache Parquet Committer. He is passionate about big data file format for efficiency\\, performance and security\\, tuning large scale services for performance\\, throughput\\, and reliability. He is an active contributor to Apache Parquet. He also has many years of experience developing large scale distributed systems like S3 Index\\, and operating system Windows.<br />\\nJuncheng Ma :<br />\\nSoftware Developer at Uber\\n</em></p>","categories":"Big Data","url":"https://apachecon.com/acah2020/tracks/bigdata-1.html#T1935"},{"uid":"acah2020-bigdata-1-R0900@apachecon.com","sequence":"1","dtstamp":"20200828T190237Z","dtstart":"20200930T090000Z","dtend":"20200930T094000Z","summary":"Integrate Apache Flink with Cloud Native Ecosystem","location":"@home","x-alt-desc":"<strong>\\nYang WangTao Yang\\n</strong>\\n<p>\\nWith the vigorous development of cloud-native and serverless computing\\, there are more and more big-data workloads\\, especially Apache Flink workloads\\, running in Alibaba cloud for better deployment and management\\, backed by Kubernetes. This presentation introduces the experiences of intergrating Flink with cloud-native ecosystem\\, including the improvements in Flink to support elasticity and natively running on Kubernetes\\, the experiences about managing dependent components like ZooKeeper\\, HDFS etc. and leveraging Kubernetes service/network/storage extensions\\, better supports for big-data workloads to satisfy requirements on multi-tanent management\\, resource fairness\\, resource elasticity etc. and achieve high scheduling performance via Apache YuniKorn.\\n</p>\\n\\n<p><em>\\nYang Wang:<br />\\nTechnical Expert of Alibaba's realtime computing team\\, Apache Flink Contributor\\, focusing on the direction of resource scheduling in Flink.<br />\\nTao Yang:<br />\\nTechnical Expert of Alibaba's realtime computing team\\, Apache Hadoop Committer\\, Apache YuniKorn Committer\\, focusing on the direction of resource scheduling in YARN and Kubernetes.\\n</em></p>","categories":"Big Data","url":"https://apachecon.com/acah2020/tracks/bigdata-1.html#R0900"},{"uid":"acah2020-bigdata-1-W1615@apachecon.com","sequence":"1","dtstamp":"20200813T193041Z","dtstart":"20200930T161500Z","dtend":"20200930T165500Z","summary":"Next Gen Data Lakes using Apache Hudi","location":"@home","x-alt-desc":"<strong>\\nBalaji Varadarajan\\, Sivabalan Narayanan\\n</strong>\\n<p>\\nData Lakes are one of the fastest growing trends in managing big data across various industries. Data Lakes offer massively scalable data processing over vast amounts of data. One of the main challenges that companies face in building a data lake is designing the right primitives for organizing their data. Apache Hudi helps implement uniform\\, best-of-breed data lake standards and primitives. With such primitives in place\\, next generation data lake would be about efficiency and intelligence. Businesses expect their data lake installations to cater to their ever changing needs while being cost efficient. In this talk\\, we will discuss new features in Apache Hudi that is catered towards building next-gen data-lake. We will start with basic Apache Hudi primitives such as upsert & delete required to achieve acceptable latencies in ingestion while at the same time providing high quality data by enforcing schematization on datasets. We will look into the novel record level index supported by Apache Hudi and how it supports efficient upserts. We will then dive into how Apache Hudi supports query optimization by leveraging its rich metadata. Efficient storage management is a key requirement for large data lake installation. We will look at how Apache Hudi supports intelligent and dynamic re-clustering of data for better storage management and faster query times. Finally\\, we will discuss how to easily onboard your existing dataset to Apache Hudi format\\, so you can leverage Apache Hudi efficiency without making any drastic changes to your existing data lake.\\n</p>\\n\\n<p><em>\\nBalaji Varadarajan:<br />\\nBalaji Varadarajan is currently a Staff Engineer in Robinhood's data platform team. Previously he was a tech lead in Uber data platform working on Apache Hudi and Hadoop platform at large. Previously\\, he was one of the lead engineers in LinkedIns databus change capture system. Balajis interests lie in large-scale distributed data systems.<br />\\nSivabalan Narayanan:<br />\\nSivabalan Narayanan is a senior software engineer at Uber overseeing data engineering broadly across the network performance monitoring domain. He is an active contributor to Apache Hudi and also big data enthusiasist whose interest lies in building data lake technologies. Previously\\, he was one of the core engineers responsible for builiding Linkedin's blob store.\\n</em></p>","categories":"Big Data","url":"https://apachecon.com/acah2020/tracks/bigdata-1.html#W1615"},{"uid":"acah2020-bigdata-1-W1655@apachecon.com","sequence":"1","dtstamp":"20200813T193041Z","dtstart":"20200930T165500Z","dtend":"20200930T173500Z","summary":"A Production Quality Sketching Library for the Analysis of Big Data","location":"@home","x-alt-desc":"<strong>\\nLee Rhodes\\n</strong>\\n<p>\\nIn the analysis of big data there are often problem queries that dont scale because they require huge compute resources to generate exact results\\, or dont parallelize well. Examples include count-distinct\\, quantiles\\, most frequent items\\, joins\\, matrix computations\\, and graph analysis. Algorithms that can produce accuracy guaranteed approximate answers for these problem queries are a required toolkit for modern analysis systems that need to process massive amounts of data quickly. For interactive queries there may not be other viable alternatives\\, and in the case of real-time streams\\, these specialized algorithms\\, called stochastic\\, streaming\\, sublinear algorithms\\, or 'sketches'\\, are the only known solution. This technology has helped Yahoo successfully reduce data processing times from days to hours or minutes on a number of its internal platforms and has enabled subsecond queries on real-time platforms that would have been infeasible without sketches. This talk provides an introduction to sketching and to Apache DataSketches\\, an open source library in C++\\, Java and Python of algorithms designed for large production analysis systems.\\n</p>\\n\\n<p><em>\\nLee Rhodes is a Distinguished Architect at Yahoo (now Verizon Media). He created the DataSketches project in 2012 to address analysis problems in Yahoo's large data processing pipelines. DataSketches was Open Sourced in 2015 and is in incubation at Apache Software Foundation. He was an author or coauthor on sketching work published in ICDT\\, IMC\\, and JCGS. He obtained his Master's Degree in Electrical Engineering from Stanford University.\\n</em></p>","categories":"Big Data","url":"https://apachecon.com/acah2020/tracks/bigdata-1.html#W1655"},{"uid":"acah2020-bigdata-1-W1735@apachecon.com","sequence":"1","dtstamp":"20200813T193041Z","dtstart":"20200930T173500Z","dtend":"20200930T181500Z","summary":"Cylon - Fast Scalable Data Engineering","location":"@home","x-alt-desc":"<strong>\\nSupun Kamburugamuve\\, Niranda Perera\\n</strong>\\n<p>\\nMachine learning (ML) and deep learning (DL) fields have made amazing progress in the past few years. Modern ML/DL applications have outgrown resource requirements beyond a single node's capability. However\\, this is just a small part of the issues facing the overall data processing environment\\, which must also support a raft of big data engineering for pre- and post-data processing\\, communication\\, and system integration. The big data tools surrounding the ML/DL applications need to be able to easily integrate with existing ML/DL frameworks in a multitude of languages\\, which particularly increases user productivity and efficiency. All this demands an efficient and highly distributed integrated approach for data processing\\, yet many of today's popular data analytics tools are unable to satisfy all these requirements at the same time. This presentation introduces Cylon\\, an open-source high performance distributed data processing library that can be seamlessly integrated with the existing Big Data and AI/ML frameworks. It is developed with a flexible C++ core on top of the Apache Arrow data format and exposes language bindings to C++\\, Java\\, and Python. The presentation discusses Cylon's architecture in detail and reveals how it can be imported as a library to existing applications or operate as a standalone framework. Initial experiments show that Cylon outperforms popular tools such as Apache Spark and Dask with major performance improvements for key operations with the ability to integrate with them. Finally\\, we show how Cylon can enable high-performance data pre-processing in popular AI tools such as Pytorch\\, Tensorflow\\, and Jupyter notebook without taking away Data scientists productivity.\\n</p>\\n\\n<p><em>\\nSupun Kamburugamuve:<br />\\nSupun Kamburugamuve has a Ph.D. in computer science specializing in high-performance data analytics. He is working in the role of a principal software engineer at the Digital Science Center of Indiana University where he leads Twister2 and Cylon high-performance data analytics projects. Supun is an elected member of the Apache Software Foundation and has contributed to many open-source projects including Apache Web Services projects and Apache Heron. Before joining Indiana University\\, Supun worked on middleware systems and was a key member of the WSO2 ESB project\\, which is an open-source enterprise integration solution being widely used by enterprises. Supun has given many technical talks at research conferences and technical conferences including Strata NY\\, Big Data Conference\\, and Apache Con.<br />\\nNiranda Perera:<br />\\nNiranda Perera is a second-year grad student at Indiana University - Luddy School of Informatics\\, Computing\\, and Engineering (SICE). He is enrolled in the Intelligent Systems Engineering Department and advised by Prof. Geoffery Fox. He is an active contributor to the Twister2 project and a founding member of the Cylon high-performance data analytics project. He completed his Bachelor's at the University of Moratuwa\\, Sri Lanka\\, and prior to joining IU\\, he was a contributor to WSO2 Data Analytics Server\\, which was a part of WSO2 middleware stack.\\n</em></p>","categories":"Big Data","url":"https://apachecon.com/acah2020/tracks/bigdata-1.html#W1735"},{"uid":"acah2020-bigdata-1-W1815@apachecon.com","sequence":"1","dtstamp":"20200813T193041Z","dtstart":"20200930T181500Z","dtend":"20200930T185500Z","summary":"Snakes on a Plane: Interactive Data Exploration with PyFlink and Zeppelin Notebooks","location":"@home","x-alt-desc":"<strong>\\nMarta Paes\\n</strong>\\n<p>\\nStream processing has fundamentally changed the way we build and think about data pipelines  but the technologies that unlock the value of this powerful paradigm havent always been friendly to non-Java/Scala developers. Apache Flink has recently introduced PyFlink\\, allowing developers to tap into streaming data in real-time with the flexibility of Python and its wide ecosystem for data analytics and Machine Learning. In this talk\\, we will explore the basics of PyFlink and showcase how developers can make use of a simple tool like interactive notebooks (Apache Zeppelin) to unleash the full power of an advanced stream processor like Flink.\\n</p>\\n\\n<p><em>\\nMarta is a Developer Advocate at Ververica (formerly data Artisans) and a contributor to Apache Flink. After finding her mojo in open source\\, she is committed to making sense of Data Engineering through the eyes of those using its by-products. Marta holds a Masters in Biomedical Engineering\\, where she developed a particular taste for multi-dimensional data visualization\\, and previously worked as a Data Warehouse Engineer at Zalando and Accenture.\\n</em></p>","categories":"Big Data","url":"https://apachecon.com/acah2020/tracks/bigdata-1.html#W1815"},{"uid":"acah2020-bigdata-1-W1855@apachecon.com","sequence":"1","dtstamp":"20200813T193041Z","dtstart":"20200930T185500Z","dtend":"20200930T193500Z","summary":"Unified Data Processing with Apache Spark and Apache Pulsar","location":"@home","x-alt-desc":"<strong>\\nJia Zhai\\n</strong>\\n<p>\\nLambda is widely used in the industry when people need to process both real-time and historical data to get a result. It is effective\\, and a good balance of speed and reliability. But there are still challenges to use Lambda in the practice. The biggest detraction has been the need to maintain two distinct (and possibly complex) systems to generate both batch and streaming layers. Thus\\, the operational cost of maintaining multiple clusters is nontrivial\\, and in some cases\\, one business logic would have to be split into many segments across different places\\, which is a challenge to maintain as the business grows and it also increases communication overhead. In this session\\, we'd like to present a unique data processing architecture with Apache Spark and Apache Pulsar\\, a solution\\, with the core idea of \\\"One data storage\\, one computing engine\\, and one API\\\"\\, to solve the problems of Lambda architecture.\\n</p>\\n\\n<p><em>\\nJia Zhai is the co-founder of StreamNative\\, as well as PMC member of both Apache Pulsar and Apache BookKeeper\\, and contributes to these two projects continually.\\n</em></p>","categories":"Big Data","url":"https://apachecon.com/acah2020/tracks/bigdata-1.html#W1855"},{"uid":"acah2020-bigdata-1-W1935@apachecon.com","sequence":"1","dtstamp":"20200813T193041Z","dtstart":"20200930T193500Z","dtend":"20200930T201500Z","summary":"Cluster Management in Apache Ecosystem & Kubernetes","location":"@home","x-alt-desc":"<strong>\\nShekhar Prasad Rajak\\n</strong>\\n<p>\\nApache have powerful cluster & resource manager already\\, so do we really need to use Kubernetes for the deployment while using Apache projects ? Let's find out what type of cluster management system Apache already have \\,How cluster management works in each of below cases and when we don't need any other cluster management top of it and when we can leverage the power of both this apache cluster modes and Kubernetes in resource & cluster management. * Apache Spark Standalone: A simple cluster manager available as part of the Spark distribution.. * Apache Mesos: A general purpose distributed OS level push based scheduler & resource manager. * Apache Hadoop YARN: A distributed computing framework for monolithic job scheduling and cluster resource management for Hadoop cluster (Apache/CDH/HDP) We will see some benchmarks and features that kubernetes can provide but it is not present(or not mature enough) in the Apache ecosystem\\, but still using\\, one or both can improve the performance. We will deep dive into fundamentals of Kubernetes and Apache distribution\\, resource & cluster management system\\, Job scheduling\\, to get clear cut idea behind both ecosystems and why they are best in particular cases like Big Data\\, Machine Learning\\, Load balancer\\, and so on. Applications are containerised in Kubernetes Pod\\, Kubernetes Service is used as Load balancer\\, Kubernetes High availability is because of distribution of Pods in worker nodes\\, Local Storage\\, Persistent volume & Networking and many other features will be compared side by side with Apache Ecosystem. Like in Mesos\\, Application Group models dependencies as a tree of groups and Components are started in dependency order\\, Mesos-DNS works as basic load balancer\\, applications distribution among slave nodes\\, two-level scheduling mechanism\\, modern kernel \\\"cgroups\\\" in Linux & \\\"zones\\\" in Solaris\\, and so on. Along with the comparison & benchmark the talk will provide practical guide to use the Apache project with Kubernetes. Audience will understand the Software System design and generic problems of processing the request through the cluster & resource managers and why it is important to have modular\\, micro service based\\, loosely coupled software design\\, so that it can easily go through the container or OS level cluster management systems. This talk is clearly not to show who is winning but how can you win in your time\\, in the dark situation.\\n</p>\\n\\n<p><em>\\nShekhar is passionate about Open Source Softwares and active in various Open Source Projects. During college days he has contributed SymPy - Python library for symbolic mathematics \\, Data Science related Ruby gems like: daru\\, dart-view(Author)\\, nyaplot - which is under Ruby Science Foundation (SciRuby)\\, Bundler: a gem to bundle gems\\, NumPy & SciPy for creating the interactive website and documentation website using sphinx and Hugo framework\\, CloudCV for migrating the Angular JS application to Angular 8\\, and few others. He has successfully completed Google Summer of Code 2016 & 2017 and mentored students after that on 2018\\, 2019. Shekhar also talked about daru-view gem in RubyConf India 2018 and PyCon India 2017 on SymPy & SymEngine.\\n</em></p>","categories":"Big Data","url":"https://apachecon.com/acah2020/tracks/bigdata-1.html#W1935"},{"uid":"acah2020-bigdata-1-R1615@apachecon.com","sequence":"1","dtstamp":"20200828T190237Z","dtstart":"20201001T161500Z","dtend":"20201001T165500Z","summary":"Column encryption & Data Masking in Parquet - Protecting data at the lowest layer","location":"@home","x-alt-desc":"<strong>\\nPavi Subenderan\\, Xinli Shang\\n</strong>\\n<p>\\nIn a typical big dataset\\, only a minority of columns are actually sensitive and need to be protected. Columnar file formats like Apache Parquet allow for column level access control through encryption. This means the small number of sensitive columns in a dataset can be protected through encryption\\, while the non-sensitive columns can be open for access. Data masking features for encrypted columns bring further convenience and allows users to leverage encrypted columns even without access to them. The combination of column encryption and data masking maximizes accessibility to your data without compromising the security of sensitive data. In the first half\\, we will go over column encryption design and features in Parquet. We will cover considerations when operating parquet column encryption in production like Key Management Service\\, performance tradeoffs\\, encryption algorithm choice\\, etc. In the second half\\, we will cover the new Data Masking features in Parquet. There will be discussion about motivation behind data masking\\, the security implications of masking and implementation. Finally we will look at the tradeoffs between the different types of masks and the limitations of each type in terms of compression ratio\\, table joins\\, usability and administration overhead.\\n</p>\\n\\n<p><em>\\nPavi Subenderan:<br />\\nPavi is a Software Engineer on Uber's Data Infra team. His focus is on data security\\, privacy and open source big data technologies. He has been working on Parquet column encryption for 1.5 years and more recently on data masking.<br />\\nXinli Shang:<br />\\nXinli Shang is a tech lead on the Uber Data Infra team\\, Apache Parquet Committer. He is passionate about big data file format for efficiency\\, performance and security\\, tuning large scale services for performance\\, throughput\\, and reliability. He is an active contributor to Apache Parquet. He also has many years of experience developing large scale distributed systems like S3 Index\\, and operating system Windows.\\n</em></p>","categories":"Big Data","url":"https://apachecon.com/acah2020/tracks/bigdata-1.html#R1615"},{"uid":"acah2020-bigdata-1-R1655@apachecon.com","sequence":"1","dtstamp":"20200828T190237Z","dtstart":"20201001T165500Z","dtend":"20201001T173500Z","summary":"GDPRs Right to be Forgotten in Apache Hadoop Ozone","location":"@home","x-alt-desc":"<strong>\\nDinesh Chitlangia\\n</strong>\\n<p>\\nApache Hadoop Ozone is a robust\\, distributed key-value object store for Hadoop with layered architecture and strong consistency. It isolates the namespace management from the block and node management layer\\, which allows users to independently scale on both axes. Ozone is interoperable with the Hadoop ecosystem as it provides OzoneFS (Hadoop compatible file system API)\\, data locality and plug-n-play deployment with HDFS as it can be installed in an existing Hadoop cluster and can share storage disks with HDFS. Ozone solves the scalability challenges with HDFS by being size agnostic. Consequently\\, it allows users to store trillions of files in Ozone and access them as if they are on HDFS. Ozone plugs into existing Hadoop deployments seamlessly\\, and programs like Yarn\\, MapReduce\\, Spark\\, Hive and work without any modifications. In the era of increasing need for data privacy and regulations\\, Ozone also provides built-in support for GDPR compliance with strong focus on Right to be Forgotten i.e.\\, Data Erasure. At the end of this presentation the audience will be able to understand: 1. HDFS scalability challenges 2. Ozones Architecture as a solution 3. Overview of GDPR 4. GDPR implementation in Ozone\\n</p>\\n\\n<p><em>\\nDinesh is a Software Engineer with strong expertise in Java\\, Distributed Systems for ~10 years. He has been involved with Hadoop ecosystem for the last 4 years and is an Apache Hadoop Committer. Dinesh is currently working at Cloudera\\, performing the role of a proactive support consultant for Premier customers and enjoys contributing to Open Source community. Outside of technology\\, Dinesh has a serious hobby in Landscape/Cityscape photography.\\n</em></p>","categories":"Big Data","url":"https://apachecon.com/acah2020/tracks/bigdata-1.html#R1655"},{"uid":"acah2020-bigdata-1-R1735@apachecon.com","sequence":"1","dtstamp":"20200828T190237Z","dtstart":"20201001T173500Z","dtend":"20201001T181500Z","summary":"Global File System View Across all Hadoop Compatible File Systems with the LightWeight Client Side Mount Points.","location":"@home","x-alt-desc":"<strong>\\nUma Maheswara Rao Gangumalla\\n</strong>\\n<p>\\nApache Hadoop File System layer has integrations to many popular storage systems including cloud storages like S3\\, Azure Data Lake Storage etc\\, along with in-house Apache Hadoop Distributed File System. When users want to migrate data between file systems\\, its very difficult for them to update their meta storages when they persist file system paths with schemes. For Example the Apache Hive persists the URI paths in meta-store. In Apache Hadoop\\, we came up with a solution(HDFS-15289) for this problem\\, i.e\\, the View FileSystem Overload Scheme with the configurable scheme and mount points. In this talk\\, we will cover in details\\, how users can enable it and how easily users can migrate data between file systems without modifying their meta-store. Its completely transparent to users with respective to the file paths. We will present one of the use cases with Apache Hive partitioning\\, that is the user can move one/some of their partition data to a remote file system and just add a mount point on the default file system(ex: HDFS) where the user was working with. Here Hive queries will work transparently from the user point of view even though the data resides in a remote storage cluster ex: Apache Hadoop Ozone or S3. This will be very useful when users want to move certain kinds of data\\, ex: Cold Partitions\\, Small Files can be moved to remote clusters from a primary HDFS cluster without affecting applications. The Mount tables are maintained at the central server\\, all clients will load the tables while initializing the file system and also can refresh on modification of mount points. So\\, that all the initializing clients will be in sync. This will make users life easier to migrate data between cloud and on-premise storages in a much flexible way.\\n</p>\\n\\n<p><em>\\nUma Maheswara Rao Gangumalla is an Apache Software Foundation Member[1]. An Apache Hadoop\\, BookKeeper\\, Incubator committer and a member of the Project Management Committee[2]\\, and a long-term active contributor to the Apache Hadoop project. He is also mentoring several incubator projects at Apache. Uma holds a bachelor's degree in Electronics and Communications Engineering. He has more than 13 years of experience in large scale Distributed Software Platforms design and development. Currently\\, Uma is working as a Principal Software Engineer at Cloudera\\, Inc\\, California\\, and primarily focuses on open source big data technologies. Prior to this\\, Uma worked as a Software Architect in Intel Corporation\\, California. [1] https://www.apache.org/foundation/members.html [2] http://people.apache.org/phonebook.html?uid=umamahesh\\n</em></p>","categories":"Big Data","url":"https://apachecon.com/acah2020/tracks/bigdata-1.html#R1735"},{"uid":"acah2020-bigdata-1-R1815@apachecon.com","sequence":"2","dtstamp":"20200828T190237Z","dtstart":"20201001T181500Z","dtend":"20201001T185500Z","summary":"Apache Hadoop YARN fs2cs: Converting Fair Scheduler to Capacity Scheduler","location":"@home","x-alt-desc":"<strong>\\nBenjamin Teke\\n</strong>\\n<p>\\nApache Hadoop YARN has two popular schedulers\\, Fair Scheduler and Capacity Scheduler. Although the two are based on different principles\\, convergent evolution pushed them to be similar both in functionality and the feature set they offer. By now it seems to be a good idea to merge the two or chose one over the other so the entire user base can enjoy one unified support effort and knowledge base. In this talk\\, we will present our approach which is offering users a way to migrate from Fair Scheduler to Capacity Scheduler by exploring migration paths and filling feature parity gaps. We will also talk about challenges and those aspects of the migration need some engineering effort in order to keep the achievements of fine-tuning Fair Scheduler installations over many years. We will explain why Capacity Scheduler is our scheduler of choice\\, the way we analyzed differences between the two schedulers\\, how we found migration paths\\, and finally\\, we will present a tool (fs2cs) we developed to help users automate the process.\\n</p>\\n\\n<p><em>\\nBenjamin is a senior software developer with many years of experience in the presentation of bigdata for the telecom industry (mainly Kafka and HBase). Since early 2020\\, he has been an integral part of the YARN team at Cloudera. He gained general knowledge in YARN\\, and recently he started to specialise in Schedulers. He lives in Budapest and besides his interest in photography and cars\\, he is passionately automatizing his home via IoT.\\n</em></p>","categories":"Big Data","url":"https://apachecon.com/acah2020/tracks/bigdata-1.html#R1815"},{"uid":"acah2020-bigdata-1-R1855@apachecon.com","sequence":"1","dtstamp":"20200828T190237Z","dtstart":"20201001T185500Z","dtend":"20201001T193500Z","summary":"HDFS Migration from 2.7 to 3.3 and enabling Router Based Federation (RBF) in production","location":"@home","x-alt-desc":"<strong>\\nAkira Ajisaka\\n</strong>\\n<p>\\nIn a production HDFS cluster in Yahoo! JAPAN\\, the namespace has become too large and it won't fit in a single NameNode in the near future. Therefore we want to split the big namespace into some small one and use federation. There is an existing ViewFS solution but the clients need to add the mount table in their configs when using the ViewFS. Our clusters have too many clients\\, so we want to minimize the change of client configs. RBF is a new solution. In contrast to ViewFS\\, the Router manages the mount table\\, and clients don't have to set the mount table explicitly. In this talk\\, I will introduce the internals of RBF and how we configured the mount tables to load-balance among namespaces in production. RBF has some limitations. For example\\, rename (mv) operation is not allowed between different namespaces. I will talk about how we work around the limitations. In addition\\, the developers are going to eliminate some of the limitations in the community. I'll introduce the progressions as well. Next\\, I will introduce the improvements of recent HDFS. For example\\, multiple standby NameNodes\\, observer NameNodes\\, and DataNode maintenance mode are features that will greatly reduce the operation cost. I'm going to introduce them and how to enable those features. Upgrading from 2.7 to 3.3 is a big jump and we hit many incompatible changes. For the administrators who are going to upgrade their HDFS clusters\\, I would like to introduce the differences as many as possible.\\n</p>\\n\\n<p><em>\\nAkira Ajisaka develops and validates some new features of Apache Hadoop such as HDFS Router Based Federation for our use. Also\\, he troubleshoots and improves management/operations in our Hadoop clusters. He maintenances Apache Hadoop to improve its quality as an Apache Hadoop/Yetus committer and PMC member.\\n</em></p>","categories":"Big Data","url":"https://apachecon.com/acah2020/tracks/bigdata-1.html#R1855"},{"uid":"acah2020-bigdata-1-R1935@apachecon.com","sequence":"1","dtstamp":"20200828T190237Z","dtstart":"20201001T193500Z","dtend":"20201001T201500Z","summary":"Apache Beam: using cross-language pipeline to execute Python code from Java SDK","location":"@home","x-alt-desc":"<strong>\\nAlexey Romanenko\\n</strong>\\n<p>\\nThere are many reasons why we would need to execute Python code in Java data processing pipelines (and vice versa) - e.g. Machine Learning libraries\\, IO connectors\\, users Python code - and several different ways to do that. With the End of Life of Python 2 started this year\\, its getting more challenging since not all old solutions still work well for Python 3. One of the potential options for this could be using a cross-language pipeline and Portable Runner in Apache Beam. In this talk Im going to talk about what the cross-language pipeline in Beam is\\, how to create a mixed Java/Python pipeline\\, how to set up and run it\\, what kind of requirements and pitfalls we can expect in this case. Also\\, Ill show a demo of a use case where we need to execute a custom users Python 3 code in the middle of Java SDK pipeline and run it with Portable Spark Runner.\\n</p>\\n\\n<p><em>\\nAlexey Romanenko is Principal Software Engineer at Talend France\\, with more than 18 years of experience in software development. During his career\\, he has been working on very different projects\\, like high-load web services\\, web search engine and cloud storages. He is Apache Beam PMC member and committer\\, he contributed to different Beam IO components and Spark Runner.\\n</em></p>","categories":"Big Data","url":"https://apachecon.com/acah2020/tracks/bigdata-1.html#R1935"},{"uid":"acah2020-bigdata-2-T1615@apachecon.com","sequence":"1","dtstamp":"20200828T190237Z","dtstart":"20200929T161500Z","dtend":"20200929T165500Z","summary":"Spark and Iceberg at Apple's Scale - Leveraging differential files for efficient upserts and deletes","location":"@home","x-alt-desc":"<strong>\\nAnton Okolnychyi\\, Vishwanath Lakkundi\\n</strong>\\n<p>\\nApple leverages Apache Spark for processing large datasets to power key components of Apples production services. As users begin to use Apache Spark in a bigger range of data processing scenarios\\, it is essential to support efficient and transactional update/delete/merge operations even in read-mostly data lake environments. For example\\, such functionality is required to implement change data capture\\, support some forms of slowly changing dimensions in data warehousing\\, fix corrupted records without rewriting complete partitions. The original implementation of update/delete/merge operations in Apple's internal version of Apache Spark relied on snapshot isolation in Apache Iceberg and rewriting complete files if at least one record had to be changed. This approach performs well if we can limit the scope of updates/deletes to a small number of files using indexing. However\\, modifying a couple of records in a large number of files is still expensive as all unmodified records in touched files have to be copied over. Therefore\\, Apple collaborates with other members of the Apache Iceberg and Apache Spark communities on a way to leverage differential files\\, an efficient method for storing large and volatile databases\\, for update/delete/merge operations. This approach allows us to reduce write amplification\\, support online updates to data warehouses and sustain more concurrent operations on the same table. This talk will briefly describe common ways to implement updates in analytical databases\\, challenges between providing updates and optimizing data structures for reading\\, outline the proposed solution alongside its benefits and drawbacks.\\n</p>\\n\\n<p><em>\\nAnton is a committer and PMC member of Apache Iceberg as well as an Apache Spark contributor at Apple. He has been dealing with internals of various Big Data systems for the last 5 years. At Apple\\, Anton is working on data lakes and an elastic\\, on-demand\\, secure\\, and fully managed Spark as a service. Prior to joining Apple\\, he optimized and extended a proprietary Spark distribution at SAP. Anton holds a Masters degree in Computer Science from RWTH Aachen University.\\n<br />\\nVishwanath Lakkundi is the engineering lead for the team that\\nfocuses on Data Orchestration and Data Lake at Apple. This team is\\nresponsible for development of an elastic fully managed Apache Spark as\\na service\\, a Data Lake engine based on Apache Iceberg and a data\\npipelines product based on Apache Airflow. He has been working with\\nApple since the lastseven years focusing on various analytics\\ninfrastructure and platform products.\\n</em></p>","categories":"Big Data Track (2)","url":"https://apachecon.com/acah2020/tracks/bigdata-2.html#T1615"},{"uid":"acah2020-bigdata-2-T1655@apachecon.com","sequence":"0","dtstamp":"20200828T190237Z","dtstart":"20200929T165500Z","dtend":"20200929T173500Z","summary":"Presentation of DLab toolset","location":"@home","x-alt-desc":"<strong>\\nMykola Bodnar\\, Vira Vitanska\\, Oleg Fuks\\n</strong>\\n<p>\\nWe are going to introduce DLab: - Similar user experience across AWS\\, GCP\\, and Azure clouds - Automatically configurable exploratory environment integrated with enterprise security and templates - Project level collaboration environment across multiple clouds - Aggregated billing with cost comparison across cloud providers - Project level resource management \\n</p>\\n\\n<p><em>\\nMykola Bodnar:<br />\\nMykola Bodnar has worked in EPAM since 2019\\, primary skill DevOps.BigData\\;<br />\\nVira Vitanska:<br />\\nVira has worked in EPAM since 2017 as a functional tester.<br />\\nOleg Fuks<br />\\nOleg Fuks has been working in EPAM since 2019\\, primary skill is Java.\\n</em></p>","categories":"Big Data Track (2)","url":"https://apachecon.com/acah2020/tracks/bigdata-2.html#T1655"},{"uid":"acah2020-bigdata-2-T1735@apachecon.com","sequence":"0","dtstamp":"20200828T190237Z","dtstart":"20200929T173500Z","dtend":"20200929T181500Z","summary":"Efficient Spark Scheduling on K8s with Apache YuniKorn","location":"@home","x-alt-desc":"<strong>\\nWeiwei Yang\\, Gao Li\\n</strong>\\n<p>\\nApache Yunicorn (Incubating) is a new open-source project\\, which is a standalone resource scheduler of container orchestration platforms. Currently\\, it provides a fully functional resource scheduler alternative for K8s that manages and schedules Big Data workloads. We embrace Apache Spark for data engineering and machine learning\\, and by running Spark on K8s\\, we are able to exploit compute power promisingly under such highly elastic\\, scalable\\, and multi-paradigm architecture. We made a lot of effort on enhancing the core resource scheduling\\, in order to bring high performance\\, efficient-sharing\\, and multi-tenancy oriented capabilities to Spark jobs. In this talk\\, we will focus on revealing the architecture of the cloud-native infrastructure\\; How we leverage YuniKorn scheduler to redefine the resource scheduling on Cloud. We will introduce how YuniKorn manages quotas\\, resource sharing\\, and auto-scaling\\, and ultimately how to schedule large scale Spark jobs efficiently on Kubernetes in the cloud.\\n</p>\\n\\n<p><em>\\nWeiwei Yang:<br />\\nWeiwei Yang is a Staff Software Engineer from Cloudera\\, an Apache Hadoop committer and PMC member. He is focused on technology around large scale\\, hybrid computation systems. Before Cloudera\\, he worked in Alibabas realtime computation infrastructure team that serves large scale big data workloads. Currently\\, Weiwei is leading the efforts for resource scheduling and management on K8s. Weiwei holds a masters degree from Peking University.<br />\\nGao Li:<br />\\nLi Gao is an engineering lead and infrastructure software engineer from Databricks\\, a leading open source and cloud vendor focusing on unified analytics cloud. Lis focuses are mainly on large scale distributed infrastructure across public cloud vendors and bringing kubernetes to the AI and big data compute workloads. Prior to Databricks\\, Li has led major data infrastructure and data platform efforts in companies such as Lyft\\, Fitbit\\, Salesforce\\, etc.\\n</em></p>","categories":"Big Data Track (2)","url":"https://apachecon.com/acah2020/tracks/bigdata-2.html#T1735"},{"uid":"acah2020-bigdata-2-T1815@apachecon.com","sequence":"0","dtstamp":"20200828T190237Z","dtstart":"20200929T181500Z","dtend":"20200929T185500Z","summary":"Everything you want to know about running Apache Big Data projects on ARM datacenters","location":"@home","x-alt-desc":"<strong>\\nZhenyu Zheng\\, Sheng Liu\\n</strong>\\n<p>\\nWith more and more vendors starts to provide ARM based chips\\, PCs and datacenters\\, more and more people starts to think about the portability of Apache big data projects running on ARM based hardwares: Will it run? How will it perform? Are there any difference running on different platforms? In this session\\, we will introduce what we have done in the past years to make Apache Big Data projects be able to running on ARM platforms\\, including the problems we sloved\\, how did we add CIs to related projects to provide a long-term ARM support\\, and what are the remaining gaps in projects like Hadoop\\, Spark\\, Hive\\, HBase\\, Kudu etc.\\, these general ideas can provide very useful information for users\\, devlopers and other projects that are intrested to verify thier portability on ARM platforms. We will also introduce some of our comparison results between running Big Data cluster on ARM datacenter and x86 datacenters\\, together with some improvement ideas which could make Big Data projects perform better on ARM datacenters.\\n</p>\\n\\n<p><em>\\nZhenyu Zheng:<br />\\nWorking in Open Source communities for over 5 years\\, currently focus on ARM portability for Open Source software.<br />\\nSheng Liu:<br />\\nWorking on OpenSource for over 7 years\\, currently focus on arm portability for big data projects and performance tuning\\n</em></p>","categories":"Big Data Track (2)","url":"https://apachecon.com/acah2020/tracks/bigdata-2.html#T1815"},{"uid":"acah2020-bigdata-2-T1855@apachecon.com","sequence":"1","dtstamp":"20200828T190237Z","dtstart":"20200929T185500Z","dtend":"20200929T193500Z","summary":"Heating Up Analytical Workloads with Apache Druid","location":"@home","x-alt-desc":"<strong>\\nGian Merlino\\n</strong>\\n<p>\\nApache Druid is a modern analytical database that implements a memory-mappable storage format\\, indexes\\, compression\\, late tuple materialization\\, and a vectorized query engine that can operate directly on compressed data. This talk goes into detail on how Druid's query processing layer works\\, and how each component contributes to achieving top performance for analytical queries. We'll also share war stories and lessons learned from optimizing Druid for different kinds of use cases.\\n</p>\\n\\n<p><em>\\nGian Merlino is CTO and a co-founder of Imply\\, a San Francisco based technology company\\, and a committer on Apache Druid. Previously\\, Gian led the data ingestion team at Metamarkets (now a part of Snapchat) and held senior engineering positions at Yahoo. He holds a BS in Computer Science from Caltech.\\n</em></p>","categories":"Big Data Track (2)","url":"https://apachecon.com/acah2020/tracks/bigdata-2.html#T1855"},{"uid":"acah2020-bigdata-2-T1935@apachecon.com","sequence":"0","dtstamp":"20200828T190237Z","dtstart":"20200929T193500Z","dtend":"20200929T201500Z","summary":"FOSS Never Forgets: An Introduction to Free and Open Source Solutions for Data Processing and Management with an Emphasis on Fault Tolerance","location":"@home","x-alt-desc":"<strong>\\nKatie McMillan\\n</strong>\\n<p>\\nThis talk will provide an introduction to data processing and management in two of the most popular Free and Open Source Solutions: Hadoop and PostgreSQL. The talk will link these solutions to current challenges in big data\\, with an emphasis on the fault tolerancy of each solution. It will then show how the solutions can be used together\\, using Foreign Data Wrappers. By combining the functionality of PostgreSQL and Hadoop with Foreign Data Wrappers\\, one is able to apply ACID properties to processes (transactions) in Hadoop.\\n</p>\\n\\n<p><em>\\nKatie McMillan leads strategic and creative projects that advance the public health\\, information technology\\, and social sectors. She currently works for a multi-site Canadian hospital\\, and is an advisor to local\\, national and international healthcare and public interest initiatives. Katie brings passion and talent for new now know-how and engaging a diverse range of stakeholders to co-create solutions to complex organizational\\, industry and systems challenges. She leverages a multi-disciplinary background in Health Systems\\, Geographic Information Systems\\, Public Policy\\, and Data Science\\, and works with communities to design\\, select\\, implement\\, and monitor solutions in order to improve outcomes with an emphasis on multi-intervention approaches\\, interoperability\\, mixed methods\\, and Ubuntu. She has worked for organizations including Health Canada\\, the Canadian Institute for Health Information\\, and the Royal College of Physicians and Surgeons of Canada.\\nAdditional information:\\n</em></p>","categories":"Big Data Track (2)","url":"https://apachecon.com/acah2020/tracks/bigdata-2.html#T1935"},{"uid":"acah2020-bigdata-2-W1615@apachecon.com","sequence":"1","dtstamp":"20200828T190237Z","dtstart":"20200930T161500Z","dtend":"20200930T165500Z","summary":"Apache Spark Development Lifecycle at Workday","location":"@home","x-alt-desc":"<strong>\\nEren Avsarogullari\\, Pavel Hardak\\n</strong>\\n<p>\\nApache Spark is the backbone of Workday's Prism Analytics Platform\\, supporting various data processing use-cases such as Data Ingestion\\, Preparation(Cleaning\\, Transformation & Publishing) and Discovery. At Workday\\, we extend Spark OSS repo and build custom Spark releases covering our custom patches on the top of Spark OSS patches. Custom Spark release development introduces the challenges when supporting multiple Spark versions against to a single repo and dealing with large numbers of customers\\, each of which can execute their own long-running Spark Applications. When building the custom Spark releases and new Spark features\\, dedicated Benchmark pipeline is also important to catch performance regression by running the standard TPC-H & TPC-DS queries against to both Spark versions and monitoring Spark driver & executors' runtime behaviors before production. At deployment phase\\, we also follow progressive roll-out plan leveraged by Feature Toggles used to enable/disable the new Spark features at the runtime. As part of our development lifecycle\\, Feature Toggles help on various use cases such as selection of Spark compile-time and runtime versions\\, running test pipelines against to both Spark versions on the build pipeline and supporting progressive roll-out deployment when dealing with large numbers of customers and long-running Spark Applications. On the other hand\\, executed Spark queries' operation level runtime behaviors are important for debugging and troubleshooting. Incoming Spark release is going to introduce new SQL Rest API exposing executed queries' operation level runtime metrics and we transform them to queryable Hive tables in order to track operation level runtime behaviors per executed query. In the light of these\\, this session aims to cover Spark feature development lifecycle at Workday by covering custom Spark Upgrade model\\, Benchmark & Monitoring Pipeline and Spark Runtime Metrics Pipeline details through used patterns and technologies step by step.\\n</p>\\n\\n<p><em>\\nEren Avsarogullari<br />\\nEren Avsarogullari holds both B.Sc & M.Sc. degrees in Electrical & Electronics Engineering. Currently\\, he works at Workday on Data Analytics as Data Engineer. His current focus are Apache Spark internals and Distributed System challenges. He is also open source contributor and member at Apache Software Foundation (Contributed Projects: Apache Spark\\, Pulsar\\, Heron).<br />\\nPavel Hardak<br />Pavel is Director of Product Management with Workday. He works on Prism Analytics product\\, focusing on backend technologies\\, powered by Hadoop and Apache Spark. Pavel is particularly excited about Big Data\\, cloud\\, and open source\\, not necessarily in this order. Before Workday\\, Pavel was with Basho\\, the company behind Riak\\, open-source NoSQL database with Mesos\\, Spark and Kafka integrations. Earlier\\, Pavel was with Boundary\\, which has developed real-time SaaS monitoring solution and was acquired by BMC Corp. Before that\\, Pavel worked in Product Management and Engineering roles\\, focusing on Big Data\\, Cloud\\, Networking and Analytics\\, and authored several patents.\\n</em></p>","categories":"Big Data Track (2)","url":"https://apachecon.com/acah2020/tracks/bigdata-2.html#W1615"},{"uid":"acah2020-bigdata-2-W1655@apachecon.com","sequence":"1","dtstamp":"20200828T190237Z","dtstart":"20200930T165500Z","dtend":"20200930T173500Z","summary":"Build a reliable\\, efficient and easy-to-use service for Apache Spark at Uber's scale","location":"@home","x-alt-desc":"<strong>\\nNan Zhu\\, Wei Han\\n</strong>\\n<p>\\nAs the global platform supporting 14+ million daily trips\\, Uber leverages huge amounts of data to power decisions like pricing\\, routing\\, etc. Spark is the backbone of large-scale batch computing at Uber. Nearly 250K+ Spark applications serve in scenarios like data ingestion\\, data cleaning/transformation\\, and machine learning model training/inference\\, etc. on a daily basis. Running Spark as a service at Ubers scale faces many challenges. (a) reliability is the top priority for us while there are many factors that could cause outages and negative business impact. We build an end-to-end robust solution from job service to the Spark distro as well as the thoughtfully designed integrations with other components in data infrastructure. (b) centralized management of Spark applications is also crucial at Ubers scale. In the past 2-3 years\\, the Spark ecosystem in Uber has been evolving from a partially managed situation to a service with full-fledged functionalities like monitoring\\, version management\\, etc. (c) Processing the massive volume of data in Uber raises challenges against the efficiency of Spark framework itself. We have developed optimizations for nested column pruning\\, parallel hive table committing implementation\\, etc. to significantly improve the Spark application performance. In this talk\\, we will walk through the aforementioned journey in Uber and share the experiences and lessons learned along the way. We hope that this talk can showcase how Apache software serves industry worldwide\\, help others who face similar challenges\\, and raise more discussions around the topic.\\n</p>\\n\\n<p><em>\\nNan Zhu:<br />\\nNan is Tech Lead of Spark team in Uber. He works on Spark service handling 100s of 1000s of Spark application every day in Uber\\, and the internal features which scales Spark to handle massive volume of data in Uber. He is also PMC member of XGBoost\\, one of the most popular machine learning library in both industry and academia.<br />\\nWei Han:<br />\\nWei Han is an Engineering Manager\\, leading a few teams in Ubers data platform org\\, including Spark platform\\, Data security and Compliance\\, Privacy platform\\, and File Format(Apache Parquet)\\n</em></p>","categories":"Big Data Track (2)","url":"https://apachecon.com/acah2020/tracks/bigdata-2.html#W1655"},{"uid":"acah2020-bigdata-2-W1735@apachecon.com","sequence":"1","dtstamp":"20200828T190237Z","dtstart":"20200930T173500Z","dtend":"20200930T181500Z","summary":"Project Optimum: Spark Performance at LinkedIn Scale","location":"@home","x-alt-desc":"<strong>\\nYuval Degani\\n</strong>\\n<p>\\nA typical day for Spark at LinkedIn means running 35 million RAM-GB-hours\\, on top of 7 million CPU-core-hours of 20\\,000 unique applications. Maintaining predictable performance and SLAs\\, as-well-as optimizing performance across a complex infrastructure stack and a massive application base\\, all while keeping up with a 4x YoY workload growth is an immense challenge. Project Optimum addresses those needs by providing a set of performance analysis\\, reporting\\, profiling\\, and regression detection tools designed for a massive-scale production environment. It allows platform developers as-well-as application developers to detect even subtle regressions or improvements in application performance with a limited sample set\\, using a statistically-rigorous approach that can be automated and integrated into production monitoring systems. In this talk\\, we will cover how Project Optimum is used at LinkedIn to scale our Spark infrastructure while providing a reliable production environment. We will demonstrate its role as an ad-hoc performance debugging and profiling tool\\, an automatic regression detection tracking pipeline\\, and an elaborate reporting system geared towards providing users with insights about their jobs.\\n</p>\\n\\n<p><em>\\nYuval is a Staff Software Engineer at Linkedin\\, where he is focused on scaling and developing new features for Hadoop and Spark. Before that\\, Yuval was a Sr. Engineering Manager at Mellanox Technologies\\, leading a team working on introducing network acceleration technologies to Big Data and Machine Learning frameworks. Prior to his work in the Big Data and AI fields\\, Yuval was a developer\\, an architect\\, and later a team leader in the areas of low-level kernel development for cutting-edge high-performance network devices. Yuval holds a BSc in Computer Science from the Technion Institute of Technology\\, Israel.\\n</em></p>","categories":"Big Data Track (2)","url":"https://apachecon.com/acah2020/tracks/bigdata-2.html#W1735"},{"uid":"acah2020-bigdata-2-W1815@apachecon.com","sequence":"1","dtstamp":"20200828T190237Z","dtstart":"20200930T181500Z","dtend":"20200930T185500Z","summary":"Secure your Big Data Cloud cluster with SDX (Ranger\\, Atlas\\, Knox\\, HMS)","location":"@home","x-alt-desc":"<strong>\\nDeepak Sharma\\n</strong>\\n<p>\\nSecurity is very important aspect whether it is onprem or cloud infrastructure. we are here to discuss about how can we secure our DWX/Data science/Data mart cluster with SDX/Data lake. Data Lake security and governance is managed by a shared set of services referred to as a Data Lake cluster. A Data Lake cluster includes the following services: Hive MetaStore (HMS) -- table metadata Apache Ranger -- fine-grained authorization policies\\, auditing Apache Atlas -- metadata management and governance: lineage\\, analytics\\, attributes Apache Knox: Authenticating Proxy for Web UIs and HTTP APIs -- SSO IDBroker -- identity federation\\; cloud credentials this talk will explain how can we secure multiple workload using single/shared datalake and what are the configuration we need to take care\\, and how each of the apache open source component like Ranger\\, atlas\\, knox helps in this case.\\n</p>\\n\\n<p><em>\\nDeepak Sharma\\, Apache Ranger Committer Senior Software Engineer\\, Cloudera.\\n</em></p>","categories":"Big Data Track (2)","url":"https://apachecon.com/acah2020/tracks/bigdata-2.html#W1815"},{"uid":"acah2020-bigdata-2-W1855@apachecon.com","sequence":"1","dtstamp":"20200828T190237Z","dtstart":"20200930T185500Z","dtend":"20200930T193500Z","summary":"Extracting Patient Narrative from Clinical Notes : Implementing Apache Ctakes at scale using Apache Spark","location":"@home","x-alt-desc":"<strong>\\nDebdipto Misra\\n</strong>\\n<p>\\nPatient notes not only document patient history and clinical conditions but are rich in contextual data and are usually more reliable sources of medical information compared to discrete values in the Electronic Health Record (EHR). For a medium-sized integrated Health System like Geisinger this amounts to approximately fifty thousand notes each day. For information extraction on retrospective data\\, the volume can run into millions of notes depending on the selection criteria. This talk describes the journey taken by the Data Science Team at Geisinger to implement a distributed pipeline which uses Apache Ctakes as the Natural Language Processing (NLP) Engine to annotate notes across the entire spectrum of patient care. From re-writing certain components in the Ctakes engine to architecting data store and pipeline optimization for a better throughput\\, this talk delves into various technical difficulties faced while aspiring to truly do NLP at scale on clinical notes. Towards the end\\, the talk also demonstrates few usecases and how using Ctakes has helped clinicians and stakeholders to extract patient narratives from patient notes using Apache Solr and Banana.\\n</p>\\n\\n<p><em>\\nDebdipto Misra is a Data Scientist with Geisinger Health. Previously\\, he worked with AOL Inc. as a Platform Engineer in Audience Analytics and with EMC Corp. as a Systems Engineer. He has worked in the Data Mining and Analytics space for over half a decade. He won a fellowship and presented the Evolution of Prosthetics using Pattern Recognition on Ultrasound Signals at the 2014 IEEE Big Data Conference in Washington\\, DC. He has also published at multiple journals and presented at healthcare conferences like HIMSS. Currently\\,his main focus is on building capacity planning tools for healthcare organizations for bed-supply demand using various deep learning approaches and integrating it with patient notes.\\n</em></p>","categories":"Big Data Track (2)","url":"https://apachecon.com/acah2020/tracks/bigdata-2.html#W1855"},{"uid":"acah2020-bigdata-2-W1935@apachecon.com","sequence":"1","dtstamp":"20200828T190237Z","dtstart":"20200930T193500Z","dtend":"20200930T201500Z","summary":"Zeus: Ubers Highly Scalable and Distributed Shuffle as a Service","location":"@home","x-alt-desc":"<strong>\\nMayank Bansal\\, Bo Yang\\n</strong>\\n<p>\\nZeus is an efficient\\, highly scalable and distributed shuffle as a service which is powering all Data processing (Spark and Hive) at Uber. Uber runs one of the largest Spark and Hive clusters on top of YARN in industry which leads to many issues such as hardware failures (Burn out Disks)\\, reliability and scalability challenges. Zeus is built ground up to support hundreds of thousands of jobs and millions of containers which shuffles petabytes of shuffle data. Zeus has changed the paradigm of current external shuffle which resulted in far better performance for shuffle. Although the shuffle data is getting written Remote\\, the performance is better or the same for most of the Jobs. In this talk well take a deep dive into the Zeus architecture and describe how its deployed at Uber. We will then describe how its integrated to run shuffle for Spark\\, and contrast it with Sparks built-in sort-based shuffle mechanism. We will also talk about future roadmap and plans for Zeus.\\n</p>\\n\\n<p><em>\\nMayank Bansal:<br />\\nMayank Bansal is currently working as a Staff engineer at Uber in data infrastructure team. He is co-author of Peloton. He is Apache Hadoop Committer and Oozie PMC and Committer. Previously he was working at ebay in hadoop platform team leading YARN and MapReduce effort. Prior to that he was working at Yahoo and worked on Oozie.<br />\\nBo Yang:<br />\\nBo is Sr. Software Engineer II in Uber and working on Spark team. In the past he worked on many streaming technologies.\\n</em></p>","categories":"Big Data Track (2)","url":"https://apachecon.com/acah2020/tracks/bigdata-2.html#W1935"},{"uid":"acah2020-bigdata-2-R1655@apachecon.com","sequence":"0","dtstamp":"20200828T190237Z","dtstart":"20201001T165500Z","dtend":"20201001T173500Z","summary":"Flink SQL in 2020: Time to show off!","location":"@home","x-alt-desc":"<strong>\\nTimo Walther\\n</strong>\\n<p>\\nFour years ago\\, the Apache Flink community started adding SQL support to ease and unify the processing of static and streaming data. Today\\, Flink runs business critical batch and streaming SQL queries at Alibaba\\, Huawei\\, Lyft\\, Uber\\, Yelp\\, and many others. Although the community made significant progress in the past years\\, there are still many things on the roadmap and the development is still speeding up. In the past months\\, several significant improvements and extensions were added including support for DDL statements\\, refactorings of the type system and the catalog interface\\, as well as Apache Hive integration. Since it is difficult to follow all development efforts that happen around Flink SQL and its ecosystem\\, it is time for an update. This session will focus on a comprehensive demo of what is possible with Flink SQL in 2020. Based on a realistic use case scenario\\, we'll show how to define tables which are backed by various storage systems and how to solve common tasks with streaming SQL queries. We will demonstrate Flink's Hive integration and show how to define and use user-defined functions. We'll close the session with an outlook of upcoming features.\\n</p>\\n\\n<p><em>\\nTimo Walther is a committer and PMC member of the Apache Flink project. He studied Computer Science at TU Berlin. Alongside his studies\\, he participated in the Database Systems and Information Management Group there and worked at IBM Germany. Timo joined the project before it became part of the Apache Software Foundation. Today he works as a senior software engineer at Ververica. In Flink\\, he is mainly working on the Table & SQL ecosystem.\\n</em></p>","categories":"Big Data Track (2)","url":"https://apachecon.com/acah2020/tracks/bigdata-2.html#R1655"},{"uid":"acah2020-bigdata-2-R1735@apachecon.com","sequence":"0","dtstamp":"20200828T190237Z","dtstart":"20201001T173500Z","dtend":"20201001T181500Z","summary":"Stepping towards Bigdata on ARM","location":"@home","x-alt-desc":"<strong>\\nVinayakumar B\\, Liu Sheng\\n</strong>\\n<p>\\nWe find ARM processors in most devices around us always. ARM processors are mostly used today in small devices due to their power efficiency and reduced cost. But until few years back\\, ARM processors were found in only small devices. In recent years\\, since major OS providers are supporting ARM processors\\, ARM ecosystem is stepping into server class business as well. Now since major cloud providers started providing instances based on ARM\\, with far less price of-course\\, many businesses moving towards using ARM processors for the deployment. Major factors for considering the deployment environment for any business are Cost and Performance. ARM being cheaper (upto ~50%) than x86\\, reduces the overall cost of deployment and maintainance for horizontally scalable business deployments. So what about Bigdata workloads on ARM? Bigdata components are mostly scalable. They will benefit from this model as well\\, provided they support deployment on ARM. This talk\\, discusses about such initiative to support deployment and optimizations on ARM servers of Major bigdata components and reduce the total cost of deployment and management for the user. This includes contributions done in various apache projects like\\, Hadoop\\, Spark\\, Hive\\, HBase\\, Kudu\\, Impala\\, to support deployments on ARM nodes and optimizations for ARM processor. Finally\\, shows some benchmark results in ARM and x86 deployments.\\n</p>\\n\\n<p><em>\\nVinayakumar B:<br />\\nVinayakumar B\\, having vast experience with Hadoop for 10+ years. Focuses on improvements in and around Hadoop and Big-Data. Contributing to Hadoop community from 8+ years\\, and currently a Committer and member of Apache Hadoop PMC.<br />\\nLiu Sheng:<br />\\nLiuSheng\\, Focusing on promoting opensource projects in Big-Data components running on ARM platform. Contributed towards building ARM CI for Hadoop community and others. Currently working on ARM support for Kudu project\\n</em></p>","categories":"Big Data Track (2)","url":"https://apachecon.com/acah2020/tracks/bigdata-2.html#R1735"},{"uid":"acah2020-bigdata-2-R1815@apachecon.com","sequence":"0","dtstamp":"20200828T190237Z","dtstart":"20201001T181500Z","dtend":"20201001T185500Z","summary":"Managing Transaction on Ethereum with Apache Airflow","location":"@home","x-alt-desc":"<strong>\\nMichael Ghen\\n</strong>\\n<p>\\nApache Airflow is a Python-based workflow management system that can be used to actively monitor and execute transactions on blockchain networks like Ethereum. This presentation is an introduction to Apache Airflow followed by a demonstration of a production deployment. Apache Airflow is an excellent tool for anyone already familiar with Python. Its ability to process jobs and handle errors makes it a good choice tool for managing activity on blockchain networks. The goal of this talk is to demonstrate how Apache Airflow can be used for environmental scanning and batch processing transactions. The demonstration will cover using Airflow and Python for monitoring and executing ERC20 token transactions on the Ethereum blockchain.\\n</p>\\n\\n<p><em>\\nMichael Ghen is a computer engineer from Philadelphia that has contributed to Apache Airflow and Apache Unomi. He has a B.S. in computer engineering from Pennsylvania State University and an M.S. in analytics from Brandeis. Currently\\, he is a GAANN Cybersecurity Fellow at Drexel where he is pursuing a Ph.D. in electrical engineering. He previously served as a data architect and engineer at start-ups and non-profits where he used Apache Airflow to build data pipelines.\\n</em></p>","categories":"Big Data Track (2)","url":"https://apachecon.com/acah2020/tracks/bigdata-2.html#R1815"},{"uid":"acah2020-bigdata-2-R1855@apachecon.com","sequence":"0","dtstamp":"20200828T190237Z","dtstart":"20201001T185500Z","dtend":"20201001T193500Z","summary":"After NoSQL discover CloudSQL databases","location":"@home","x-alt-desc":"<strong>\\nRomain Manni-Bucau\\, Enrico Olivelli\\n</strong>\\n<p>\\nApplication without persistence are rare and since some years the persistence layer is changing a lot. After years where SQL databases where the only ones\\, we saw NoSQL popping up bringing new concepts. More recently\\, the cloud changed again our paradigms with distributed computing and microservices. However\\, even with these brand new solutions\\, we still lack the flexibility of the SQL in terms of evolutivity and tooling. This is where HerdDB is entering into the game. Built as a SQL database\\, its foundations are Apache BookKeeper (bookie for close friends) and Apache Calcite. Therefore it brings to our architecture new solutions. This talk will first go through the challenges which led to creating HerdDB\\, then how it is designed and why it merges the best of both NoSQL and SQL worlds and finally we will illustrate its usage by two applications using very different deployment modes (from plain old bare metal to Kubernetes) using Apache Meecrowave and Geronimo Microprofile Stack.\\n</p>\\n\\n<p><em>\\nRomain Manni-Bucau:<br />\\nJoined the Apache EE family (OpenWebBeans\\, Meecrowave\\, Johnzon\\, BatchEE...) in 2011. My goal is to make development a detail of an idea becoming reality.<br />\\nEnrico Olivelli:<br />\\nSoftware Developer Manager at https://MagNews.com and https://EmailSuccess.com. PMC in Apache BookKeeper\\,ZooKeeper\\,Curator\\, Committer in Maven. OpenSource/ASF Enthusiast Initial creator of other OpenSource projects:HerdDB\\,BlazingCache\\,BlobIt\\n</em></p>","categories":"Big Data Track (2)","url":"https://apachecon.com/acah2020/tracks/bigdata-2.html#R1855"},{"uid":"acah2020-bigdata-2-R1935@apachecon.com","sequence":"1","dtstamp":"20200828T190237Z","dtstart":"20201001T193500Z","dtend":"20201001T201500Z","summary":"Apache Big-Data meets Cloud-Native and Kubernetes","location":"@home","x-alt-desc":"<strong>\\nMrton Elek\\n</strong>\\n<p>\\nApache big-data projects / the Hadoop ecosystem is widely adopted and very popular so the Kubernetes / Cloud-native tools. Surprisingly there are only a very minimal number of projects in the intersection of the two words. This presentation explains why could it be\\, shows the key problems to run Apache Big-Data projects (such as Hadoop\\, Kafka\\, Flink\\, Spark...) on Kubernetes and gives a demo of a possible solution.\\n</p>\\n\\n<p><em>\\nMarton Elek is PMC in Apache Hadoop and Apache Ratis projects and working on the Apache Hadoop Ozone at Cloudera. Ozone is a new Hadoop sub-project which provides an S3 compatible Object Store for Hadoop on top of a new generalized binary storage layer. He is also working on the containerization of Hadoop and creating different solutions to run Apache Big Data projects in Kubernetes and other could native environments.\\n</em></p>","categories":"Big Data Track (2)","url":"https://apachecon.com/acah2020/tracks/bigdata-2.html#R1935"},{"uid":"acah2020-camel-T1615@apachecon.com","sequence":"1","dtstamp":"20200810T143450Z","dtstart":"20200929T161500Z","dtend":"20200929T165500Z","summary":"What's new with Apache Camel 3?","location":"@home","x-alt-desc":"<strong>\\nAndrea Cosentino\\, Claus Ibsen\\n</strong>\\n<p>\\nWith the release of Apache Camel 3\\, the Camel family has been extended to include a full range of projects that are tailored to popular platforms including Spring Boot\\, Quarkus\\, Kafka\\, Kubernetes\\, and others\\; creating an ecosystem. Lets discover it.\\n</p>\\n\\n<p><em>\\nAndrea Cosentino:<br />\\nAndrea Cosentino (@oscerd on Github and @oscerd2 on Twitter) is an open-source addicted and software developer. Hes co-leading Apache Camel and hes actually the PMC Chair of the project. Hes currently working on expanding the Camel ecosystem through new subprojects like Camel-k\\, Camel-Quarkus and Camel-Kafka-connector (the latest project in the family). Andrea is active on multiple projects like Apache Karaf\\, where he is committer\\, Apache Servicemix\\, where he is PMC Member\\, Fabric8 Kubernetes-client\\, where he is a core maintainer and many others. Andrea is active on social media and blogs\\, trying to spread the word about Apache Camel and open-source in general. He is actually Senior Software Engineer in Red Hat\\, working in the Red Hat Fuse team\\, focusing on integration. He is based in Rome\\, Italy\\, where he lives with his wife and son.<br />\\nClaus Ibsen:<br />\\nClaus Ibsen (@davsclaus) is an open-source enthusiast and software developer. He's co-leading the Apache Camel project\\, a project used for integration\\; which he has been working on full time for more than a decade. Currently Claus is working on expanding Camel into cloud-native and serverless with the latest innovations of Apache Camel K and Camel Quarkus. With passion and enthusiasm Claus evangelizes about Apache Camel\\, Java and open source by being active on social media\\, writing blogs and books\\, speaking at conferences\\, etc. Claus is also active in other open source projects such as Apache ActiveMQ\\, Eclipse Vert.x\\, Fabric8\\, Hawtio\\, and Quarkus. Besides being a JavaChampion\\, Claus is also a member at Apache Software Foundation. Prior to joining Red Hat\\, he has worked as a software developer\\, architect\\, and consultant for over a decade. He is based in Denmark.\\n</em></p>","categories":"Camel/Integration","url":"https://apachecon.com/acah2020/tracks/camel.html#T1615"},{"uid":"acah2020-camel-T1655@apachecon.com","sequence":"1","dtstamp":"20200810T143450Z","dtstart":"20200929T165500Z","dtend":"20200929T173500Z","summary":"Making Enterprise Integration Patterns Work for You","location":"@home","x-alt-desc":"<strong>\\nJustin Reock\\n</strong>\\n<p>\\nLearn about the powerful world of Enterprise Integration Patterns as implemented by the amazing Apache Camel framework. This session covers:<br />\\nA  Camel basics\\, understanding Exchanges\\, Routes\\, and how to implement EIPs with them<br />\\nB  Examples of real implementations of common EIPs like Content Based Routers and Recipient Lists<br />\\nC  Integration of Camel with common endpoints\\, like JMS\\, FTP\\, and HTTP\\n</p>\\n\\n<p><em>\\nJustin has over 20 years experience working in various software roles and is an outspoken free software evangelist\\, delivering enterprise solutions and community education on databases\\, integration work\\, architecture\\, and technical leadership. He is currently the Chief Architect at OpenLogic by Perforce.\\n</em></p>","categories":"Camel/Integration","url":"https://apachecon.com/acah2020/tracks/camel.html#T1655"},{"uid":"acah2020-camel-T1735@apachecon.com","sequence":"1","dtstamp":"20200810T143450Z","dtstart":"20200929T173500Z","dtend":"20200929T181500Z","summary":"Getting started with Apache Camel on Quarkus","location":"@home","x-alt-desc":"<strong>\\nAlexandre Gallice\\n</strong>\\n<p>\\nApache Camel is the proven Swiss knife of integration for more than a decade and still growing in the cloud era. As a Java based framework\\, it makes perfect sense for Camel to reap the benefit from Quarkus\\, the Kubernetes Java stack tailored for OpenJDK HotSpot and GraalVM. In this hands on demo\\, I will show what it looks like to develop with Camel Quarkus. One could expect to take away some key concepts and maybe a willingness to join the lively Camel community :)\\n</p>\\n\\n<p><em>\\nAlexandre is a Senior Software Engineer at Red Hat and a member of the Apache Camel Project Management Committee. He is deepening his interest in Open Source for a few years now with a current focus on Camel Quarkus.\\n</em></p>","categories":"Camel/Integration","url":"https://apachecon.com/acah2020/tracks/camel.html#T1735"},{"uid":"acah2020-camel-T1815@apachecon.com","sequence":"1","dtstamp":"20200810T143451Z","dtstart":"20200929T181500Z","dtend":"20200929T185500Z","summary":"Build and Deploy Cloud Native Camel Quarkus Routes With Tekton and\\nKnative","location":"@home","x-alt-desc":"<strong>\\nOmar Al-Safi\\n</strong>\\n<p>\\nIn this talk\\, we will leverage all cloud native stacks and tools to build Camel Quarkus routes natively using GraalVM native-image on Tekton pipeline and deploy these routes to Kubernetes cluster with Knative installed. We will dive into the following topics in the talk: - Introduction to Camel - Introduction to Camel Quarkus - Introduction to GraalVM Native Image - Introduction to Tekon - Introduction to Knative - Demo shows how to deploy end to end a Camel Quarkus route which include the following steps: - Look at whole deployment pipeline for Cloud Native Camel Quarkus routes - Build Camel Quarkus routes with GraalVM native-image on Tekton pipeline. - Deploy Camel Quarkus routes to Kubernetes cluster with Knative Targeted Audience: Users with basic Camel knowledge\\n</p>\\n\\n<p><em>\\nOmar Al-Safi is an Open Source Software Engineer at Talend. He is an active Apache Camel contributor and Apache Camel PMC. His experience before contributing to Apache Camel\\, revolved around building streaming platforms with Kafka\\, Kafka Streams as well as contributing to Debezium project.\\n</em></p>","categories":"Camel/Integration","url":"https://apachecon.com/acah2020/tracks/camel.html#T1815"},{"uid":"acah2020-camel-T1855@apachecon.com","sequence":"2","dtstamp":"20200810T143451Z","dtstart":"20200929T185500Z","dtend":"20200929T193500Z","summary":"Camel Kafka Connectors: when camel meets kafka","location":"@home","x-alt-desc":"<strong>\\nAndrea Tarocchi\\, Hugo Guerrero\\n</strong>\\n<p>\\nApache Kafka is getting used as an event backbone in new organizations every day. We would love to send every byte of data through the event bus. However\\, most of the time\\, connecting to simple third party applications and services becomes a headache that involves several lines of code and additional applications. As a result\\, connecting Kafka to services like Google Sheets\\, communication tools such as Slack or Telegram\\, or even the omnipresent Salesforce\\, is a challenge nobody wants to face. Wouldnt you like to have hundreds of connectors readily available out-of-the-box to solve this problem? Due to these challenges\\, communities like Apache Camel are working on how to speed up development on key areas of the modern application\\, like integration. The Camel Kafka Connect project\\, from the Apache foundation\\, has enabled their vastly set of connectors to interact with Kafka Connect natively. So\\, developers can start sending and receiving data from Kafka to and from their preferred services and applications in no time without a single line of code. In summary\\, during this session we will: - Introduce you to the Camel Kafka Connector sub-project from Apache Camel - Go over the list of connectors available as part of the project - Showcase a couple of examples of integrations using the connectors - Share some guidelines on how to get started with the Camel Kafka Connectors\\n</p>\\n\\n<p><em>\\nAndrea Tarocchi:<br />\\nAndrea Tarocchi is a Senior Software Engineer at Red Hat. He has been solving application integration problems for more than 10 years spanning different roles. Co-creator of camel-kakfa-connector project\\, Apache Camel committer. He is a long time opensource enthusiast been lucky enough to work with and contribute to some great open source projects\\, like Apache Camel\\, Apache Kafka\\, Drools to mention a few.<br />\\nHugo Guerrero:<br />\\nHugo Guerrero works at Red Hat as an APIs and messaging developer advocate. In this role\\, he helps the marketing team with technical overview and support to create\\, edit\\, and curate product content shared with the community through webinars\\, conferences\\, and other activities. With more than 15 years of experience as a developer\\, consultant\\, architect\\, and software development manager\\, he also works on open source software with major private and federal public sector clients in Latin America\\n</em></p>","categories":"Camel/Integration","url":"https://apachecon.com/acah2020/tracks/camel.html#T1855"},{"uid":"acah2020-camel-T1935@apachecon.com","sequence":"1","dtstamp":"20200810T143451Z","dtstart":"20200929T193500Z","dtend":"20200929T201500Z","summary":"Integrating Postgres with Apache Camel and ActiveMQ","location":"@home","x-alt-desc":"<strong>\\nJustin Reock\\n</strong>\\n<p>\\nLearn how to use Postgres as a backing persistence adapter for the ActiveMQ messaging platform\\, as well as an integration endpoint for the powerful Apache Camel integration framework. Not only will you learn about JDBC\\, but you'll also get a solid introduction to these two mature and powerful integration platforms.\\n</p>\\n\\n<p><em>\\nJustin has over 20 years experience working in various software roles and is an outspoken free software evangelist\\, delivering enterprise solutions and community education on databases\\, integration work\\, architecture\\, and technical leadership. He is currently the Chief Architect at OpenLogic by Perforce.\\n</em></p>","categories":"Camel/Integration","url":"https://apachecon.com/acah2020/tracks/camel.html#T1935"},{"uid":"acah2020-camel-W1615@apachecon.com","sequence":"2","dtstamp":"20200810T143451Z","dtstart":"20200930T161500Z","dtend":"20200930T165500Z","summary":"Camel API Gateway","location":"@home","x-alt-desc":"<strong>\\nRodrigo Coelho\\n</strong>\\n<p>\\nOpen Source light API Gateway built with Apache Camel<br />\\nRecently I was challenged to find alternatives to the existing API Gateway infrastructure.<br />\\nNot being able to find any solution with all we need to offer\\, Apache Camel was the perfect candidate.<br />\\nWe called it CAPI Gateway!<br />\\nCAPI provides the following features: Light API Gateway powered by Apache Camel and Spring Boot\\,  Dynamic Routes (REST and Websockets)\\, Customizable processors\\, Integration with external Identity Providers (Default is Keycloak)\\, API Manager Interface\\, distributed tracing system (Zipkin)\\, Metrics (Prometheus)\\, API Subscription Engine (Keycloak)\\, Traffic management (Apache Camel Kafka)\\, analytics for the metrics (Grafana) and Error/Blocking strategies.\\n</p>\\n\\n<p><em>\\nRodrigo is a Software Architect and Technology lover particularly focus on Open Source technology and DevOps\\, and how we can apply this new paradigms to the real and challenging enterprise world. With more than 17 years of experience\\, I've always made a huge effort to stay on top of the big changes in the software industry\\, not only on the software itself but also on paradigm. Since 2013 I've been working as a software and solution architect\\, giving my contribution to companies like bpost (Belgium) and AXA Bank (Belgium). On this last project at AXA I was responsible for the design and implementation from scratch of a strategic application. Since January I've been working as a Software Architect \\, Technology Expert and Open Source advocate for the Reusable Components Office at the European Commission.\\n</em></p>","categories":"Camel/Integration","url":"https://apachecon.com/acah2020/tracks/camel.html#W1615"},{"uid":"acah2020-camel-W1655@apachecon.com","sequence":"2","dtstamp":"20200810T143451Z","dtstart":"20200930T165500Z","dtend":"20200930T173500Z","summary":"How to contribute textual tooling for Apache Camel in several IDEs","location":"@home","x-alt-desc":"<strong>\\nAurlien Pupier\\n</strong>\\n<p>\\nApache Camel allows to configure Integration projects using several textual Domain Specific Languages. In this talk\\, you will learn how tooling is proposed for several IDEs and editors thanks to the Language Server Protocol and its implementation for Apache Camel language. Entry points to allow you to join the party and contribute will be presented.\\n</p>\\n\\n<p><em>\\nAurlien is working in Red Hat Integration Tooling team. Developing tooling targeting developers for more than 10 years.\\n</em></p>","categories":"Camel/Integration","url":"https://apachecon.com/acah2020/tracks/camel.html#W1655"},{"uid":"acah2020-camel-W1735@apachecon.com","sequence":"0","dtstamp":"20200828T190237Z","dtstart":"20200930T173500Z","dtend":"20200930T181500Z","summary":"Serverless Integration Anatomy","location":"@home","x-alt-desc":"<strong>\\nChristina Lin\\n</strong>\\n<p>\\nA quick study of the structure on building Serverless Integration. Piecing together how Kubernetes\\, Knative\\, Kafka and Apache Camel. Going over the lifecycle of a serverless application\\, from development\\, deployment to monitoring it live. Talk about things to consider when building this type of architecture. At the end a quick demo to show how it works.\\n</p>\\n\\n<p><em>\\nChristina Lin is the Technical evangelist for Red Hat Integration Products. She helps to grow market awareness and establish thought leadership for Fuse\\, AMQ and 3scale. By creating online videos\\, getting started blogs and also spoke at many conference around the globe. She has worked in software integration for the finance\\, telecom\\, and manufacturing industries\\, mostly architectural design and implementation. These real life system experiences helps her to be practical and combining open source technology\\, she hopes to bring more innovative ideas for the future system development.\\n</em></p>","categories":"Camel/Integration","url":"https://apachecon.com/acah2020/tracks/camel.html#W1735"},{"uid":"acah2020-camel-W1815@apachecon.com","sequence":"1","dtstamp":"20200810T143451Z","dtstart":"20200930T181500Z","dtend":"20200930T185500Z","summary":"Testing Camel K integrations with Cloud Native BDD","location":"@home","x-alt-desc":"<strong>\\nChristoph Deppisch\\n</strong>\\n<p>\\nApache Camel K is a lightweight integration platform built from Apache Camel. Integrations built with Camel K run natively on Kubernetes and are specifically designed for serverless architectures. With the declarative nature in Camel K users can instantly run integration code written in Camel DSL on their preferred cloud. The presentation outlines typical integration scenarios with Camel K and shows how to write automated tests for these enterprise integrations. The session covers classical service provider/consumer scenarios with common messaging protocols (e.g. REST\\, JMS\\, Kafka) as well as more complex integrations with data access and 3rd party Saas services included. The tests itself will also be Cloud Native citizens and make use of Behavior Driven Development concepts.\\n</p>\\n\\n<p><em>\\nChristoph is a senior software engineer at Red Hat working on Middleware application services with Apache Camel. He has worked in enterprise integration projects for more than 10 years and has gained special interest in test automation. Christoph is the founder of the Open Source test framework Citrus and believes in automated integration testing with passion.\\n</em></p>","categories":"Camel/Integration","url":"https://apachecon.com/acah2020/tracks/camel.html#W1815"},{"uid":"acah2020-camel-W1855@apachecon.com","sequence":"2","dtstamp":"20200810T143451Z","dtstart":"20200930T185500Z","dtend":"20200930T193500Z","summary":"\\\"Cloud Native\\\" My Camel","location":"@home","x-alt-desc":"<strong>\\nMichael Costello\\, David Gordon \\n</strong>\\n<p>\\nThis talk takes a look at our traditional enterprise integration needs\\, how we have typically solved them with Enterprise Integration Patterns (EIP) via Apache Camel (running in Apache Karaf) and how to use a new way of deploying Camel (and ultimately our enterprise integration solutions) with Camel K to put enterprise integration on \\\"Cloud Native\\\" steroids. During the talk we'll discuss a typical integration performed using Enterprise Integration Patterns with Apache Camel\\, why approaching problems with this technology is even more valuable in our brave new cloud world\\, and demonstrate how to put this on cloud native steroids with Camel K.\\n</p>\\n\\n<p><em>\\nMike has spent the last 2 decades in the enterprise integration space. Beginning with his love for J2EE\\, emerged a love for Service Oriented Architecture and as the years carried on his romance with MicroServices and cloud native distributed integration platforms began to really flourish. Mike\\, spent his college years at the University of Texas and currently works for Red Hat as an Architect in an Emerging Techonology Practice (Enterprise Integration) of Red Hat Consulting. His views may or may not be shared by his employer (or anyone else for that matter). When not swashbuckling with containers\\, or integrating event streams\\, Mike may be found kicking a soccer ball around the Austin\\, Texas area.\\n<br />\\nDavid helps organizations use open source software to implement integration solutions. David specializes in open source including Camel\\, Spring Boot\\, Kubernetes\\, ActiveMQ\\, Enmasse\\, Kafka\\, Strimzi\\, 3scale\\, Keycloak\\, Istio\\, and more. David helps design and develop implementations using these components\\, and leverage those experiences to develop feedback for engineering groups he works with and upstream development communities.\\n</em></p>","categories":"Camel/Integration","url":"https://apachecon.com/acah2020/tracks/camel.html#W1855"},{"uid":"acah2020-camel-W1935@apachecon.com","sequence":"1","dtstamp":"20200810T143451Z","dtstart":"20200930T193500Z","dtend":"20200930T201500Z","summary":"Software Architecture and Architectors: useless VS valuable","location":"@home","x-alt-desc":"<strong>\\nAndrei Shakirin\\n</strong>\\n<p>\\nTalk introduces definitions and sense of system architecture. Presenter will show seven cases from real projects\\, where wrong\\, missing or over-sophisticated architecture decisions really hurt the development teams. The rescue solution and lesson learned will be presented for each situation. The presenting cases and solutions are related to Apache Projects: Apache Karaf\\, CXF\\, Camel\\, Kafka. Open discussion and own cases and project situations are welcome.\\n</p>\\n\\n<p><em>\\nAndrei is a software architect in the Talend team developing the open source Application Integration platform based on Apache projects. The areas of his interest are REST API design\\, Microservices\\, Cloud\\, resilient distributed systems\\, security and agile development. Andrei is PMC and committer of Apache CXF and committer of Syncope projects. He is member of OASIS S-RAMP Work Group and speaker at Java and Apache conferences.</em></p>","categories":"Camel/Integration","url":"https://apachecon.com/acah2020/tracks/camel.html#W1935"},{"uid":"acah2020-camel-R1615@apachecon.com","sequence":"1","dtstamp":"20200810T143451Z","dtstart":"20201001T161500Z","dtend":"20201001T165500Z","summary":"Panel on the future of Software Integration","location":"@home","x-alt-desc":"<strong>\\nMaria Arias de Reyna Dominguez\\n</strong>\\n<p>\\nPanel on the future of Software Integration.\\n</p>\\n\\n<p><em>\\nMara Arias de Reyna is a Java Senior Software Engineer\\, geospatial enthusiast and Open Source advocator. She has been a community leader and core maintainer of several free and open source projects since 2004. She is currently working at Red Hat where she focuses on Middleware and maintains Apache Camel and Syndesis. Mara is an experienced keynoter and speaker. Between 2017 and 2019 Mara was the elected President of OSGeo\\, the Open Source Geospatial Foundation which serves as an umbrella for the most used geospatial free and open source software. She is also well known as a feminist and Women In Tech activist.\\n</em></p>","categories":"Camel/Integration","url":"https://apachecon.com/acah2020/tracks/camel.html#R1615"},{"uid":"acah2020-camel-R1735@apachecon.com","sequence":"1","dtstamp":"20200812T151913Z","dtstart":"20201001T173500Z","dtend":"20201001T181500Z","summary":"Camel Lightning Talks","location":"@home","x-alt-desc":"<strong>\\nYou! Any speaker is welcome!\\n</strong>\\n<p>\\nCurrently submitted lightning talks are shown here on the left\\, submit your lightning talk using the form on the right. Please allow the form to load fully.\\n<p>\\n<iframe src=\\\"https://airtable.com/embed/shrX9WHCWoBNfNLgf?backgroundColor=orange&viewControls=on\\\" frameborder=\\\"0\\\" onmousewheel=\\\"\\\" width=\\\"45%\\\" height=\\\"1350\\\" style=\\\"background: transparent\\; border: 1px solid #ccc\\;\\\"></iframe>\\n<iframe src=\\\"https://airtable.com/embed/shrQcG9JiTdk4JRRx?backgroundColor=orange\\\" frameborder=\\\"0\\\" onmousewheel=\\\"\\\" width=\\\"45%\\\" height=\\\"1350\\\" style=\\\"background: transparent\\; border: 1px solid #ccc\\;\\\"></iframe>\\n</p>\\n\\n<p><em>\\n</em></p>","categories":"Camel/Integration","url":"https://apachecon.com/acah2020/tracks/camel.html#R1735"},{"uid":"acah2020-cassandra-T0930@apachecon.com","sequence":"1","dtstamp":"20200810T213949Z","dtstart":"20200929T093000Z","dtend":"20200929T101000Z","summary":"Lessons Learned: Building Cassandra DBaaS on Alibaba Cloud","location":"@home","x-alt-desc":"<strong>\\nMaxwell Guo\\n</strong>\\n<p>\\nDuring this session\\, we will share the lessons we learned when we built Apache Cassandra as a Service on Alibaba Cloud. Specific topics include: - how we boosted Cassandra performance through soft raid on cloud disk\\, - why we do a continuous full incremental backup - how to apply automatic data repairs Additionally\\, we will share the experience of doing non-stop data migration between different Cassandra clusters and between Cassandra and other databases as well as how we optimize a Cassandra service for different use case.\\n</p>\\n\\n<p><em>\\nMaxwell is a cloud software architect at Alibaba\\, working on offering Apache Cassandra as a cloud based service.\\n</em></p>","categories":"Cassandra","url":"https://apachecon.com/acah2020/tracks/cassandra.html#T0930"},{"uid":"acah2020-cassandra-T1615@apachecon.com","sequence":"1","dtstamp":"20200810T213949Z","dtstart":"20200929T161500Z","dtend":"20200929T165500Z","summary":"Towards Practical Self-Healing Distributed Databases","location":"@home","x-alt-desc":"<strong>\\nDinesh Joshi\\, Joey Lynch\\n</strong>\\n<p>\\nAs distributed databases expand in popularity\\, there is ever-growing research into new database architectures that are designed from the start with built-in self-tuning and self- healing features. In real world deployments\\, however\\, migration to these entirely new systems is impractical and the challenge is to keep massive fleets of existing databases available under constant software and hardware change. Apache Cassandra is one such existing database that helped to popularize \\\"scale-out\\\" distributed databases and it runs some of the largest existing deployments of any open-source distributed database. In this talk\\, we demonstrate the techniques needed to transform the typical\\, highly manual\\, Apache Cassandra deployment into a self-healing system. We start by composing specialized agents together to surface the needed signals for a self-healing deployment and to execute local actions. Then we show how to combine the signals from the agents into the cluster level control- planes required to safely iterate and evolve existing deployments without compromising database availability. Finally\\, we show how to create simulated models of the database's behavior\\, allowing rapid iteration with minimal risk. With these systems in place\\, it is possible to create a truly self-healing database system within existing large-scale Apache Cassandra deployments.\\n</p>\\n\\n<p><em>\\nDinesh Joshi:<br />\\nDinesh A. Joshi has been a professional Software Engineer for over a decade building highly scalable realtime Web Services and Distributed Streaming Data Processing Architectures serving over 1 billion devices. Dinesh is an active contributor to the Apache Cassandra codebase. He has a Masters degree in Computer Science (Distributed Systems & Databases) from Georgia Tech\\, Atlanta\\, USA.<br />\\nJoey Lynch:<br />\\nJoey helps keep the wheels on the bus for Netflixs data infrastructure.\\n</em></p>","categories":"Cassandra","url":"https://apachecon.com/acah2020/tracks/cassandra.html#T1615"},{"uid":"acah2020-cassandra-T1655@apachecon.com","sequence":"1","dtstamp":"20200810T213949Z","dtstart":"20200929T165500Z","dtend":"20200929T173500Z","summary":"Building Apache Cassandra 4.0: behind the scenes","location":"@home","x-alt-desc":"<strong>\\nDinesh Joshi\\n</strong>\\n<p>\\nBuilding a database is hard. Building a distributed database is harder. Building a distributed database that the industry relies on is even harder. Our goal to build Apache Cassandra 4.0 is to make it rock solid. In this talk\\, we go behind the scenes to show you how the Apache Cassandra community is building and testing Apache Cassandra 4.0 so that it is the most stable release ever!\\n</p>\\n\\n<p><em>\\nDinesh A. Joshi has been a professional Software Engineer for over a decade building highly scalable realtime Web Services and Distributed Streaming Data Processing Architectures serving over 1 billion devices. Dinesh is an active contributor to the Apache Cassandra codebase. He has a Masters degree in Computer Science (Distributed Systems & Databases) from Georgia Tech\\, Atlanta\\, USA.\\n</em></p>","categories":"Cassandra","url":"https://apachecon.com/acah2020/tracks/cassandra.html#T1655"},{"uid":"acah2020-cassandra-T1735@apachecon.com","sequence":"1","dtstamp":"20200810T213949Z","dtstart":"20200929T173500Z","dtend":"20200929T181500Z","summary":"5 Ways to Solve Cassandra GC Problems","location":"@home","x-alt-desc":"<strong>\\nCaroline George\\n</strong>\\n<p>\\nGarbage Collection can be painful\\, impact performance and stability\\, and can even take down entire clusters. In this talk\\, we will start by going over the 5 most common reasons for GC in Apache Cassandra. Then we will discuss ways to address these issues. And end with how to monitor your cluster going forward to avoid running into GC problems.\\n</p>\\n\\n<p><em>\\nSpent over 6 years working with Apache Cassandra as an SE at Datastax\\, Caroline is now helping customers increase performance and provide stability with their JVM at Azul Systems. Originally from France\\, she has spent most of her life in NYC and holds a BA in Computer Science from NYU and MBA from NYU Stern School of Business.\\n</em></p>","categories":"Cassandra","url":"https://apachecon.com/acah2020/tracks/cassandra.html#T1735"},{"uid":"acah2020-cassandra-T1815@apachecon.com","sequence":"1","dtstamp":"20200810T213949Z","dtstart":"20200929T181500Z","dtend":"20200929T185500Z","summary":"Cloud-Native Cassandra","location":"@home","x-alt-desc":"<strong>\\nPatrick McFadin\\n</strong>\\n<p>\\nKubernetes is becoming a standard tool to deploy large scale infrastructure and lately\\, Apache Cassandra. We'll look at some of the methods used to deploy Cassandra using Kubernetes including storage options\\, networking configuration\\, and monitoring. In the past year\\, the Apache Cassandra project has also taken on the task of creating a common operator closer to the project. This will be a chance to get the latest status of the operator effort and where it will be headed post-Cassandra 4.0.\\n</p>\\n\\n<p><em>\\nPatrick McFadin is the VP of Developer Relations at DataStax\\, where he leads a team devoted to making users of Apache Cassandra successful. He has also worked as Chief Evangelist for Apache Cassandra and consultant for DataStax\\, where he helped build some of the largest and exciting deployments in production. Previous to DataStax\\, he was Chief Architect at Hobsons and an Oracle DBA/Developer for over 15 years.\\n</em></p>","categories":"Cassandra","url":"https://apachecon.com/acah2020/tracks/cassandra.html#T1815"},{"uid":"acah2020-cassandra-T1855@apachecon.com","sequence":"1","dtstamp":"20200810T213949Z","dtstart":"20200929T185500Z","dtend":"20200929T193500Z","summary":"Getting started with Cassandra the right way","location":"@home","x-alt-desc":"<strong>\\nErick Ramirez\\n\\n</strong>\\n<p>\\nCassandra users run into problems particularly when they're new to the technology. In this session\\, I'll talk about: - the common pitfalls so you don't fall into the trap\\; - top things users ask for help\\; - how to quickly diagnose issues\\; - where to get help.\\n</p>\\n\\n<p><em>\\nI'm an Apache Cassandra enthusiast at DataStax. I've been educating and helping other users become successful with Cassandra for 7 years. I answer questions on various channels including ASF Slack and the users mailing list.\\n</em></p>","categories":"Cassandra","url":"https://apachecon.com/acah2020/tracks/cassandra.html#T1855"},{"uid":"acah2020-cassandra-T1935@apachecon.com","sequence":"1","dtstamp":"20200810T213949Z","dtstart":"20200929T193500Z","dtend":"20200929T201500Z","summary":"Advanced data modeling techniques for Cassandra","location":"@home","x-alt-desc":"<strong>\\nArturo Hinojosa\\, Michael Raney\\n</strong>\\n<p>\\nWhether your storing timeseries data for a messaging app or device metadata for an industrial IoT application\\, your Cassandra data model can have a massive impact on your applications performance and scalability. In this talk\\, we will walk through advanced techniques and best practices for building highly scalable\\, fast\\, and robust data models. You will learn how to model your data based on your queries and access patterns to ensure you have well-distributed data that will enable your application to scale up as traffic grows. We will talk through examples of deformalizing data\\, modeling complex relationships\\, and optimizations that you can apply to your schemas and data models to improve performance.\\n</p>\\n\\n<p><em>\\nArturo Hinojosa:<br />\\nArturo Hinojosa is a Principal Product Manager on the Amazon Keyspaces (for Apache Cassandra) team at Amazon Web Services (AWS). Arturo is responsible for Amazon Keyspaces' overall product strategy and has been with AWS for over four years.<br />\\nMichael Raney:<br />\\nMichael is the lead specialist solution architect (SA) for Amazon Keyspaces (for Apache Cassandra). As the lead SA for Amazon Keyspaces\\, Michael works with customers every day to design cloud-based NoSQL solutions for large-scale distributed systems.\\n</em></p>\\n\\n\\n\\n<!-- Asia -->","categories":"Cassandra","url":"https://apachecon.com/acah2020/tracks/cassandra.html#T1935"},{"uid":"acah2020-cassandra-W0900@apachecon.com","sequence":"1","dtstamp":"20200810T213949Z","dtstart":"20200930T090000Z","dtend":"20200930T094000Z","summary":"Large scale Cassandra Use Cases and Best Practices at Huawei Consumer Cloud","location":"@home","x-alt-desc":"<strong>\\nDuican Huang\\n</strong>\\n<p>\\nCassandra is widely used in key business scenarios in Huawei Consumer Cloud. You can find Cassandra databases serving as the real-time data store behind almost all Huawei consumer electronic products that are used by billions of people in China and the rest of the world. With a long history of Cassandra adoption ever since 2010\\, Huawei Consumer Clouds Cassandra deployments have grown to 30\\,000+ nodes\\, supporting more than 10 million operations per second with average latency of 4ms\\, and the maximum number of table records reaches 300 billion. Along this journey\\, we have gained a lot of experience in data modeling\\, fine-tuning leveled compaction with high node density\\, day-to-day operations such as repair and handling tombstones\\, monitoring and problem identification and quick resolution under very tight SLA\\, which we are thrilled to share with the community. We also summarized our lessons learned and best practices in managing those low-latency\\, high-concurrency and mission-critical use cases.\\n</p>\\n\\n<p><em>\\nDuican Huang is a Huawei Senior R&D Engineer\\n</em></p>","categories":"Cassandra","url":"https://apachecon.com/acah2020/tracks/cassandra.html#W0900"},{"uid":"acah2020-cassandra-W0940@apachecon.com","sequence":"1","dtstamp":"20200810T213949Z","dtstart":"20200930T094000Z","dtend":"20200930T102000Z","summary":"Making Cassandra more capable\\, faster\\, and more reliable","location":"@home","x-alt-desc":"<strong>\\nHiroyuki Yamada\\, Yuji Ito\\n</strong>\\n<p>\\nCassandra is widely adopted in real-world applications and used by large and sometimes mission-critical applications because of its high performance\\, high availability and high scalability. However\\, there is still some room for improvement to take Cassandra to the next level. We have been contributing to Cassandra to make it more capable\\, faster\\, and more reliable by\\, for example\\, proposing non-invasive ACID transaction library\\, adding GroupCommitLogService\\, and maintaining and conducting Jepsen testing for lightweight transactions. This talk will present the contributions we have done including the latest updates in more detail\\, and the reasons why we made such contributions. This talk will be one of the good starting points for discussing the next generation Cassandra.\\n</p>\\n\\n<p><em>\\nHiroyuki Yamada:<br />\\nHiroyuki Yamada is CTO and CEO at Scalar\\, Inc. He has been passionate about parallel and distributed data management systems for more than 15 years. Prior to Scalar\\, he worked at IIS UTokyo\\, Yahoo\\, IBM. Ph.D. from the University of Tokyo.<br />\\nYuji Ito:<br />\\nWorking on distributed database/storage. Formerly\\, worked on SSD firmware. Master's degree in Information Science and Technology from The University of Tokyo.\\n</em></p>\\n\\n\\n\\n\\n<!-- NA/EU -->","categories":"Cassandra","url":"https://apachecon.com/acah2020/tracks/cassandra.html#W0940"},{"uid":"acah2020-cassandra-W1615@apachecon.com","sequence":"1","dtstamp":"20200810T213949Z","dtstart":"20200930T161500Z","dtend":"20200930T165500Z","summary":"How Netflix Manages Version Upgrades of Cassandra at Scale","location":"@home","x-alt-desc":"<strong>\\nSumanth Pasupuleti\\n</strong>\\n<p>\\nWe at Netflix have about 70% of our fleet on Apache Cassandra 2.1\\, while the remaining 30% is on 3.0. We have embarked on a multi quarter task of upgrading our 2.1 fleet to 3.0\\, as part of which we are doing several kinds of verification overarching both correctness and performance. It is a known issue that cross version streaming is not supported in Cassandra. To work around this\\, we've also developed a version agnostic upgrade mechanism using our desire based automation\\, to avoid needing to do cross version streaming. Through this approach\\, we can tolerate loosing a node while the upgrade is in progress and the cluster is in mixed mode of major versions. As part of this talk\\, I would like to elaborate on what kinds of verification we are doing as well as the upgrade mechanism we have developed to avoid cross version streaming.\\n</p>\\n\\n<p><em>\\nSumanth Pasupuleti is a Senior Software Engineer at Netflix\\, focusing on innovating and operating at scale\\, both caching and persistent datastore solutions like EVCache and Cassandra\\, offered as a platform within Netflix.\\n</em></p>","categories":"Cassandra","url":"https://apachecon.com/acah2020/tracks/cassandra.html#W1615"},{"uid":"acah2020-cassandra-W1655@apachecon.com","sequence":"1","dtstamp":"20200810T213949Z","dtstart":"20200930T165500Z","dtend":"20200930T173500Z","summary":"Hidden features of Apache Cassandra 4.0","location":"@home","x-alt-desc":"<strong>\\nDinesh Joshi\\n</strong>\\n<p>\\nApache Cassandra 4.0 is a huge community effort! It has over 400 patches including features and bug fixes. We have a lot of features that are well known and there are great features that are not so well known. In this talk\\, you will learn about some of those hidden features that might make your life easier\\, give you great performance boost or just surprise you!\\n</p>\\n\\n<p><em>\\nDinesh A. Joshi has been a professional Software Engineer for over a decade building highly scalable realtime Web Services and Distributed Streaming Data Processing Architectures serving over 1 billion devices. Dinesh is an active contributor to the Apache Cassandra codebase. He has a Masters degree in Computer Science (Distributed Systems & Databases) from Georgia Tech\\, Atlanta\\, USA.\\n</em></p>","categories":"Cassandra","url":"https://apachecon.com/acah2020/tracks/cassandra.html#W1655"},{"uid":"acah2020-cassandra-W1815@apachecon.com","sequence":"1","dtstamp":"20200810T213949Z","dtstart":"20200930T181500Z","dtend":"20200930T185500Z","summary":"Reasoning about Cassandra performance from first principles","location":"@home","x-alt-desc":"<strong>\\nJeff Hajewski\\n</strong>\\n<p>\\nThere are a plethora of articles and blog posts on Cassandra performance and performance tuning. Typically these resources contain specific pieces of advice on how to improve read or write throughput. The problem with these resources is that they focus on a specific solution to specific problem. In this talk we will start from first principles and develop a mental model that will allow us to reason about Cassandra's performance. The goal of the talk is for attendees to leave with a deeper understanding of how Cassandra works and how they can use that information to think through Cassandra's performance characteristics. We will start off by looking at how Cassandra stores data\\, the underlying data structures\\, and the implications of these design choices. The next two parts of the talk will discuss how Cassandra handles reads and writes and the associated trade-offs in the context of distributed systems. This talk is suitable both for those that regularly use Cassandra as well as those who are new to Cassandra because we focus on the ideas and principles behind Cassandra\\, rather than specific APIs or configurations.\\n</p>\\n\\n<p><em>\\nJeff is a software engineer at Salesforce\\, where he works on distributed systems for machine learning on streaming data. Prior to working at Salesforce he did his PhD at the University of Iowa. He works remotely from Iowa\\, where he lives with his wife\\, kid\\, and dog.\\n</em></p>","categories":"Cassandra","url":"https://apachecon.com/acah2020/tracks/cassandra.html#W1815"},{"uid":"acah2020-cassandra-W1855@apachecon.com","sequence":"3","dtstamp":"20200810T213949Z","dtstart":"20200930T185500Z","dtend":"20200930T193500Z","summary":"Cassandra with SSL and SOX on Docker","location":"@home","x-alt-desc":"<strong>\\nTejaswi Malepati\\,\\nAaron Ploetz\\n</strong>\\n<p>\\nAs the number of clusters to manage grows\\, it would become harder for operations and maintaining corresponding configurations across different clusters. At Target\\, we manage around 100+ clusters and thought of experimenting with docker as the entire world is moving towards containerization. Once we started migrating we had to develop certain tools for managing along with running certain Adhoc operations on them. We have a wide range of clusters with different data classifications\\, hence based on conditional categorization\\, integration of SSL and SOX was also achieved with only a single docker image across all the clusters along with different versions of Cassandra. With docker\\, our way of provisioning and maintaining has changed along with reduced resource effort for any of the mentioned tasks. For example\\, upgrades were pretty slick as everything was integrated to the single docker image being used\\, integrating SSL and SOX to existing clusters was slick with just changing a configuration parameter. This shows the journey of Cassandra standardization at Target which is spread across 5 different infrastructure domains with a minimum of 2 regions per domain.\\n</p>\\n\\n<p><em>\\nTejaswi Malepati is the Cassandra Tech Lead for Target. He has been instrumental in designing and building custom Cassandra integrations\\, including a web-based SQL interface and data validation frameworks between Oracle and Cassandra. Tejaswi earned a master's degree in computer science from the University of New Mexico\\, and a bachelor's degree in electronics and communication from Jawaharlal Nehru Technological University in India. He is passionate about identifying and analyzing data patterns in datasets using R\\, Python\\, Spark\\, Cassandra\\, and MySQL.\\n</em></p>","categories":"Cassandra","url":"https://apachecon.com/acah2020/tracks/cassandra.html#W1855"},{"uid":"acah2020-cassandra-W1935@apachecon.com","sequence":"1","dtstamp":"20200810T213949Z","dtstart":"20200930T193500Z","dtend":"20200930T201500Z","summary":"Cassandra Upgrade in production : Strategies and Best Practices","location":"@home","x-alt-desc":"<strong>\\nLaxmikant Upadhyay\\n</strong>\\n<p>\\nThis session will cover how to perform Cassandra cluster upgrade in production effectively. We will learn about best practices for planning & executing Cassandra upgrades. We will also discuss and understand different Cassandra upgrade strategies and their respective pros & cons so that Operations team can select the appropriate strategy. Finally\\, we will talk about standard upgrade issues and how we have created custom solutions at Ericsson to overcome those issues. The session is useful for Cassandra Operators\\, Administrators and other Cassandra users involved in planning\\, performing and testing upgrades.\\n</p>\\n\\n<p><em>\\nLaxmikant Upadhyay is an Apache Cassandra enthusiast with over 10 years of experience in developing mutliple distributed scalable and HA software solutions. Currently\\, he works as Sr. Data engineer (nosql) and Cassandra SME with American Express R&D. He is core contributor of open source Cassandra Audtiing plugin ecaudit . He has designed and implemented multiple distributed\\, fault tolerant\\, scalable and HA software systems. He has helped many teams in designing efficient and scalable data model and performance tuning of C*.\\n</em></p>","categories":"Cassandra","url":"https://apachecon.com/acah2020/tracks/cassandra.html#W1935"},{"uid":"acah2020-cassandra-R1615@apachecon.com","sequence":"1","dtstamp":"20200810T213949Z","dtstart":"20201001T161500Z","dtend":"20201001T165500Z","summary":"Getting Involved with the Apache Cassandra Project","location":"@home","x-alt-desc":"<strong>\\nEkaterina Dimitrova\\n</strong>\\n<p>\\nThey say its always hard the first time you do something. Is it really? In this talk we prove the opposite and give some guidance on how to simplify the way new open-source contributors learn and contribute for the first time to a project like Apache Cassandra. Contributions can happen in many forms\\, from documentation\\, testing and bug fixing to developing new cool features. Come\\, join us in our exciting adventure to Cassandra 4.0\\, the most stable release ever\\, and beyond to 5.0!\\n</p>\\n\\n<p><em>\\nCassandra contributor and distributed systems deva.\\n</em></p>","categories":"Cassandra","url":"https://apachecon.com/acah2020/tracks/cassandra.html#R1615"},{"uid":"acah2020-cassandra-R1655@apachecon.com","sequence":"1","dtstamp":"20200810T213949Z","dtstart":"20201001T165500Z","dtend":"20201001T173500Z","summary":"Containerized Cassandra Cluster (CCC)","location":"@home","x-alt-desc":"<strong>\\nStanislav Kelberg\\n</strong>\\n<p>\\nElegant and fully controllable Cassandra cluster for local testing and development. A modern and robust alternative to ccm (Cassandra Cluster Manager)\\, taking advantage of containers\\, while keeping the full control of Cassandra configuration. This talk will demonstrate how to easily test locally against a cluster with production like features\\, for example: multi DC\\, SSL\\, Authentication etc.\\n</p>\\n\\n<p><em>\\nStan is a seasoned DevOps engineer who has worked for small startups and large enterprises like Deutsche Bank and Sky. Stan has been heavily involved with Cassandra and DSE in the last 6 years\\, 4 of which he has worked for digilalis.io\\n</em></p>","categories":"Cassandra","url":"https://apachecon.com/acah2020/tracks/cassandra.html#R1655"},{"uid":"acah2020-cassandra-R1735@apachecon.com","sequence":"1","dtstamp":"20200810T213949Z","dtstart":"20201001T173500Z","dtend":"20201001T181500Z","summary":"Re-imaging Cassandra authentication using short-term credentials","location":"@home","x-alt-desc":"<strong>\\nArturo Hinojosa\\, Derek Chen-Becker\\, Brian Houser\\n</strong>\\n<p>\\nApache Cassandra manages access by using traditional usernames and passwords. However\\, organizations and developers are moving towards more secure access management techniques for programmatic access\\, such as using short-term credentials. In this talk\\, we will dive deep on how Amazon Web Services (AWS) designed and built an open-source authentication plugin for Cassandra drivers that enables developers to use short term credentials for access management instead of hard-coding credentials in their application code. You will learn how the plugin integrates with Cassandra drivers and how the security model works in comparison to traditional authentication.\\n</p>\\n\\n<p><em>\\nArturo Hinojosa:<br />\\nArturo Hinojosa is a Principal Product Manager on the Amazon Keyspaces (for Apache Cassandra) team. Arturo is responsible for the overall product strategy of Amazon Keyspaces and has been with Amazon Web Services (AWS) for over four years.<br />\\nDerek Chen-Becker:<br />\\nDerek Chen-Becker is a senior software development engineer on the Amazon Keyspaces (for Apache Cassandra) team. Derek is the original author of the AWS authentication plugin for Apache Cassandra drivers. Derek is interested in network engineering and enterprise software development\\, with focuses in distributed systems\\, monitoring and management.<br />\\nBrian Houser:<br />\\nBrian Houser is a Senior Software Development Engineer on the Amazon Keyspaces (for Apache Cassandra) team. Brian leads open-source efforts for Amazon Keyspaces and has been with Amazon for more than 10 years.\\n</em></p>","categories":"Cassandra","url":"https://apachecon.com/acah2020/tracks/cassandra.html#R1735"},{"uid":"acah2020-cassandra-R1815@apachecon.com","sequence":"1","dtstamp":"20200810T213949Z","dtstart":"20201001T181500Z","dtend":"20201001T185500Z","summary":"Upgrading Cassandra using Automation\\, with cstar","location":"@home","x-alt-desc":"<strong>\\nValerie Parham-Thompson\\n</strong>\\n<p>\\nI recently did an upgrade of 200+ nodes of Cassandra across multiple environments sitting behind multiple applications using the cstar tool. We chose the cstar tool because\\, out of all automation options\\, it has topology awareness specifically to Cassandra. I will share my experience with this upgrade\\, including observations and surprises\\, as well as a walk-through of the process using a Cassandra cluster provisioned in Docker.\\n</p>\\n\\n<p><em>\\nWith experience as an open-source DBA and developer for software-as-a-service environments\\, Valerie has expertise in web-scale data storage and data delivery\\, including MySQL\\, Cassandra\\, Postgres\\, and MongoDB.\\n</em></p>","categories":"Cassandra","url":"https://apachecon.com/acah2020/tracks/cassandra.html#R1815"},{"uid":"acah2020-cassandra-R1855@apachecon.com","sequence":"2","dtstamp":"20200810T213949Z","dtstart":"20201001T185500Z","dtend":"20201001T193500Z","summary":"Hadoop as a Cassandra SSTables producer","location":"@home","x-alt-desc":"<strong>\\nSerban Teodorescu\\, Adelina Vidovici\\n</strong>\\n<p>\\nWere using a lambda architecture\\, with Hadoop used for the main database and Cassandra deployed as persistent cache at edges\\, in total about 7-800 Cassandra nodes. One issue is daily push of data from Hadoop to Cassandra\\, which is the main factor that impacts the clusters performance and costs. We used to produce JSON data in Hadoop\\, then convert it to SSTables at the edges and streaming them to Cassandra. Ill show why this architecture is unable to take advantage of Cassandra 4 streaming improvements\\, why is that important for us\\, how to combine Hadoop with Cassandra vnodes in order to achieve optimal streaming\\, and show some (preliminary) performance figures. The later is work in progress\\, but I hope it will be finished by the time the conference is startin\\n</p>\\n\\n<p><em>\\nSerban Teodorescu<br />\\nI'm at SRE at Adobe\\, part of a small team that manages 30+ Cassandra clusters for Adobe Audience Manager. Previously\\, I was a Python programmer\\, and I'm still trying to find out how a software developer who preferred SQL databases ended up as an SRE for a Cassandra team\\, and then started to work in Java.<br />\\nAdelina Vidovici<br />\\nI'm Software Engineer in Adobe Romania with a background in Computer Science and a big passion for Chemistry.\\nIn the last 2.5 years\\, I was part of the Adobe Audience Manager team and Ive got the chance to learn and work with Big Data technologies: Trust me! We have cookies! :)\\nBesides work\\, I enjoy reading\\, travelling and going for a bike ride from time to time.\\n</em></p>","categories":"Cassandra","url":"https://apachecon.com/acah2020/tracks/cassandra.html#R1855"},{"uid":"acah2020-cassandra-R1935@apachecon.com","sequence":"0","dtstamp":"20200828T190237Z","dtstart":"20201001T193500Z","dtend":"20201001T201500Z","summary":"Truth Hurts: How to Migrate your Data Model to Apache Cassandra","location":"@home","x-alt-desc":"<strong>\\nAmanda Moran\\n</strong>\\n<p>\\nI just took a DNA test\\, and it turns out my data model is 100% wrong. This session will focus on how to correctly data model for Apache Cassandra and NoSQL databases. Topics will include: - A brief comparison of relational databases and NoSQL databases - The benefits of Apache Cassandra - Transitioning a relational data model to a Cassandra data model - Common issues that can be solved with a good data model This session is intended for folks new to Cassandra/NoSQL or folks transitioning from operations to a more data engineering and cloud-focused role.\\n</p>\\n\\n<p><em>\\nAmanda has been an committer and PMC member for Apache Trafodion since 2015. She was previously a Developer Advocate with DataStax where she spent many\\, many hours helping users get better with Apache Cassandra.\\n</em></p>","categories":"Cassandra","url":"https://apachecon.com/acah2020/tracks/cassandra.html#R1935"},{"uid":"acah2020-community-T0930@apachecon.com","sequence":"1","dtstamp":"20200810T143451Z","dtstart":"20200929T093000Z","dtend":"20200929T101000Z","summary":"Apache Software Foundation and The Apache Way (in Hindi language) [ALC Indore]","location":"@home","x-alt-desc":"<strong>Swapnil M Mane</strong>\\n\\n<p>\\nThis talk will be part of Track prepared by ALC Indore\\, and *language for the talk will be Hindi*. In this session\\, will speak about Apache Software Foundation\\, and about the Apache Way. # Apache Software Foundation -- History -- Projects -- How Apache project works? -- Apache Project Ecosystem # The Apache Way -- Community - over code -- Merit - recognizing your work -- Communication - how communities communicate -- Open Development -- Decision Making - Consensus \\n</p>\\n\\n<p><em>\\nAn open-source enthusiast\\, and promoter. -- Apache Software Foundation Member -- Apache Central Services / Editorial Member -- Founder & Chair\\, Apache Local Community (ALC) -- PMC Member\\, Apache Community Development\\, OFBiz\\, Roller -- Founder Vue.js Indore community Skilled in E-commerce\\, Order Management System\\, Omni-Channel\\, and PWA strategy. Strong information technology professional with the intensive experience of building enterprise-grade applications. \\n</em>\\n</p>","categories":"Community","url":"https://apachecon.com/acah2020/tracks/community.html#T0930"},{"uid":"acah2020-community-T1010@apachecon.com","sequence":"1","dtstamp":"20200810T143451Z","dtstart":"20200929T101000Z","dtend":"20200929T105000Z","summary":"InnerSource updates in China","location":"@home","x-alt-desc":"<strong>Jerry Tan</strong>\\n\\n<p>\\nThere are more and more projects donated to Apache foundation from China in recent years. So more engineers are familiar with Apache Way. InnerSource is adopting Apache Way with an organization. And then More companies are beginning their InnerSource Journey. In this talk\\, I will give a brief update of InnerSource adoption in China\\, including some company's practices. Some are using it to build engineer culture\\, some are using it as a tool to remove duplicate wheels. InnerSource is a long journey\\, but I am happy to see that Chinese companies are more willing to embrace Open Source more deeply.\\n</p>\\n\\n<p><em>\\nCommitter of apache.org\\, mozilla.org\\, gnome.org PPMC of apache brpc incubating project\\, InnerSourceCommon Foundation member\\, InnerSource advocator in China\\, more than 20 years of Open Source Experience  \\n</em>\\n</p>","categories":"Community","url":"https://apachecon.com/acah2020/tracks/community.html#T1010"},{"uid":"acah2020-community-T1050@apachecon.com","sequence":"1","dtstamp":"20200810T143451Z","dtstart":"20200929T105000Z","dtend":"20200929T113000Z","summary":"Apache Pulsar: a borderless community","location":"@home","x-alt-desc":"<strong>Jennifer Huang</strong>\\n\\n<p>\\nApache Pulsar is an open-source distributed pub-sub messaging system originally created at Yahoo and now part of the Apache Software Foundation(ASF). It is a multi-tenant\\, high-performance solution for server-to-server messaging. After graduation from ASF\\, the community grows bigger and stronger\\, with more and more users and contributors. This presentation shares how Apache Pulsar develops a borderless community in a short period.\\n</p>\\n\\n<p><em>\\nJennifer Huang is an Apache Pulsar committer and a senior technical writer at StreamNative. She contributes to Apache Pulsar documentation and community development proactively. She is dedicated to growing the Apache Pulsar community globally. \\n</em>\\n</p>","categories":"Community","url":"https://apachecon.com/acah2020/tracks/community.html#T1050"},{"uid":"acah2020-community-T1615@apachecon.com","sequence":"1","dtstamp":"20200810T143451Z","dtstart":"20200929T161500Z","dtend":"20200929T165500Z","summary":"The Apache Way","location":"@home","x-alt-desc":"<strong>Kevin A. McGrail</strong>\\n\\n<p>\\nThe Apache Way is how the Apache Software Foundation works. It's a collection of tribal knowledge and stories that loosely define how we work. Not all of it is intuitive but it does work. In 21 years we have changed the way computing around the world happens through our mission to provide open source software to the world and doing so at no charge! Want to work more effectively with the foundation? Want to model our leadership? Want to bring a new project under our umbrella? Come learn more about the Apache Way! Kevin A. McGrail has served in a number of roles at the foundation as member\\, as a chairperson\\, in the incubator\\, as a mentor\\, in the treasury and in fundraising. He will talk about his experience and some of the mistakes he's made too.\\n</p>\\n\\n<p><em>\\nKevin A. McGrail Director of Business Growth\\, InfraShield https://www.linkedin.com/in/kmcgrail\\, kmcgrail@infrashield.com Kevin A. McGrail\\, aka KAM\\, is Director of Business Growth @ InfraShield.com doing cyberphysical security for critical infrastructure. Kevin loves Open Source Software and is a member of the Apache Software Foundation. He is a cyber security and privacy expert\\, and his research protects millions of Internet users every day. He is an advisor for SecurityUniversity.edu & Virtru.com as well as a Director at the Dysautonomia Support Network and The McGrail Foundation. His latest honor is becoming a member of the U.S. Marine Corps Cyber Auxiliary. Kevin has spoken all over the United States and worldwide in Canada\\, Germany\\, Belgium\\, Sweden & China on Open Source Software\\, the Cloud & Cybersecurity. \\n</em>\\n</p>","categories":"Community","url":"https://apachecon.com/acah2020/tracks/community.html#T1615"},{"uid":"acah2020-community-T1655@apachecon.com","sequence":"1","dtstamp":"20200810T143451Z","dtstart":"20200929T165500Z","dtend":"20200929T173500Z","summary":"Open Source changes the world!","location":"@home","x-alt-desc":"<strong>\\nBertrand Delacretaz\\n</strong>\\n\\n<p>\\nIn a world that's increasingly digital\\, Open Source software is everywhere: in your phone\\, your elevator\\, your car\\, behind your bank account...more than ever\\, Open Source is at the heart of our world. Beyond these very concrete contributions to our society's well being\\, Open Source communities have also helped design innovative collaboration techniques\\, especially around remote and distributed work. Often running without a formal boss and without a formal schedule\\, Open Source communities consistently produce software that's of great quality\\, sometimes world-changing. COVID-19 has prompted many companies and organizations to speed up their transition to digital transformation and distributed collaboration. This talk will show what Open Source communities can bring to this new world\\, based on a number of concrete example where simple tools and techniques make all the difference in terms of digital and remote collaboration.\\n</p>\\n\\n<p><em>\\nBertrand Delacretaz works as a Principal Scientist for Adobe in Basel\\, Switzerland. He's involved in software design and development for Adobe Experience Cloud products\\, which use many open source modules\\, mostly from Apache projects to which his teams contribute extensively. Bertrand is currently (2020-2021) on his eleventh term as a member of the Apache Software Foundation's Board of Directors and has been active in the Foundation for about 20 years.\\n</em></p>","categories":"Community","url":"https://apachecon.com/acah2020/tracks/community.html#T1655"},{"uid":"acah2020-community-T1735@apachecon.com","sequence":"99","dtstamp":"20200810T143451Z","dtstart":"20200929T173500Z","dtend":"20200929T181500Z","summary":"Teaching Open Source","location":"@home","x-alt-desc":"<strong>\\nDaniel Ruggeri\\n</strong>\\n<p>\\nHow did you learn about Open Source? Did you learn from a mentor? Did you learn from a colleague? Did you learn through hard fought experience? For a lot of us\\, we learned the hard way. But... what if all of the concepts\\, the tools\\, the licensing\\, the methods\\, and the terms were gathered into a course? Come join our presenter as he walks through his motivations and experience designing and teaching a college-level course about Open Source. We will discuss how to get started\\, what the curriculum includes\\, how you could deliver such a course\\, and other tips and tricks.\\n</p>\\n\\n<p><em>\\nDaniel is Vice President of Middleware at Mastercard and an Open Source evangelist. Responsible for setting the direction of Mastercard regarding the Web and Cloud space\\, he spends his days and nights playing with infrastructure and the code that powers it both inside the firewall and outside. He is a member of the Apache Software Foundation and has contributed code to Open Source projects from simple pet projects to widely utilized servers. As a lover of Open Source\\, he even taught a course about Open Source Software Development (and will share the curriculum with you!). He has spoken at several conferences about expanding Open Source in enterprises\\, introducing Open Source\\, and growing understanding of Open Source in education.\\n</em></p>","categories":"Community","url":"https://apachecon.com/acah2020/tracks/community.html#T1735"},{"uid":"acah2020-community-T1815@apachecon.com","sequence":"1","dtstamp":"20200810T143451Z","dtstart":"20200929T181500Z","dtend":"20200929T185500Z","summary":"From no open source experience to Apache Member and PMC of Commons","location":"@home","x-alt-desc":"<strong>\\nRob Tompkins\\n</strong>\\n<p>\\nBetween 2016 and 2019 I went from having not made any Apache contributions to being on the PMC of Apache Commons. I will tell my story here and give insights about how the Apache Way works. This talk has been previously given at the DC Apache Roadshow with a positive response.\\n</p>\\n\\n<p><em>\\nI am a software developer and mathematician from Richmond\\, Virginia who happens to be an Apache Member and am on the PMC of Apache Commons. I also particularly enjoy outdoor adventure sports for those interested.\\n</em></p>","categories":"Community","url":"https://apachecon.com/acah2020/tracks/community.html#T1815"},{"uid":"acah2020-community-T1855@apachecon.com","sequence":"1","dtstamp":"20200810T143451Z","dtstart":"20200929T185500Z","dtend":"20200929T193500Z","summary":"The Myth of Culture","location":"@home","x-alt-desc":"<strong>\\nKen Coar\\n</strong>\\n<p>\\nIn today's increasingly connected world\\, the word \\\"culture\\\" appears in communities a lot. Unfortunately\\, it's not a single-value term. In many cases the founders of a culture are unaware of how their ideals have morphed\\, and acquired altered or additional definitions. In this talk I intend to describe some of the factors that contribute to this sort of 'fuzzing' of consensual understanding of the term. \\n</p>\\n\\n<p><em>\\nKen Coar\\, an open sourcerer and opinionist\\, has written code for 40+ years. He was one of the founders of The Apache Software Foundation\\, served on its board of directors for years\\, and was responsible for the ApacheCon conferences for several years as well. He also served on the board of the Open Source Initiative. Currently he prefers to write code in Ruby\\, but has contributed to rubygems\\, CPAN\\, PHP\\, and Apache httpd.\\n</em></p>","categories":"Community","url":"https://apachecon.com/acah2020/tracks/community.html#T1855"},{"uid":"acah2020-community-T1935@apachecon.com","sequence":"0","dtstamp":"20200928T141406Z","dtstart":"20200929T193500Z","dtend":"20200929T201500Z","summary":"The State of D&I at the ASF","location":"@home","x-alt-desc":"<strong>\\nAnita Sarma\\, \\nDaniel Izquierdo\\,\\nGriselda Cuevas\\,\\nMariam Guizani\\n</strong>\\n<p>\\nIn this talk we'll talk about the research efforts the D&I committee has been working on for the past year. We'll talk with our researchers and will deep dive into the insights and results we obtained from the three phases of our work: The ASF Community Survey\\, the D&I research interviews and our quantitative analysis with Bitergia's technology. We'll also have an opportunity to ask questions to the researchers and the team behind this effort. \\n</p>\\n\\n<p><em>\\nAnita Sarma<br />\\nAnita Sarma is an Associate Professor at Oregon State University. Before this she was an Assistant Professor at University of Nebraska\\, Lincoln\\; a post-doctoral scholar at Carnegie Mellon University\\, and a doctoral student at University of California\\, Irvine. Through this journey her passion has been on helping humans make better software and work together. A primary focus of her research is in facilitating onboarding of newcomers and increasing diversity in open source projects. Overall\\, Dr. Sarmas research has resulted in more than 100 peer-reviewed publications an several best paper records and the NSF CAREER award. Her work has been regularly funded through the National Science Foundation and Airforce (AFOSR). <br />\\n\\nDaniel Izquierdo<br />\\nDaniel Izquierdo Cortazar is a researcher and one of the founders of Bitergia\\, a company that provides software analytics for open source ecosystems. Currently the chief data officer at Bitergia\\, he is focused on the quality of the data\\, research of new metrics\\, analysis\\, and studies of interest for Bitergia customers via data mining and processing. Daniel holds a PhD in free software engineering from the Universidad Rey Juan Carlos in Madrid\\, where he focused on the analysis of buggy developers activity patterns in the Mozilla community.<br />\\nDaniel is an active member of the CHAOSS community at the D&I and GrimoireLab working groups as well as an active member of the InnerSource Commons.\\n\\nGriselda Cuevas<br />\\n\\nGriselda is the V.P. of D&I at the ASF and also a product manager in Google Cloud. She has 13 years of experience in a variety of industries\\, from oil and gas to cloud computing. She has a Masters in Operation Research and Data Science from UC Berkeley and is passionate about data engineering\\, open source technology\\, information architecture\\, diversity and inclusion in tech & Italian wines.\\nIn her spare time she reads and works on diversity and inclusion topics\\, specially around building frameworks that enable participation of under represented groups in tech.\\n \\nMariam Guizani\\nMariam Guizani is a PhD student in Computer Science at Oregon State University. Her research area is in Human-Computer Interaction and Software Engineering. She studies inclusivity in open source environments with a focus on supporting cognitive diversity from the tool perspective. She holds a Master of Science in Computer Science from Oregon State University and was a recipient of the Fulbright scholarship in 2016.\\n</em></p>","categories":"Community","url":"https://apachecon.com/acah2020/tracks/community.html#T1935"},{"uid":"acah2020-community-W0900@apachecon.com","sequence":"2","dtstamp":"20200810T143451Z","dtstart":"20200930T090000Z","dtend":"20200930T094000Z","summary":"building one active internal opensource community is crucial for InnerSource","location":"@home","x-alt-desc":"<strong>Jerry Tan</strong>\\n\\n<p>\\nInnerSource is the use of Apache Way within an organization. it needs both high-level support and internal engineers community support. Sometimes\\, high-level support is easy to get and implemented\\, but internal engineers' community support is much hard\\, it means culture shift. So building one active internal open source community is very important. As OSPO of Baidu\\, I adopt InnerSource inside Baidu for more than 4 years. I will talk about how I build the internal community in my company\\, including how I plan\\, how I implement it. I need to attract the target people\\, set small tasks to let them participate and contribute\\, and incentivize them. InnerSource is a long journey\\, but it is worth the investment. \\n</p>\\n\\n<p><em>\\nCommitter of apache.org\\, mozilla.org\\, gnome.org PPMC of apache brpc incubating project\\, InnerSourceCommon Foundation member\\, InnerSource advocator in China\\, more than 20 years of Open Source Experience\\n</em>\\n</p>","categories":"Community","url":"https://apachecon.com/acah2020/tracks/community.html#W0900"},{"uid":"acah2020-community-W0940@apachecon.com","sequence":"1","dtstamp":"20200810T143451Z","dtstart":"20200930T094000Z","dtend":"20200930T102000Z","summary":"Apache Local Community (in Hindi language) [ALC Indore]","location":"@home","x-alt-desc":"<strong>Priya Sharma</strong>\\n\\n<p>\\nThis talk will be part of Track prepared by ALC Indore\\, and *language for the talk will be Hindi* Apache Local Community (ALC) is an initiative by the Apache Community Development project. ALC comprises local groups of Apache (Open Source) enthusiasts\\, called an 'ALC Chapter'. For details please refer https://s.apache.org/alc The talk will include the details on introduction\\, Present State and next plans of ALC ## Introduction About ALC ALC Roles and Responsibilities Benefits of ALC How to apply to set up ALC Chapter Code of conduct ALC Resources Addition information Contact ALC ## Present State Current ALC Chapters (Indore\\, Beijing\\, Warsaw\\, Budapest\\, and others..) Activities and health of these chapters ## Beyond (Next Steps) Establishing new ALCs and future roadmap. How to participate in this initiative. More details on ALC can be found at -- https://s.apache.org/alc The following will be the take away from the session: -- What is ALC and how to participate in this initiative.\\n</p>\\n\\n<p><em>\\nCore member\\, Apache Local Community Indore Chapter Contributor to Apache Projects since 2017\\, majorly contributed in OFBiz\\, and Community Development project.  \\n</em>\\n</p>","categories":"Community","url":"https://apachecon.com/acah2020/tracks/community.html#W0940"},{"uid":"acah2020-community-W1615@apachecon.com","sequence":"1","dtstamp":"20200810T143451Z","dtstart":"20200930T161500Z","dtend":"20200930T165500Z","summary":"Apache Local Community (ALC): Present & Beyond","location":"@home","x-alt-desc":"<strong>Swapnil M Mane</strong>\\n\\n<p>\\nApache Local Community (ALC) is an initiative by the Apache Community Development project. ALC comprises local groups of Apache (Open Source) enthusiasts\\, called an 'ALC Chapter'. For details please refer https://s.apache.org/alc The session will be majorly on two topics: #1.) How the Apache Software Foundation provides the opportunity to flourish your idea. I shared the initial ALC idea to the community around mid-2019\\, from there with the great inputs from the community and mentors\\, we have given the shape to the idea. It is a great example\\, how the community can help you to transform and enhance your idea to match global standards. #2.) Introduction\\, Present State and next plans of ALC ## 2.1 Introduction About ALC ALC Roles and Responsibilities Benefits of ALC How to apply to set up ALC Chapter Code of conduct ALC Resources Addition information Contact ALC ##2.2) Present State Current ALC Chapters (Indore\\, Beijing\\, Warsaw\\, Budapest\\, and others..) Activities and health of these chapters ##2.3) Beyond (Next Steps) Establishing new ALCs and future roadmap. How to participate in this initiative. More details on ALC can be found at -- https://s.apache.org/alc -- https://s.apache.org/alc-code-of-conduct -- https://s.apache.org/alc-guidelines -- https://s.apache.org/alc-chapters -- https://s.apache.org/alc-reports -- https://s.apache.org/establish-alc-chapter The following will be the take away from the session: -- How community engagement can help in improvising and implementing your idea. -- What is ALC and how to participate in this initiative.\\n</p>\\n\\n<p><em>\\nAn open-source enthusiast\\, and promoter. -- Apache Software Foundation Member -- Apache Central Services / Editorial Member -- Founder & Chair\\, Apache Local Community (ALC) -- PMC Member\\, Apache Community Development\\, OFBiz\\, Roller -- Founder Vue.js Indore community Skilled in E-commerce\\, Order Management System\\, Omni-Channel\\, and PWA strategy. Strong information technology professional with the intensive experience of building enterprise-grade applications.\\n</em>\\n</p>","categories":"Community","url":"https://apachecon.com/acah2020/tracks/community.html#W1615"},{"uid":"acah2020-community-W1655@apachecon.com","sequence":"1","dtstamp":"20200810T143451Z","dtstart":"20200930T165500Z","dtend":"20200930T173500Z","summary":"Growing with the Open-Source Community","location":"@home","x-alt-desc":"<strong>\\nTomasz Urbaszek\\n</strong>\\n\\n<p>\\nDuring this talk\\, I want to share lessons I've learned as a young engineer contributing to an open-source project. Those include demystifying the stereotype of OSS contributors\\, \\\"community over code\\\" approach as well as understanding the life cycle and funding of projects. But the most important lesson is why young people should join open source communities early in their careers. They can gain tremendous experience which is quite often out of their reach when working on commercial projects. Also encouraging young people to join OSS project allow us as communities to validate our contribution guides and check if we create a really welcoming environment.\\n\\n<p><em>\\nTomek is a software engineer at Polidea and Apache Airflow committer. He is an open-source enthusiast and chapter lead of ALC Warsaw. Book and philosophy lover with a big interest in financial markets. Tomek is a maths graduate from Warsaw University of Technology. \\n</em></p>","categories":"Community","url":"https://apachecon.com/acah2020/tracks/community.html#W1655"},{"uid":"acah2020-community-W1735@apachecon.com","sequence":"2","dtstamp":"20200810T143451Z","dtstart":"20200930T173500Z","dtend":"20200930T181500Z","summary":"Serve\\, Lead\\, Succeed the Open (mindful) Way to Prevent/Reverse Burnout in Boardrooms\\,","location":"@home","x-alt-desc":"<strong>\\nPrashant V. Joshi\\n</strong>\\n<p>\\nAbstract: Open Source SW (OSS) community has been brilliant in bringing the leadership out of everyone to innovate\\, contribute and transform. Covid-19\\, has brought one more challenge to the OSS community.This unique experiential talk inspires and opens minds of new and seasoned OSS community members at large to become successful servant-leaders through self-care coping mechanisms to prevent and alleviate burnout in boardrooms\\, classrooms and home-rooms in the midst of this pandemic and beyond. Yes you can! === Description: This talk engages the audience with simple\\, practical\\, scientific\\, rational and original ideas. It uses open and mindfulness principles so that every mind opens up (body too!)\\, and gets inspired towards self-transformation. Whether you are a seasoned OSS guru or a novice/curious aspirant\\, this talk is for you. Why? A closed mind is a dangerous thing that creates toxic leadership for oneself and others. Misery loves company is a saying we love to use. Data shows that Rudeness costs millions to companies. Burnout is officially a disease according to World Health Organization. Depression costs lives. In a 2019 World Happiness Report surveying 156 countries\\, Finland was #1 (home of some Open Source pioneers) while the US was #19. So what? Time to shift the paradigm. It is mid-2020 with a re-surging Covid-19 pandemic and it is about time to bring clarity to our vision (pun intended!) so happiness loves company too - becomes a new phrase to live by. Rudeness is expensive\\, civility is NOT is another line to live by too. Well\\, how\\, you ask? Through a short presentation filled with scientific data\\, we will define the science of leadership\\, outline unique attributes of servant leadership\\, and give examples of the same. We will end with a mindful experiential component to have fun and begin the science of self-transformation. Yes\\, we can transform the OSS community together with better (open=mindful) leaders\\, awesome innovation\\, funding\\, and quality of life for all. Thank you\\n</p>\\n\\n<p><em>\\nPrashant V. Joshi M.A. M. Phil\\, E-RYT 500\\, C-IAYT\\, YACEP (public speaker\\, published author) is an electrical engineer and a computer scientist. He is an outcome-focused management & technology executive\\, open source evangelist/alliances-builder\\, educator\\, master coach/therapist and a social entrepreneur with over 30 years in the US with many global for-profit and non-profit initiatives. His mantra for success is Grow PBT (People\\, Business\\, Technology). Presently he is an executive advisor for an early-stage startup (Sukhi Inc) in 'culturally aware mental wellness'. He is also a Global Ambassador to a non-profit institution in India (one of the oldest Yoga Therapy Research/Rehab Center) for raising funds for expanding a rehab/research center for cancer and other life-style diseases. Most recently he was the Vice President of Global Alliances for MariaDB Foundation\\, an open source project founded by Monty Widenius (creator of MYSQL). Over the past 5 years Prashant has advised many early and mature startups with his big brand and PBT acumen. Prashant is a co-founder of Gurukul\\, LLC\\, A Science of Living Institution serving global communities with evidence-based yoga/holistic healing which is in its 20th year. In 2016 he co-founded Food Yogini for advocating eco-friendly foods and goods. He has brought unique leadership coaching\\, Yoga/wellness into boardrooms\\, classrooms and home-rooms over the past 25+ years in NYC\\, NJ\\, TX and globally. He is a published author and a motivational public speaker. He lives in Austin\\, TX with his wife Manju\\, daughters Veda and Illa. He loves sports and travel and one of his tag-lines is Billions Yet To Be Served... Education: EE w honors from Bombay University and Double Masters in Computer Science from CUNY Wellness Certifications: E-RYT 500 (experienced registered Yoga teacher w over 10\\,000 hours of teaching)\\, C-IAYT (certified International Yoga Therapist (w over 5\\,000 hours of therapeutic practice)\\, YACEP (Yoga Alliance Continuing Education Provider)  \\n</em></p>","categories":"Community","url":"https://apachecon.com/acah2020/tracks/community.html#W1735"},{"uid":"acah2020-community-W1815@apachecon.com","sequence":"2","dtstamp":"20200810T143451Z","dtstart":"20200930T181500Z","dtend":"20200930T185500Z","summary":"Lightweight Open Source","location":"@home","x-alt-desc":"<strong>\\nIssac Goldstand\\n</strong>\\n<p>\\nEver think about contributing to the Open Source world\\, but worried that - for some reason or another - its not up to par with playing with the big boys and girls? Thats BS! Come learn why *every* contribution in the Open Source world can be helpful! As an active participant in the Open Source community for over twenty years\\, I am constantly running into situations where prospective newcomers to the community feel daunted and undervaluating themselves and/or their contributions. In this session Ill talk about the importance of that exact sort of small contribution in some of the biggest Open Source communities and projects out there. Because sometimes a sub-domain expert is *much* more vital than yet another domain expert.\\n</p>\\n\\n<p><em>\\nIssac has been involved in the Web community for nearly 20 years. With a strong background in the Apache Web Server internals\\, and optimizing web applications\\, Issac continues to churn out highly optimized web applications in a variety of languages and servers\\, as well as mentoring teams of programmers to be as passionate about writing great software as he is. Today Issac is married with four kids\\, and is launching a start-up to chase his long time dream: turning smart homes into a day-to-day \\\"taken for granted\\\" reality. In his spare time\\, he still spends time volunteering in the tech community and mentoring hi-tech teams across a diverse range of languages and disciplines.\\n</em></p>","categories":"Community","url":"https://apachecon.com/acah2020/tracks/community.html#W1815"},{"uid":"acah2020-community-W1855@apachecon.com","sequence":"1","dtstamp":"20200810T143451Z","dtstart":"20200930T185500Z","dtend":"20200930T193500Z","summary":"The Apache Way: Practical Open Source Project Management","location":"@home","x-alt-desc":"<strong>\\nShane Curcuru\\n</strong>\\n<p>\\nThe Apache Way is useful for organizations and individuals to be more effective at working in distributed communities. Open source software does not necessarily mean open development - and true community-led open development is where the fun starts in working in FOSS! There are a lot more aspects to consider and areas to invest in as you move forward through the open source journey. These are just the starting points to work on. A key reminder: open source works best when you're working with softare that you actually use. Taking the time to choose which teams or projects that you open up or participate is well worth the investment to keep your team's efforts focused. Come learn the behaviors you can use to succeed at Apache!\\n</p>\\n\\n<p><em>\\nShane is founder of Punderthings LLC consultancy\\, helping organizations find better ways to engage with the critical open source projects that power modern technology and business. He blogs and tweets about open source governance and trademark issues\\, and has spoken at major technology conferences like ApacheCon\\, OSCON\\, All Things Open\\, Community Leadership Summit\\, and Ignite. Shane is serving a seventh term as an elected Director of the ASF\\, providing governance oversight\\, community mentoring\\, and fiscal review for all Apache projects. Previously\\, shane served as VP Brand Management for the ASF for eight years\\, and wrote the trademark and branding policies that cover all 200+ Apache projects\\, including assisting projects with defining and policing their trademarks\\, as well as negoitating agreements with various software vendors using Apache software brands. Otherwise\\, Shane is: a father and husband\\, a BMW driver and punny guy. Oh\\, and we have cats. Follow @ShaneCurcuru and read about open source communities and see his FOSS Foundation directory: http://ChooseAFoundation.com/\\n</em></p>","categories":"Community","url":"https://apachecon.com/acah2020/tracks/community.html#W1855"},{"uid":"acah2020-community-R1615@apachecon.com","sequence":"1","dtstamp":"20200810T143451Z","dtstart":"20201001T161500Z","dtend":"20201001T165500Z","summary":"The economics of vendor neutrality and vendor domination","location":"@home","x-alt-desc":"<strong>Myrle Krantz</strong>\\n\\n<p>\\nGame theory applied to open source can be used to explain how participants profit from open source\\, but existing models work from the assumption that all contributors receive the same benefit from the decisions made. They do not. For example: Some may benefit more when software is faster\\, while others benefit when it is more configurable. It is often necessary to choose between the two. What happens to the general social welfare captured and made available by an open source project when a narrow economic interest dominates its decision making?\\n</p>\\n\\n<p><em>\\nMyrle Krantz is currently serving as the Treasurer for the Apache Software Foundation. She is a former board member for the Apache Software Foundation\\, conference chair for ApacheCon Europe in 2019\\, a member of the Diversity and Inclusion Initiative\\, and the Community Development Committee. She has served as VP for the core banking open source project Fineract. Myrle has her computer science degree from Rice University in Houston\\, and her MBA from the Rotterdam School of Management. Myrle is an American living in the Voreifel in Germany with her two daughters\\, a husband\\, and a hunting dog. She loves to read\\, and plays piano badly.\\n</em>\\n</p>","categories":"Community","url":"https://apachecon.com/acah2020/tracks/community.html#R1615"},{"uid":"acah2020-community-R1655@apachecon.com","sequence":"1","dtstamp":"20200810T143451Z","dtstart":"20201001T165500Z","dtend":"20201001T173500Z","summary":"Welcoming community strengthens the Apache Way","location":"@home","x-alt-desc":"<strong>\\nJarek Potiuk\\n</strong>\\n\\n<p>\\nThis talk is about making the Apache Airflow a more welcoming community\\, by applying more of the principles that are the core of Apache Way. This is the story of the changes that we implemented over the last few years as a group of committers and PMCs in Apache Airflow. When our team started to contribute to Apache Airflow\\, we realized how hard it was to start contributing to the project at the very beginning. While we became few of the most active community members (committers and PMCs) of Apache Airflow\\, we learned our ropes\\, but we have not forgotten that others have similar problems and from the very beginning we started to work on making it easier to become the member of the community on many levels. We started from improving the development environment\\, going through documentation improvements\\, implementing some best coding practices and CI automation around it\\, and ending at mentoring and communication guidelines. We would like to share with the other Apache projects some tips and learning on what you can apply to be a more welcoming project. This talk will be half-deep-down technical and half-soft-skills\\, talking about how both sides are needed in order to be more welcoming. There is a \\\"Success at Apache\\\" blog and Feathercast video about this subject - now is the time to tell some more details in a talk.\\n</p>\\n\\n<p><em>\\nAfter more than 20 years of career in IT from a junior programmer to CTO of 60+ company\\, Jarek has chosen to continue his path as individual contributor. He got back from the half-technical\\, half-managerial path he has been following since\\, and with the vast experience in both technologies and business side of IT\\, as well as being organizer of 500+ attendees IT conference\\, Jarek's engineering skills are supplemented by an understanding of people\\, business\\, customers and partners\\, and with the strong understanding that relationships with people are the key to success in either of the roles. Jarek worked in many roles and many types of companies and he tried it all - from few people mobile payment startup\\, robotics + AI startup where he was a robotics engineer\\, working at software house as an engineer and Head of Technology\\, building and leading 60+ software house\\, Centre of Expertise expert at one of the biggest FMCG companies\\, and being Tech Lead Manager at Google\\, and currently - became one of the most active full-time committers and PMC members of one of the most popular Open-Source Workflow Orchestrator for Big Data - Apache Airflow. Jarek is an experienced technical team leader. Loves leading and motivating professional teams of developers\\, testers\\, IT admins\\, project managers\\, but only if he can actively participate in all of the things his people do. Patient mentor. Focused on achieving realistic deliverables with quality matching the expectations. Balancing well research and pragmatic approach\\, with a strong flair for innovations. Jarek likes to use his voice and for a good reason.\\n</em></p>","categories":"Community","url":"https://apachecon.com/acah2020/tracks/community.html#R1655"},{"uid":"acah2020-community-R1735@apachecon.com","sequence":"2","dtstamp":"20200810T143451Z","dtstart":"20201001T173500Z","dtend":"20201001T181500Z","summary":"How to help companies be the best open source participants possible","location":"@home","x-alt-desc":"<strong>\\nStormy Peters\\n</strong>\\n<p>\\nCompanies are participating more and more in open source. Projects that figure out how to work most effectively with companies will benefit the most while maintaining their autonomy. Come discuss and learn the best ways to include companies in your open source software plans. Learn how to leverage the resources companies can bring to a project while maintaining your project's governance model. Learn how to keep companies in the loop and still hear all the individual voices. Learn how to accept resources without sacrificing autonomy. The speaker is experienced in both running Open Source Programs Office for large companies as well as leading open source software non-profits. This unique perspective to both sides allows her to bring some novel suggestions. Many of us have experience working with open source software projects and companies. This presentation will be a mix of real examples and audience discussion.\\n</p>\\n\\n<p><em>\\nStormy Peters is Director of the Open Source Programs Office at Microsoft. She works with people and teams across Microsoft to help make sure Microsoft uses and contributes to open source software in a way that makes it possible for the world to achieve more through open source software. Stormy is passionate about open source software and educates companies and communities on how open source software is changing the software industry. She is a compelling speaker who engages her audiences during and after her presentations. She has given keynotes at 4\\,000+ person events such as OSCON\\, PyCon and LinuxConf Australia as well as talks to small groups. You can find videos of her talks online. Before joining Microsoft\\, Stormy held leadership positions in open source and developer roles at Red Hat where she was head of the Community Leads\\, the Cloud Foundry Foundation where she was VP of Developer Relations and Mozilla where she led Developer Relations. Previously\\, she served as executive director of the GNOME Foundation and at OpenLogic where she set up their OpenLogic Expert Community. Stormy graduated from Rice University with a B.A. in Computer Science.  \\n</em></p>","categories":"Community","url":"https://apachecon.com/acah2020/tracks/community.html#R1735"},{"uid":"acah2020-community-R1815@apachecon.com","sequence":"1","dtstamp":"20200810T143451Z","dtstart":"20201001T181500Z","dtend":"20201001T185500Z","summary":"Who pays for open source foundations?","location":"@home","x-alt-desc":"<strong>\\nShane Curcuru\\n</strong>\\n<p>\\nOpen source sustainability is more than just individuals figuring out how to make a living off of open source. Have you ever wondered who actually pays for open source? ### Abstract Open source sustainability is more than just individuals figuring out how to make a living off of open source. Have you ever wondered who actually pays for open source? Not just developers\\, but the whole ecosystem around major open source projects\\, either at a FOSS Foundation\\, independent or an open core project at a company? The major software projects we all rely on are mostly hosted at Foundations like Apache\\, Eclipse\\, Linux\\, or Software Freedom Conservancy. Those foundations provide a wide variety of support to project communities\\, including legal and licensing assistance\\, trademark management\\, event support\\, and more. As non-profits\\, these foundations rely on donors and sponsors for all of their work. So who pays for all of this critical support for open source foundations? Come find out what companies are behind the popular open source foundations and major independent projects\\, and who's actually paying for all of the other support work that's done to keep the servers running\\, press releases coming\\, and license compliance work. Surprises are guaranteed\\; I know I was surprised when I realized how many different FOSS projects that Microsoft is an annual sponsor for\\, and what projects a few other companies supported with their cash.\\n</p>\\n\\n<p><em>\\nShane is founder of Punderthings LLC consultancy\\, helping organizations find better ways to engage with the critical open source projects that power modern technology and business. He blogs and tweets about open source governance and trademark issues\\, and has spoken at major technology conferences like ApacheCon\\, OSCON\\, All Things Open\\, Community Leadership Summit\\, and Ignite. Shane is serving a tenth term as an elected Director of the ASF\\, providing governance oversight\\, community mentoring\\, and fiscal review for all Apache projects. Previously\\, shane served as VP Brand Management for the ASF for eight years\\, and wrote the trademark and branding policies that cover all 200+ Apache projects\\, including assisting projects with defining and policing their trademarks\\, as well as negoitating agreements with various software vendors using Apache software brands. Otherwise\\, Shane is: a father and husband\\, a BMW driver and punny guy. Oh\\, and we have cats. Follow @ShaneCurcuru and read about open source communities and see his FOSS Foundation directory at http://ChooseAFoundation.com/\\n</em></p>","categories":"Community","url":"https://apachecon.com/acah2020/tracks/community.html#R1815"},{"uid":"acah2020-community-R1855@apachecon.com","sequence":"1","dtstamp":"20200810T143451Z","dtstart":"20201001T185500Z","dtend":"20201001T193500Z","summary":"A view from the ivory tower: Participating in Apache as a member of academia","location":"@home","x-alt-desc":"<strong>\\nMichael Mior\\n</strong>\\n<p>\\nAcademics in an ivory tower conjures images of people toiling away nicely insulated from many of the concerns of reality. While this has it's advantages\\, anyone who's tried to use a project written for a research paper under a deadline can attest that it doesn't always result in useful code. While completing my PhD\\, I found an Apache project that fit well with the work I was doing s I rolled up my sleeves to write some code to make it more useful for solving my own problems. I've since had the opportunity to join the project's PMC and now as a faculty member\\, I continue to find value in encouraging my own students to contribute to Apache projects. I'll discuss how academics and Apache projects can find mutual benefit in close collaboration. \\n</p>\\n\\n<p><em>\\nMichael completed his Masters degree at the University of Toronto and received a PhD from the University of Waterloo. While completing his PhD\\, he began contributing to the Apache Calcite project and has since joined the Calcite PMC. He joined RIT as an Assistant Professor in 2018. His research revolves around schema design and management and data integration for non-relational data. He continues to look for opportunity for himself and his students to contribute to Apache projects.\\n</em></p>","categories":"Community","url":"https://apachecon.com/acah2020/tracks/community.html#R1855"},{"uid":"acah2020-content-T1615@apachecon.com","sequence":"2","dtstamp":"20200810T143451Z","dtstart":"20200929T161500Z","dtend":"20200929T165500Z","summary":"Making New Friends - Traffic Control and Varnish","location":"@home","x-alt-desc":"<strong>\\nEric Friedrich\\n</strong>\\n<p>\\nTraffic Control is a full featured CDN Control Plane built around Traffic Server caches. This session describes integration of ATCs Traffic Router and Traffic Manager with Varnish Cache. Varnish Cache uses a specialized configuration language (VCL) which is generated alongside Traffic Control configuration files.\\n</p>\\n\\n<p><em>\\nEric is currently a Content Distribution Architect with Disney Streaming Services. He is also a PMC member and committer of Apache Traffic Control.\\n</em></p>","categories":"Content Delivery","url":"https://apachecon.com/acah2020/tracks/content.html#T1615"},{"uid":"acah2020-content-T1655@apachecon.com","sequence":"3","dtstamp":"20200810T143451Z","dtstart":"20200929T165500Z","dtend":"20200929T173500Z","summary":"Flexible Topologies: Scaling your CDN to N tiers","location":"@home","x-alt-desc":"<strong>\\nZach Hoffman\\, Robert O Butts\\, Jeremy Mitchell\\n</strong>\\n<p>\\nUntil recently\\, any CDN built using Apache Traffic Control would be limited to at most 2 tiersan Edge Tier and a Mid Tierwhich limits the CDN's ability to scale as needed. A recent project allows a CDN to be broken into Topologies\\, each of which can span any number of tiers. This talk explores the changes to the project this initiative has involved\\, the capabilities of Flexible Topologies\\, and the steps involved to adapt an existing CDN to use Flexible Topologies.\\n</p>\\n\\n<p><em>\\nZach Hoffman lives in Denver\\, Colorado and is a software engineer at Comcast. They spend their spare time contributing to online puzzle game communities and playing the piano. When working on Apache Traffic Control\\, they focus on its Go and Java components.\\n</em></p>\\n<p><em>\\nRobert O Butts is a software engineer who works on Apache Traffic Control for the Comcast CDN. Rob is a Principal Engineer at Comcast with a Masters in Computer Science focusing on Parallel Processing. Rob has worked on nearly every component of the Apache Traffic Control CDN. He is the primary author of Traffic Monitor\\, was the initial primary author of the Golang Traffic Ops\\, and wrote the Grove HTTP Caching Proxy. He is currently working on extending Apache Traffic Server for Traffic Control's needs.\\n</em></p>\\n<p><em>\\nJeremy has been an Apache Traffic Control contributor for over 5 years with a primary focus on the Traffic Portal (UI) and Traffic Ops API components. During that time\\, he has witnessed exponential growth of the Comcast CDN enabled by the power\\, flexibility and reliability of ATC.\\n</em></p>","categories":"Content Delivery","url":"https://apachecon.com/acah2020/tracks/content.html#T1655"},{"uid":"acah2020-content-T1735@apachecon.com","sequence":"1","dtstamp":"20200810T143451Z","dtstart":"20200929T173500Z","dtend":"20200929T181500Z","summary":"Preview: Parent Selection Strategy Plugins\\, How They Work\\, and What They Mean for Apache Traffic Control","location":"@home","x-alt-desc":"<strong>\\nRobert O Butts\\n</strong>\\n<p>\\nParent Selection Strategies are an upcoming feature of Apache Traffic Server. We will discuss how they work\\, and why they're valuable to Apache Traffic Control. We will also preview Parent Selection Strategy Plugins\\, a feature currently being developed\\, what they may look like\\, how a plugin may be written by ATS users and ATC administrators\\, and the additional benefits Strategy Plugins offer to Apache Traffic Control deployments.\\n</p>\\n\\n<p><em>\\nRobert O Butts is a software engineer who works on Apache Traffic Control for the Comcast CDN. Rob is a Principal Engineer at Comcast with a Masters in Computer Science focusing on Parallel Processing. Rob has worked on nearly every component of the Apache Traffic Control CDN. He is the primary author of Traffic Monitor\\, was the initial primary author of the Golang Traffic Ops\\, and wrote the Grove HTTP Caching Proxy. He is currently working on extending Apache Traffic Server for Traffic Control's needs.\\n</em></p>","categories":"Content Delivery","url":"https://apachecon.com/acah2020/tracks/content.html#T1735"},{"uid":"acah2020-content-T1815@apachecon.com","sequence":"1","dtstamp":"20200810T143451Z","dtstart":"20200929T181500Z","dtend":"20200929T185500Z","summary":"Traffic Ops API Design","location":"@home","x-alt-desc":"<strong>\\nBrennan Fieck\\n</strong>\\n<p>\\nFor the past few months\\, the Traffic Ops working group has been iterating on a design document for the Traffic Ops API. Some pieces of it have already been incorporated into the existing API\\, others are still a work in progress. This talk will be an overview of the design in progress\\, motivations and considerations\\, and lessons learned.\\n</p>\\n\\n<p><em>\\nBrennan is a software engineer on the CDN team at Comcast. Brennan is an Apache Traffic Control committer and one of the leads of the Traffic Ops Working group.\\n</em></p>","categories":"Content Delivery","url":"https://apachecon.com/acah2020/tracks/content.html#T1815"},{"uid":"acah2020-content-T1855@apachecon.com","sequence":"1","dtstamp":"20200810T143451Z","dtstart":"20200929T185500Z","dtend":"20200929T193500Z","summary":"Uplink redundancy for Apache Traffic Control CDN Caches","location":"@home","x-alt-desc":"<strong>\\nSergey Dremin\\n</strong>\\n<p>\\nApache Traffic Control CDN Caches at Comcast have been configured with a simple LAG with LACP connection to a single uplink router with a single IP. That created maintance costs for the CDN caused by router maintanence\\, and impacted overall reliability during router outages. To solve these problems an update to ATC now enables configuring connections to multiple uplink routers. Virtual IPs can now be assigned to the cache and advertised to the rest of the network via BGP peering allowing further flexibility with content routing.\\n</p>\\n\\n<p><em>\\nSergey is a Sr Engineer on the CDN team at Comcast.\\n</em></p>","categories":"Content Delivery","url":"https://apachecon.com/acah2020/tracks/content.html#T1855"},{"uid":"acah2020-content-T1935@apachecon.com","sequence":"1","dtstamp":"20200810T143451Z","dtstart":"20200929T193500Z","dtend":"20200929T201500Z","summary":"Extending Automation towards Self-Service CDNs","location":"@home","x-alt-desc":"<strong>\\nJonathan Gray\\n</strong>\\n<p>\\nApache Traffic Control is a set of applications designed to complement Apache Traffic Server to comprise a Content Delivery Network. Currently the creation of production-like CDN environments is a complex process. I will be demonstrating how Ive been able to augment existing OSS Ansible automation to produce disposable test CDN environments.\\n</p>\\n\\n<p><em>\\nJonathan Gray has been with Comcast on the Content Delivery Network team for approaching 3 years focusing on Operations and Automation. Prior to that he's served as a software developer\\, integrator\\, and devops lead for Milsoft Utility Solutions for 7 years. He holds a Bachelor of Science Degree in Computer Science from Abilene Christian University where he also served as an IT Systems Administrator\\, Datacenter Administrator\\, and Virtualization Administrator for over 3 years.\\n</em></p>","categories":"Content Delivery","url":"https://apachecon.com/acah2020/tracks/content.html#T1935"},{"uid":"acah2020-ctakes-T1615@apachecon.com","sequence":"1","dtstamp":"20200810T143451Z","dtstart":"20200929T161500Z","dtend":"20200929T165500Z","summary":"Apache cTAKES: First Principles and Customization","location":"@home","x-alt-desc":"<strong>\\nSean Finan\\n</strong>\\n<p>\\nBuilt using Apache UIMA\\, Apache clinical Text Analysis and Knowledge Extraction System (cTAKES) is a modular and extensible tool for Natural Language Processing. This is a quick start tutorial on adding custom elements to cTAKES. We illustrate creating simple classes to input\\, process and output data. This involves a concise overview of Apache uimaFIT and the cTAKES type system\\, as well as building a UIMA pipeline using piper files.\\n</p>\\n\\n<p><em>\\nSean Finan is a software developer in the Natural Language Processing lab at Boston Children's Hospital. He has worked with Apache cTAKES for the past 8 years\\, contributing code and supporting the community.\\n</em></p>","categories":"cTAKES","url":"https://apachecon.com/acah2020/tracks/ctakes.html#T1615"},{"uid":"acah2020-ctakes-T1655@apachecon.com","sequence":"1","dtstamp":"20200810T143451Z","dtstart":"20200929T165500Z","dtend":"20200929T173500Z","summary":"Integration UIMA components into cTAKES","location":"@home","x-alt-desc":"<strong>\\nSiamak Barzegar\\n</strong>\\n<p>\\nApache cTAKES (clinical Text Analysis and Knowledge Extraction System) is an open-source Natural Language Processing system for extraction of information from Electronic Health Records (EHR). cTAKES consists of a number of components that work just with English documents. We integrated two important tools (HeidelTime and FreeLing) into cTAKES that provide language analysis functionalities (Temponym Tagging\\, Morphological Analysis\\, Named Entity Detection\\, PoS-Tagging\\, Parsing\\, Word Sense Disambiguation\\, Semantic Role Labelling\\, so forth) for a variety of languages. Also\\, we adapted HeidelTimes grammar and FreeLing to the Medical domain in Spanish. Due to having different type systems in components of cTAKES and HeidelTime and FreeLing\\, we had interoperability challenges that were solved by adapting the native type system of cTAKES for HeidelTime and FreeLings Wrapper.\\n</p>\\n\\n<p><em>\\nSiamak Barzegar is a Senior Research Engineer at Biomedical Text Mining Unit at Barcelona Supercomputing Center in Spain. He won the Science Foundation Ireland (SFI) research scholarship and received his PhD degree from the National University of Ireland\\, Galway in December 2018. The main area of his work/research is focusing on Natural Language Processing\\, Distributional Semantics\\, Word Embeddings\\, Deep Learning\\, Knowledge Extraction on Multilingual & Specific Domains.\\n</em></p>","categories":"cTAKES","url":"https://apachecon.com/acah2020/tracks/ctakes.html#T1655"},{"uid":"acah2020-ctakes-T1735@apachecon.com","sequence":"1","dtstamp":"20200810T143451Z","dtstart":"20200929T173500Z","dtend":"20200929T181500Z","summary":"Secret Engines of Apache cTAKES","location":"@home","x-alt-desc":"<strong>\\nSean Finan\\n</strong>\\n<p>\\nThe Apache clinical Text Analysis and Knowledge Extraction System (cTAKES) default pipeline is a standard in the natural language processing clinical research community. What is past that standard? While the default clinical pipeline uses almost 20 analysis engines\\, there are dozens more in various cTAKES modules. We present and discuss the top 5 annotation engines you never knew you had.\\n</p>\\n\\n<p><em>\\nSean Finan is a software developer in the Natural Language Processing lab at Boston Children's Hospital. He has worked with Apache cTAKES for the past 8 years\\, contributing code and supporting the community.\\n</em></p>","categories":"cTAKES","url":"https://apachecon.com/acah2020/tracks/ctakes.html#T1735"},{"uid":"acah2020-ctakes-T1815@apachecon.com","sequence":"1","dtstamp":"20200810T143451Z","dtstart":"20200929T181500Z","dtend":"20200929T185500Z","summary":"Advanced Dictionary use in Apache cTAKES","location":"@home","x-alt-desc":"<strong>\\nSean Finan\\, Jeff Miller\\n</strong>\\n<p>\\nNamed Entity Recognition is at the core of all complete natural language processing tools. Out of the box clinical Text Analysis and Knowledge Extraction System (cTAKES) uses a dictionary containing part of the Unified Medical Language System (UMLS) that covers most common clinical terms. But it also comes with a custom dictionary creator. If you think that your clinical research is directed\\, then you should probably have a directed dictionary. UMLS subsets\\, non-english dictionaries and novel custom dictionaries have all been successfully used with cTAKES. This is an overview of cTAKES named entity recognition with the essential what\\, why and how of custom dictionaries as the centerpiece. Also discussed will be configuration to use discontiguous spans and subsumption of short terms.\\n</p>\\n\\n<p><em>\\nSean Finan:<br />\\nSean Finan is a software developer in the Natural Language Processing lab at Boston Children's Hospital. He has worked with Apache cTAKES for the past 8 years\\, contributing code and supporting the community.<br />\\nJeff Miller:<br />\\nJeff Miller leads a team of data scientists at the Children's Hospital of Philadelphia (CHOP). His work focuses on developing tools to help researchers analyze clinical data. Jeff holds a master's degree in applied statistics from Penn State University.\\n</em></p>","categories":"cTAKES","url":"https://apachecon.com/acah2020/tracks/ctakes.html#T1815"},{"uid":"acah2020-ctakes-W1615@apachecon.com","sequence":"1","dtstamp":"20200810T213949Z","dtstart":"20200930T161500Z","dtend":"20200930T165500Z","summary":"REST Support for Apache cTAKES","location":"@home","x-alt-desc":"<strong>\\nGandhirajan N\\, Sean Finan\\n</strong>\\n<p>\\nApache cTAKES is a natural language processing system for the extraction of information from electronic medical record clinical free-text. It's predominantly a desktop-based application. This session will talk about enabling REST support in cTAKES. We will be setting up UMLS knowledge sources in MySQL DB using scripts generated by cTAKES Dictionary Creator GUI which in turn uses MetamorphoSys UMLS installation wizard. We will deploy the cTAKES web REST module in tomcat and the application will use the cTAKES engine to perform analysis of the payload passed via REST call against the MySQL DB source and returns the analysis findings as JSON. We will also have a quick demo of the steps mentioned above. This will help healthcare industry to perform NLP analysis using cTAKES engine with just a REST endpoint.\\n</p>\\n\\n<p><em>\\nGandhirajan N:<br />\\nSoftware developer with 15 years of experience in product design and development. Currently working on developing cloud-native applications using Spring Boot and deploying the same in Azure. Apache committer in cTAKES and Cordova projects.<br />\\nSean Finan:<br />\\nSean Finan is a Software Developer in the CHIP-NLP group\\, contributing his experience to their ongoing projects that utilize and help expand the capabilities of Natural Language Processing. Originally a Geophysicist and Materials Scientist\\, Sean gained his interest in software development while creating computer simulations as analogues of physical processes studied in his laboratory research. After leaving academia and a year of employment at the Mayo Clinic\\, Sean moved to Houston to work eleven years with Landmark Graphics\\, the leading provider of scientific software for the energy industry. PMC and committer in Apache cTAKES project.\\n</em></p>","categories":"cTAKES","url":"https://apachecon.com/acah2020/tracks/ctakes.html#W1615"},{"uid":"acah2020-ctakes-W1655@apachecon.com","sequence":"1","dtstamp":"20200810T213949Z","dtstart":"20200930T165500Z","dtend":"20200930T173500Z","summary":"SpaCTeS: Extraction of Information on Diagnosis of Stroke from Electronic Health Reports","location":"@home","x-alt-desc":"<strong>\\nSiamak Barzegar\\n</strong>\\n<p>\\nMost of the relevant data produced on stroke clinical settings consist of unstructured data (clinical narrative texts in Electronic Health Records (EHR). We tested new TM techniques to assist in the process of extracting relevant information from hospital discharge reports of patients diagnosed with a stroke (2016 to 2017). We developed a TM pipeline structured into iterative phases to gradually improve the quality of transforming narrative discharge reports into structured clinical data representations and generating good practice recommendations. The initial system was developed using Apache cTAKES\\, a natural language processing for information extraction from the EHR system initially developed by the Mayo Clinic. The main challenge was the heterogeneity of source data (3000 documents in Spanish and Catalan from 28 different hospitals). We developed an analysis tool to test the quality of texts by identifying missing information and non-standard usage of notations and vocabularies. The system also produced a normalized version of the texts. These results allowed us a detailed analysis of the stroke narrative records and the identification of aspects such as heterogeneity (and its problems) and degree of standardization\\, all of which are critical to enabling better exploitation of the information contained in EHR by TM approaches.\\n</p>\\n\\n<p><em>\\nSiamak Barzegar is a Senior Research Engineer at Biomedical Text Mining Unit at Barcelona Supercomputing Center in Spain. He won the Science Foundation Ireland (SFI) research scholarship and received his PhD degree from the National University of Ireland\\, Galway in December 2018. The main area of his work/research is focusing on Natural Language Processing\\, Distributional Semantics\\, Word Embeddings\\, Deep Learning\\, Knowledge Extraction on Multilingual & Specific Domains.\\n</em></p>","categories":"cTAKES","url":"https://apachecon.com/acah2020/tracks/ctakes.html#W1655"},{"uid":"acah2020-ctakes-W1735@apachecon.com","sequence":"1","dtstamp":"20200810T213949Z","dtstart":"20200930T173500Z","dtend":"20200930T181500Z","summary":"Customize cTAKES for Automated Adverse Drug Event Surveillance in Pediatric Pulmonary Hypertension","location":"@home","x-alt-desc":"<strong>\\nChen Lin\\n</strong>\\n<p>\\nBased on the Apache clinical Text Analysis Knowledge Extraction System (cTAKES)\\, an open-source NLP system\\, we built a customized pipeline and processed 149\\,038 notes for 984 pediatric Pulmonary Hypertension (PH) patients for detecting textual mentions and signs/symptoms that may represent adverse drug events (ADE). Our pipeline featured a customized dictionary for interested term mentions and emphasized term negation\\, temporality of events\\, proximity among mentions\\, for a refined detection for co-occurrence of medications and potential drug effects. Analysis showed our automatic ADE detection system identified up to 7-fold higher ADE rates than those ascertained from diagnostic codes.\\n</p>\\n\\n<p><em>\\nChen is an Applications Development Specialist in the Childrens Hospital Informatics Program-Natural Language Processing (CHIP-NLP) group. Chen is actively incorporating statistical and machine learning technologies into advanced NLP tasks being investigated here at CHIP-NLP. Topics include automatic feature selection\\, coreference resolution\\, disease activity classification based on clinical narratives\\, etc. Chen has worked on several projects including the development of a novel complementary mining process that made use of unused features by a priori defined phenotypes\\; he authored interactive phenotype-mining and visualizing software\\; in addition Chen has research experience in deriving human cancer gene interaction networks based on genome-wide survival analysis.\\n</em></p>","categories":"cTAKES","url":"https://apachecon.com/acah2020/tracks/ctakes.html#W1735"},{"uid":"acah2020-ctakes-R1615@apachecon.com","sequence":"1","dtstamp":"20200810T213949Z","dtstart":"20201001T161500Z","dtend":"20201001T165500Z","summary":"Extracting Patient Narrative from Clinical Notes : Implementing Apache Ctakes at scale using Apache Spark","location":"@home","x-alt-desc":"<strong>\\nDebdipto Misra\\n</strong>\\n<p>\\nPatient notes not only document patient history and clinical conditions but are rich in contextual data and are usually more reliable sources of medical information compared to discrete values in the Electronic Health Record (EHR). For a medium-sized integrated Health System like Geisinger this amounts to approximately fifty thousand notes each day. For information extraction on retrospective data\\, the volume can run into millions of notes depending on the selection criteria. This talk describes the journey taken by the Data Science Team at Geisinger to implement a distributed pipeline which uses Apache Ctakes as the Natural Language Processing (NLP) Engine to annotate notes across the entire spectrum of patient care. From re-writing certain components in the Ctakes engine to architecting data store and pipeline optimization for a better throughput\\, this talk delves into various technical difficulties faced while aspiring to truly do NLP at scale on clinical notes. Towards the end\\, the talk also demonstrates few usecases and how using Ctakes has helped clinicians and stakeholders to extract patient narratives from patient notes using Apache Solr and Banana.\\n</p>\\n\\n<p><em>\\nDebdipto Misra is a Data Scientist with Geisinger Health. Previously\\, he worked with AOL Inc. as a Platform Engineer in Audience Analytics and with EMC Corp. as a Systems Engineer. He has worked in the Data Mining and Analytics space for over half a decade. He won a fellowship and presented the Evolution of Prosthetics using Pattern Recognition on Ultrasound Signals at the 2014 IEEE Big Data Conference in Washington\\, DC. He has also published at multiple journals and presented at healthcare conferences like HIMSS. Currently\\,his main focus is on building capacity planning tools for healthcare organizations for bed-supply demand using various deep learning approaches and integrating it with patient notes.\\n</em></p>","categories":"cTAKES","url":"https://apachecon.com/acah2020/tracks/ctakes.html#R1615"},{"uid":"acah2020-ctakes-R1655@apachecon.com","sequence":"2","dtstamp":"20200810T213949Z","dtstart":"20201001T165500Z","dtend":"20201001T173500Z","summary":"Fault-Tolerant\\, Distributed\\, and Scalable Natural Language Processing with cTAKES","location":"@home","x-alt-desc":"<strong>\\nJeritt Thayer\\, Jeffrey Miller\\n</strong>\\n<p>\\nElectronic health records contain a substantial amount of clinical information as unstructured free text. This information has the potential to enhance clinical decision making as well as provide insight for secondary health related research. Apache Clinical Text Analysis and Knowledge Extraction System (cTAKES) is a health specific natural language processing (NLP) system that has demonstrated success in the health care industry. However\\, analyzing large sets of notes with cTAKES can take months or even years to complete. By combining cTAKES with Apache Spark\\, we developed a fault-tolerant and scalable NLP pipeline that respects the single threaded limitation inherent in cTAKES pipelines. It is capable of processing millions of clinical notes in minutes on a large computing cluster. We have also configured the pipeline to make it easy to adjust common settings like changing negation detection algorithms and toggling whether or not to detect entities over discontinuous spans. At the completion of this session\\, you will have a practical example of processing large volumes of unstructured text using cTAKES and be able to identify the benefits of using different Apache distributed computing frameworks such as Spark and Beam.\\n</p>\\n\\n<p><em>\\nJeritt Thayer<br />\\nJeritt Thayer is a software engineer at Children's Hospital of Philadelphia. His work focuses on designing\\, developing\\, and evaluating novel systems to support patient engagement\\, medical decision making\\, and care delivery. Prior to his career in software\\, Jeritt was a professional soccer player. Jeritt is passionate about developing applications that support asynchronous and non-colocated communication to improve provider coordination and patient outcomes.<br />\\nJeff Miller:<br />\\nJeff Miller leads a team of data scientists at the Children's Hospital of Philadelphia (CHOP). His work focuses on developing tools to help researchers analyze clinical data. Jeff holds a master's degree in applied statistics from Penn State University.\\n\\n</em></p>","categories":"cTAKES","url":"https://apachecon.com/acah2020/tracks/ctakes.html#R1655"},{"uid":"acah2020-ctakes-R1735@apachecon.com","sequence":"1","dtstamp":"20200810T213949Z","dtstart":"20201001T173500Z","dtend":"20201001T181500Z","summary":"Apache cTAKES and Python\\; Apache cTAKES High Throughput Orchestration","location":"@home","x-alt-desc":"<strong>\\nDmitriy Dligach\\, Sean Finan\\, Peter Abramowitsch\\n</strong>\\n<p>\\n1. The rise of Natural Language Processing Machine Learning libraries in Python has created opportunities for the Apache clinical Text Analysis and Knowledge Extraction System (cTAKES). There are also challenges in utilizing the Java-based cTAKES type system across platforms. 2. We have built a high throughput orchestration mechanism to process and publish millions of redacted and unredacted notes in a PHI-safe environment and to manage refreshes where notes can continually be re-redacted\\, or obsoleted. We have a high-urgency stream of Covid related notes that are on a weekly refresh basis.\\n</p>\\n\\n<p><em>\\nDmitriy Dligach:<br />\\nThe overarching goal of Dr. Dligach's research is developing methods for automatic semantic analysis of texts. His work spans such areas of computer science as natural language processing\\, machine learning\\, and data mining. Most recently his research has focused on semantic analysis of clinical texts. He works both on method development and applications.<br />\\nSean Finan:<br />\\nSean Finan is a software developer in the Natural Language Processing lab at Boston Children's Hospital. He has worked with Apache cTAKES for the past 8 years\\, contributing code and supporting the community.<br />\\nPeter Abramowitsch:\\nPeter Abramowitsch started using cTAKES while working in the Hearst Health Innovation Lab. He is now an Architect and cTAKES Implementer in Bakar Computational Health Sciences Institute at the University of California\\, San Francisco.\\n</em></p>","categories":"cTAKES","url":"https://apachecon.com/acah2020/tracks/ctakes.html#R1735"},{"uid":"acah2020-fineract-T1615@apachecon.com","sequence":"2","dtstamp":"20200810T143451Z","dtstart":"20200929T161500Z","dtend":"20200929T165500Z","summary":"The present of Fineract - Panel discussion","location":"@home","x-alt-desc":"<strong>\\nJavier Borkenztain\\, Michael Vorburger\\, Ed Cable\\, James Dailey\\n</strong>\\n<p>\\nIn this panel\\, we will discuss how the community is working\\, what are the current challenges and what are we seeing from our unique perspectives.\\n</p>\\n\\n<p><em>\\nJavier Borkenztain:<br />\\nJavier is a serial entrepreneur with more than twenty years of experience working with technology in several industries and more than a decade in the financial industry. He is co-Founder and CEO of Fiter. Javier was involved with the Apache Fineract and Mifos X communities since 2014\\, and he had several roles within The Mifos Initiative. He was the Founder and CEO of the first Latin American startup granted with a banking grade license from a Central Bank. Javier holds a title of Industrial and Mechanical Engineer from the Universidad de la Repblica del Uruguay\\, MBA from the IEEM\\, Universidad de Montevideo.<br />\\nMichael Vorburger:<br />\\nFather. EPFL alumni. Working on The Supercomputer for advertisement & more (at G)\\; prev. at @RedHat. Also ScratchDay.ch\\, Fineract.dev<br />\\nEd Cable:<br />\\nPioneer in catalyzing community growth and financial inclusion innovation as the leader of the global open source Mifos community for the past decade. Perched at the compelling intersection of financial inclusion and open source technology\\, I'm well-versed in fintech from multiple dimensions:<br />\\nJames Dailey:<br />\\nJames Dailey is the Board Chair and founder of Mifos\\, the open source community that created and then contributed the fineract code base. He works at the intersection of financial inclusion and energy inclusion in the global south. A serial entrepreneur\\, James has created several social ventures and open source projects\\, aimed at solving big hairy problems. He sits on the fineract PMC and helps with identifying gaps and strategies.\\n</em></p>","categories":"Fineract","url":"https://apachecon.com/acah2020/tracks/fineract.html#T1615"},{"uid":"acah2020-fineract-T1655@apachecon.com","sequence":"1","dtstamp":"20200810T143451Z","dtstart":"20200929T165500Z","dtend":"20200929T173500Z","summary":"Leverage Fintech with Fineract","location":"@home","x-alt-desc":"<strong>\\nJavier Borkenztain\\n</strong>\\n<p>\\nFintech is disrupting the financial industry\\, as Open Source disrupted the software industry. Now the two of them meet at Fineract. The Open Source platform for financial disruption. In this presentation\\, we will explore how Fineract can be utilized to implement Fintech applications from Nigeria to Mexico.\\n</p>\\n\\n<p><em>\\nJavier is a serial entrepreneur with more than twenty years of experience working with technology in several industries\\, and more than a decade in the financial industry. He is co-Founder and CEO of Fiter. Javier was involved with the Apache Fineract and Mifos X communities since 2014\\, and he had several roles within The Mifos Initiative. He was the Founder and CEO of the first Latin American startup granted with a banking grade license from a Central Bank. Javier holds a title of Industrial and Mechanical Engineer from the Universidad de la Repblica del Uruguay\\, MBA from the IEEM\\, Universidad de Montevideo.\\n</em></p>","categories":"Fineract","url":"https://apachecon.com/acah2020/tracks/fineract.html#T1655"},{"uid":"acah2020-fineract-T1735@apachecon.com","sequence":"1","dtstamp":"20200810T143451Z","dtstart":"20200929T173500Z","dtend":"20200929T181500Z","summary":"Fineract: Reinvigorating Community","location":"@home","x-alt-desc":"<strong>\\nMichael Vorburger\\n</strong>\\n<p>\\nThe Apache Fineract community has seen a marked uptake in new activity in PRs and on the dev mailing list. This talk will present some of the measures that we have taken which enabled this. It will use the Apache Fineract project as an example\\, but the shared lessons learnt will be generally applicable.\\n</p>\\n\\n<p><em>\\nMichael is a long time open sourcerer who over the years has been involved in too many projects to enumerate. He started in FLOSS one fateful night many moons ago through a friendly interaction with a maintainer on an IRC channel of what was then the Mifos project. Ever since\\, he has been actively supporting the humanitarian open source platform for financial inclusion that is now known as Apache Fineract in a volunteer capacity. Currently employed by Google\\, previously at Red Hat\\, this talk is given in a purely personal capacity.\\n</em></p>","categories":"Fineract","url":"https://apachecon.com/acah2020/tracks/fineract.html#T1735"},{"uid":"acah2020-fineract-T1815@apachecon.com","sequence":"1","dtstamp":"20200810T143451Z","dtstart":"20200929T181500Z","dtend":"20200929T185500Z","summary":"Open Source as a counterweight to PlatFins (facebook\\, Google\\, Tencent\\, Alibaba)","location":"@home","x-alt-desc":"<strong>\\nJames Dailey\\n</strong>\\n<p>\\nThe public square needs open source for payments\\, banking\\, identity and financial inclusion. Fortunately\\, a number of projects\\, some new\\, some old are there to provide a stack for payments and banking for everyone on the planet. Well survey some recent developments in Fineract\\, Mifos Payment Gateway\\, Mojaloop\\, and related technologies around auth\\, identity\\, and put it together with announcements around open G2P\\, open Banking\\, CBDC\\, and open ID. We'll get into tech enablers like biometrics on a secure device. Well put together a stack of technology built on Fineract\\, and demonstrate what might be possible. From remittances to self-sovereign identity\\, well explore how these new trends intersect with specific technologies. Low cost\\, cloud\\, offline device enabled\\, the future is open.\\n</p>\\n\\n<p><em>\\nJames Dailey is the Board Chair and founder of Mifos\\, the open source community that created and then contributed the fineract code base. He works at the intersection of financial inclusion and energy inclusion in the global south. A serial entrepreneur\\, James has created several social ventures and open source projects\\, aimed at solving big hairy problems. He sits on the fineract PMC and helps with identifying gaps and strategies.\\n</em></p>","categories":"Fineract","url":"https://apachecon.com/acah2020/tracks/fineract.html#T1815"},{"uid":"acah2020-fineract-T1855@apachecon.com","sequence":"1","dtstamp":"20200914T192219Z","dtstart":"20200929T185500Z","dtend":"20200929T193500Z","summary":"Open Banking: a Revolutionary Democratizing Force for Financial Services Innovation","location":"@home","x-alt-desc":"<strong>\\nMatt Millar\\,\\nAli Hussein Kassim\\,\\nVictor Romero\\n</strong>\\n<p>\\nThis panel consisting of technologists and practitioners from the Fineract ecosystem will look at how Open Banking is and will dramatically transform how financial services are delivered. The panel will explore how Open Banking has already impacted sectors like the UK and Europe\\, the revolutionary potential it will have for the underbanked in emerging markets\\, the ongoing status and roadmap of Open Banking APIs being implemented on top of Fineract\\, as well trends and emerging API standards that will continue to unlock new innovation democratizing financial services.\\n</p>\\n\\n<p><em>\\nMatt Millar:<br />\\nMatt is a serial entrepreneur with experience in delivering mass consumer mobile solutions. At Updraft he is co-founder and CTO\\, Updraft helps get consumers out of debt\\, and achieve their financial goals. Prior to updraft Matt built the online booking platform for Slick\\, a startup founded by Brent Hobermans Founders Factory and founded and built Tellybug\\, which pioneered app voting for TV shows including X Factor\\, Britains Got Talent\\, The Voice on primetime broadcasters including BBC\\, ITV\\, and broadcasters across Germany\\, Denmark\\, Poland\\, Sweden\\, Singapore\\, Thailand\\, Australia and more. A pioneer in the mobile industry Matt was building consumer applications on mobile before App Stores existed\\, delivering applications embedded in mobile phones produced by Nokia\\, Samsung\\, Ericsson and more.<br />\\nAli Hussein Kassim:<br />\\nAli is the CEO of Kipochi\\, a Pan-African Fintech company that enables the financial ecosystem to utilize digital technologies towards enhancing efficiency\\, bring innovative financial solutions to the unbanked across the continent and create awareness towards the transformative nature of Financial Technologies. Ali is a Co-Founder and Partner at Demo Ventures. DEMO Ventures is an early stage\\, smart capital fund\\, currently raising its inaugural fund\\, focused on digital innovation and digital transformation in selected sectors in Africa. DEMO leverages its proprietary Pan African deal to catalyze investment into early stage\\, high growth potential startups in Africa. Ali is a Global Board Advisor at the Mifos Initiative. As a true visionary in the fintech world\\, he uses his expertise and critical insight into the sector to help shape the ongoing product and community development strategy and advance sustainability endeavors for the Mifos Initiative.<br />\\nVictor Romero:<br />\\nVictor is a professional with more than 20 years of experience in the Information Technology sector\\, throughout which he has consolidated knowledge and forged technical and administrative skills focused on providing solutions\\, meeting national and international quality standards\\, focused mainly on the Financial sector. He has participated in Development\\, Operations\\, Management and Control areas\\, the experience in these areas gives him a broad vision to take strategic decisions that enhance the social and business objectives promoted by technology demanded by financial institutions\\, always acting with a high sense of professional ethical values. He co-founder of Fintecheando where we have been working with the Mifos Initiative using Fineract 1.x/CN for providing digital solutions for the Mexican financial institution which serve the population at an enterprise level.\\n</em></p>","categories":"Fineract","url":"https://apachecon.com/acah2020/tracks/fineract.html#T1855"},{"uid":"acah2020-fineract-T1935@apachecon.com","sequence":"3","dtstamp":"20200810T143451Z","dtstart":"20200929T193500Z","dtend":"20200929T201500Z","summary":"Digital Field Applications: Exploring the Spectrum of Apps to Truly Reach the Last Mile.","location":"@home","x-alt-desc":"<strong>\\nAvik Ganguly\\n</strong>\\n<p>\\nFrom loan origination to loan collections\\, from e-commerce to remittances\\, from savings to billpay\\, there are a wide range of use cases and different types of field staff and agents that DFAs enable. The APIs in Fineract already power a broad variety of digital field applications but there are many more use cases we could enable. Avik Ganguly of Fynarfin will explore the spectrum of digital field applications\\, the requirements they entail and the roadmap we can advance in Fineract. Hell illustrate the breadth of the Fineract APIs by presenting a case study on the agent banking solution his team has built on top of Fineract.\\n</p>\\n\\n<p><em>\\nAvik Ganguly is the founder of Fynarfin\\, a fintech company building scalable enterprise solutions on top of Apache Fineract. Avik is a Fintech specialist and open source evangelist who picked up his trade while working with Mifos\\, Conflux and Novopay.<br />\\n</em></p>","categories":"Fineract","url":"https://apachecon.com/acah2020/tracks/fineract.html#T1935"},{"uid":"acah2020-fineract-W1615@apachecon.com","sequence":"4","dtstamp":"20200810T213949Z","dtstart":"20200930T161500Z","dtend":"20200930T165500Z","summary":"Fineract in 2030","location":"@home","x-alt-desc":"<strong>\\nJavier Borkenztain\\, Saransh Sharma\\, David Yahalomi\\, James Dailey\\, Gabriele Columbro\\n</strong>\\n<p>\\nIn this panel\\, we will discuss the future of our community and product.\\n</p>\\n\\n<p><em>\\nJavier Borkenztain:<br />\\nJavier is a serial entrepreneur with more than twenty years of experience working with technology in several industries and more than a decade in the financial industry. He is co-Founder and CEO of Fiter. Javier was involved with the Apache Fineract and Mifos X communities since 2014\\, and he had several roles within The Mifos Initiative. He was the Founder and CEO of the first Latin American startup granted with a banking grade license from a Central Bank. Javier holds a title of Industrial and Mechanical Engineer from the Universidad de la Repblica del Uruguay\\, MBA from the IEEM\\, Universidad de Montevideo.<br />\\nSaransh Sharma:<br />\\nResearcher at Muellners<br />\\nDavid Yahalomi<br />\\nDavid is the co-founder of Hypercore a new startup that provides non-banks a SaaS platform for credit issuing\\, and Articode\\, a software development company that specializes in financial and real-time GIS systems development and deployment. David has worked for the first digital bank in Israel - Pepper - as an R&D team leader and after seeing the legacy systems used by the current banks\\, has decided to join the Fineract community after falling for the idea of open-source core financial system.\\nDavid and his team have developed Fineract as a service\\, a free service that provides easy provisioning of a Fineract tenant for developers and startups looking to build on or experiment with Fineract and Mifos X.<br />\\nJames Dailey<br />\\nJames Dailey is the Board Chair and founder of Mifos\\, the open source community that created and then contributed the fineract code base. He works at the intersection of financial inclusion and energy inclusion in the global south. A serial entrepreneur\\, James has created several social ventures and open source projects\\, aimed at solving big hairy problems. He sits on the fineract PMC and helps with identifying gaps and strategies.<br />\\nGabriele Columbro<br />\\nGabriele is an open source executive and technologist at heart. He spent over 15 years building developer ecosystems to deliver value through open source across Europe and the US. He thrives on driving innovation both contributing to open source communities and joining commercial open source ventures\\, whether its for an early stage tech startup\\, a Fortune 500 firm or a non profit foundation. Previously Director of Product Management at Alfresco\\, as Executive Director Gabriele grew the Fintech Open Source Foundation FINOS from the ground up\\, with the vision of creating a trusted arena for the global financial services industry to innovate faster\\, leveraging open source as a model of collaboration. Gabriele holds a Master in Computer Engineering\\, is a Committer for the Apache Software Foundation and advises open source startups. Hes a passionate soccer fan\\, reggae music connoisseur and special needs dad and advocate wannabe.\\n</em></p>","categories":"Fineract","url":"https://apachecon.com/acah2020/tracks/fineract.html#W1615"},{"uid":"acah2020-fineract-W1735@apachecon.com","sequence":"1","dtstamp":"20200810T213949Z","dtstart":"20200930T173500Z","dtend":"20200930T181500Z","summary":"How to implement a digital bank with a social approach using open source","location":"@home","x-alt-desc":"<strong>\\nRal Sibaja\\, Karina Ortiz\\n</strong>\\n<p>\\nLatin America has become a world benchmark where social mobility is hampered by public policies\\, access to information and therefore to financial services. The use of Open Source\\, Fineract in this particular case\\, allows to be the base of the ecosystem that is changing the difficulties in opportunities for Public and Private Institutions that have a perspective of social and financial inclusion. In the proposal we expose the challenges we face in sharing knowledge to the Fineract and Open Source community.\\n</p>\\n\\n<p><em>\\nRal Sibaja:<br />\\nDegree in Applied Mathematics and Computing. Implementation of Core Banking in financial institutions. Financial education workshops for banks and educational institutions. Participation in hackathons to provide innovative solutions to financial inclusion through the use of artificial intelligence and personal assistants.<br />\\nKarina Ortiz:<br />\\nLicenciada en Matemticas Aplicadas y Computacin. Desarrollo de aplicaciones mviles hbridas y pginas web. Experiencia en el sector fintech. Participacin en diversos hackathons para brindar soluciones a la inclusin financiera.\\n</em></p>","categories":"Fineract","url":"https://apachecon.com/acah2020/tracks/fineract.html#W1735"},{"uid":"acah2020-fineract-W1855@apachecon.com","sequence":"1","dtstamp":"20200810T213949Z","dtstart":"20200930T185500Z","dtend":"20200930T193500Z","summary":"Fintech\\, the most disruptive technology of the Century.","location":"@home","x-alt-desc":"<strong>\\nMaria Luisa Martinez\\n</strong>\\n<p>\\nThe 4th Industrial Revolution\\, technology has made us jump light-years in terms of well-being\\, human capacity\\, knowledge\\, equality\\, and social inclusion. The change has been so big\\, so profound and so fast\\, that not everybody has been available to experience it\\, and not every industry could be adaptable and innovative enough to survive this change. The power of Technology used for good\\, and all the benefits it can provide\\, has not only changed our ways of living\\, but also our ways of thinking\\, bringing financial inclusion (a key pillar in social inclusion) to places never think of thanks to mobile and electronic money. Technology and innovation applied to Finance with Financial Inclusion as its mission and vision\\, took out of poverty millions of people\\; giving them access not only to financial services\\, but also to education\\, safety and equality. In addition\\, half of the worlds population is under 30: all background people born and raced in the eye of the storm. A generation that knows and wants to change the world\\, that claims for more rights\\, benefits and comfort. A generation becoming\\, if not already become\\, the economic machine to move our financial and banking system. A System with more than 200 years\\, reluctant to adapt this new Era\\, will have what it takes to provide ethical\\, innovative\\, equally\\, useful\\, and human centered financial services? Fintechs have already started and succeeding.\\n</p>\\n\\n<p><em>\\nIm a young\\, committed woman\\, always caring for others\\, and thinking of the next steps on how to solve financial inclusion problems. I am the Vice President of Market Entry at Kuelap\\, Inc. In where my team calls me a force of nature\\, since I am convinced that as part of my generation everybody should be a change agent\\, improving the state of the world\\, and doing the little things today that will have an incredible impact tomorrow. I am a problem-solution driven person. Always searching for a way to improve and find a solution to any given problem. I am highly committed to Social Inclusion\\, and work to fulfill that vision bringing Financial Inclusion to people all over the world. Before Kuelap\\, I worked at the Mifos Initiative\\; where I was the Account Manager. The Mifos Initiative is a non-profit open-source and free software project\\, with a presence in 37 countries\\, with more than 500 Finance Institutions using the software and reaching over 6 million final users. I have worked for and committed to projects that promote Social Inclusion as a volunteer for the past decade. As a former Law student\\, I was involved in projects that included the coordination of legal assistance of the largest free clinic of Uruguay\\; project deployed by the Students Centre of Law School at the Universidad de la Repblica. Since 2013\\, I am a Global Shaper\\, an initiative of the World Economic Forum\\, which gathers young leaders to create a high impact on social projects. In 2014\\, I decided to bring my experience\\, knowledge\\, and enthusiasm on Social Inclusion to my professional life and started to work on Financial Inclusion\\, as a key pillar to overcome poverty around the world. In that regard\\, I co-founded $ERO\\, an Electronic Bank for the Base of the Pyramid in Uruguay\\, which was the first Latin-America startup in obtaining a Central Banks banking license.\\n</em></p>","categories":"Fineract","url":"https://apachecon.com/acah2020/tracks/fineract.html#W1855"},{"uid":"acah2020-fineract-W1935@apachecon.com","sequence":"0","dtstamp":"20200928T190012Z","dtstart":"20200930T193500Z","dtend":"20200930T201500Z","summary":"Running Fineract.dev like a Cloud Native SRE","location":"@home","x-alt-desc":"<strong>\\nMichael Vorburger\\n</strong>\\n<p>\\nhttps://www.fineract.dev runs https://github.com/apache/fineract as a service. This talk will give a peek behind the curtain of how that has been set up\\, detailing e.g. http://blog2.vorburger.ch/2020/05/fineractdev-cicd-from-github-to-google.html etc. It will mention the details of that cloud's used services\\, but go easy on marketing the particular cloud (Google's) that Fineract.dev runs on. We will also lightly touch upon a few SRE principles from https://landing.google.com/sre/books\\, and mention some Cloud Native application architecture principles (hint: it's not all about microservices only).\\n</p>\\n\\n<p><em>\\nMichael is a long time open sourcerer who over the years has been involved in too many projects to enumerate. He started in FLOSS one fateful night many moons ago through a friendly interaction with a maintainer on an IRC channel of what was then the Mifos project. Ever since\\, he has been actively supporting the humanitarian open source platform for financial inclusion that is now known as Apache Fineract in a volunteer capacity. Currently employed by Google\\, previously at Red Hat\\, this talk is given in a purely personal capacity.\\n</em></p>","categories":"Fineract","url":"https://apachecon.com/acah2020/tracks/fineract.html#W1935"},{"uid":"acah2020-fineract-R1615@apachecon.com","sequence":"2","dtstamp":"20200828T190237Z","dtstart":"20201001T161500Z","dtend":"20201001T165500Z","summary":"BitRupee: Passwordless Authentication using ZKP Bitcoin Protocol","location":"@home","x-alt-desc":"<strong>\\nSaransh Sharma\\,\\nAtharva Dhekne\\,\\nAdvait Madhekar\\n</strong>\\n<p>\\nIn this digital day and age\\, passwords are no longer adequate. Users worldwide are victims of multiple malfeasances like brute force attacks\\, injection attacks\\, phishing\\, unsafe credentials\\, data theft\\, among others. The flaws of using passwords - which have increasingly become predictable\\, leave users vulnerable to data and identity theft. Even the strongest passwords are easy to crack and prone to phishing nonetheless. Hence\\, given all these nuisances\\, theres a need to eliminate character-based authentication protocols\\, which would ultimately benefit all developers as well as end-users. An implementation on Apache Fineract will also be presented.\\n</p>\\n\\n<p><em>\\nSaransh Sharma<br />\\nSaransh carries out research in multiple fields like mathematics and Open Source. He is a Researcher at Muellners\\, as well as its non-profit arm Muellners Foundation. Saransh is a Core Committer of Apache Fineract and is an active supporter of Fintech. \\n<br />\\nAtharva Dhekne<br />\\nAtharva is a Research Fellow & Technical Writer for Muellners Foundation\\, working on multiple projects in Fintech as well as other domains. Being an Open Source enthusiast\\, Atharva contributes to multiple OSS organizations - he is also a Technical Writer & Core Committer at WordPress.org. Atharva is completing his undergraduate Computer Engineering studies at the University of Pune (SPPU).\\n<br />\\nAdvait Madhekar<br />\\nAdvait is a Research Fellow & Technical Writer for Muellners Foundation\\, working on multiple projects in Fintech as well as other domains. He is completing his undergraduate Mechanical Engineering studies at the University of Pune (SPPU).\\n\\n\\n</em></p>","categories":"Fineract","url":"https://apachecon.com/acah2020/tracks/fineract.html#R1615"},{"uid":"acah2020-fineract-R1655@apachecon.com","sequence":"0","dtstamp":"20200828T190237Z","dtstart":"20201001T165500Z","dtend":"20201001T173500Z","summary":"AI for All: Democratizing Data Science for Financial Inclusion","location":"@home","x-alt-desc":"<strong>\\nJeremy Engelbrecht\\, Lalit Mohan\\, Ed Cable\\n</strong>\\n<p>\\nIn this session members of the community working group leading artificial intelligence and machine learning will outline the vision and roadmap for leveraging Apache Fineract to democratize data science for financial inclusion. Theyll share case studies highlighting how members of the community are currently pioneering tools for explainable decision making\\, enhance customer experience\\, and micro-analytics. The roadmap will lay out how to evolve the Apache Fineract platform to support credit scoring and origination tools\\, targeted segmentation and predictive product insights\\, KYC\\, AML\\, and accelerated onboarding\\, fraud detection\\, chatbots with natural and regional language support\\, customer churn prediction\\, and more.\\n</p>\\n\\n<p><em>\\nJeremy Engelbrecht:<br />\\nJeremy has 20 years experience in the design of software systems from end to end (SDLC). He has worked for two banks in Southern Africa\\, First National Bank and Standard bank and also worked for a JSE listed insurance company\\, Clientele. He was the Solutions Architect for an award-winning European fintech company called Mybucks. He was selected to be the CTO to start the first digital bank in Saudi Arabia. He has co-founded LNDR which is a fintech company that has started the first digital bank in Swaziland and is the chosen fintech company of choice for a large bank in south africa. He is currently completing his MSc in advanced computer science at the University of Liverpool\\, majoring in Artificial Intelligence with a dissertation based on proving a hypothesis to use DBN(Deep Belief Networks) to do credit scoring using large unstructured datasets.<br />\\nLalit Mohan:<br />\\nLalit is pursuing his PhD in Computer Science & Engineering at IIIT Hyderabad in the area of Information Retrieval\\, Software Engineering\\, ML and NLP. Lalit has been a GSOC mentor at Mifos/Fineract for the last 2 years. Lalit has 23+ Years of IT experience at Infosys\\, Wells Fargo\\, IDRBT (Indias central bank - Reserve Bank of India - Technology research institute). He has published papers on Credit risk evaluation using ML Models\\, Digital Banking\\, FAQs on Cloud Computing for Banks\\, API Banking\\, Open source for Banks and other articles. He is also a member of Banking Industry Architecture Network (BIAN). Some of the key projects executed include a) Treasury Management for World Bank b) Establishment of Indian Banking Community Cloud c) Deployment of Payment Systems in a SaaS model to reduce the capital expenses for cooperative Banks.<br />\\nEd Cable:<br />\\nEd has been a part of the Mifos project since 2007 in its early days at Grameen Foundation. He oversaw the open source community\\, connecting its members worldwide with the tools\\, support\\, and engagement needed to build and use Mifos. Leading the growth of this burgeoning community\\, he saw the dedication and persistence of its members and decided to found COSM (now the Mifos Initiative) to unite their efforts and help them collectively fulfill the vision Grameen Foundation set out to achieve. Prior to this\\, he graduated from the Wharton School at the University of Pennsylvania where he led marketing for the nations largest student-run credit union and discovered his passion for technology-driven international development in their budding social entrepreneurship program. When hes not watching over the Mifos community\\, hes tending to another community of sorts\\, his mini-farmhouse of animals  chickens\\, bunnies\\, dogs\\, goats\\, cats\\, birds\\, and fish.\\n</em></p>","categories":"Fineract","url":"https://apachecon.com/acah2020/tracks/fineract.html#R1655"},{"uid":"acah2020-fineract-R1735@apachecon.com","sequence":"0","dtstamp":"20200828T190237Z","dtstart":"20201001T173500Z","dtend":"20201001T181500Z","summary":"To Infinity and Beyond: Scaling Fineract for the Enterprise.","location":"@home","x-alt-desc":"<strong>\\nAvik Ganguly\\, Nayan Ambali\\, Ed Cable\\, Istvan Molnar\\, Victor Romero\\n</strong>\\n<p>\\nThis panel discussion will showcase a number of institutions that have extended to Fineract to support different use cases at scale including lending\\, payments\\, wallets\\, and digital credit. They will discuss the platforms enhancements they made along with the DevOps strategies to help scale Fineract to reach millions. They will also explore the practical ways in which the community could achieve greater performance in the upstream Fineract codebase\\, the roadmap to achieve the same\\, and replicable tools the community can collaborate on to continually improve performance on both generations of Fineract.\\n</p>\\n\\n<p><em>\\nAvik Ganguly:<br />\\nAvik Ganguly is the founder of Fynarfin\\, a fintech company building scalable enterprise solutions on top of Apache Fineract. Avik is a Fintech specialist and open source evangelist who picked up his trade while working with Mifos\\, Conflux and Novopay.<br />\\nNayan Ambali:<br />\\nNayan is the Co-Founder and CEO of Finflux\\, a leading Mifos partner\\, whose cloud distribution built on top of the Fineract APIs is reaching more than several dozen financial institutions serving more than 3M clients and a portfolio greater than $2.5B USD.<br />\\nEd Cable:<br />\\nEd has been a part of the Mifos project since 2007 in its early days at Grameen Foundation. He oversaw the open source community\\, connecting its members worldwide with the tools\\, support\\, and engagement needed to build and use Mifos. Leading the growth of this burgeoning community\\, he saw the dedication and persistence of its members and decided to found COSM (now the Mifos Initiative) to unite their efforts and help them collectively fulfill the vision Grameen Foundation set out to achieve. Prior to this\\, he graduated from the Wharton School at the University of Pennsylvania where he led marketing for the nations largest student-run credit union and discovered his passion for technology-driven international development in their budding social entrepreneurship program. When hes not watching over the Mifos community\\, hes tending to another community of sorts\\, his mini-farmhouse of animals  chickens\\, bunnies\\, dogs\\, goats\\, cats\\, birds\\, and fish.<br />\\nIstvan Molnar:<br />\\nIstvan is Partner and Architect for DPC Consulting\\, an enterprise Java consultancy from Budapest\\, Hungary that has decades of experience implementing banking and real-time payment solutions at scale. Istvan has led the deployment of Mifos and Fineract at a bank in Germany as well as banks in SE Asia. Building off of the experience implementing real-time payment systems for Singapore and Hungary\\, Istvan has lead the design and architecture of Payment Hub EE\\, a powerful bridge and microservices workflow orchestration tool spearheaded by the Mifos Initiative\\, for integrating Fineract with real-time payment systems like Mojaloop.<br />\\nVictor Romero:<br />\\nVictor is a professional with more than 20 years of experience in the Information Technology sector\\, throughout which he has consolidated knowledge and forged technical and administrative skills focused on providing solutions\\, meeting national and international quality standards\\, focused mainly on the Financial sector. He has participated in Development\\, Operations\\, Management and Control areas\\, the experience in these areas gives him a broad vision to take strategic decisions that enhance the social and business objectives promoted by technology demanded by financial institutions\\, always acting with a high sense of professional ethical values. He co-founder of Fintecheando where we have been working with the Mifos Initiative using Fineract 1.x/CN for providing digital solutions for the Mexican financial institution which serve the population at an enterprise level.\\n</em></p>","categories":"Fineract","url":"https://apachecon.com/acah2020/tracks/fineract.html#R1735"},{"uid":"acah2020-fineract-R1815@apachecon.com","sequence":"0","dtstamp":"20200828T190237Z","dtstart":"20201001T181500Z","dtend":"20201001T185500Z","summary":"OpenG2P: Open Source Building Blocks for Digitizing Large Scale Cash Transfers","location":"@home","x-alt-desc":"<strong>\\nSalton Massally\\, Keyzom Massally\\, Steve Conrad\\, James Dailey\\, Ed Cable\\n</strong>\\n<p>\\nThe global COVID-19 pandemic has unleashed an unprecedented period of economic turmoil and instability that governments will have to overcome. Those in the informal economy are impacted the most and governments must be able to effectively mobilize large scale cash transfer programs to help them survive. Emerging from the government of Sierra Leone\\, based on their experiences digitizing payments to healthcare workers during the Ebola crisis\\, OpenG2P is a framework\\, community of practice\\, and set of open source building blocks to address the common pervasive challenges making it difficult to scale government to person payments. Fineract acts as the of the key building blocks managing wallets\\, accounts\\, and stores of value for individuals. This panel will explore the full open source stack for Open G2P and lessons learned in implementing large scale cash transfer programs.\\n</p>\\n\\n<p><em>\\nSalton is a believer in the transformative impact of technology. He is experienced in building and scaling tech-based solutions to expanding access to much-needed information & services to the un(der)served\\, across Africa. Focused on Digital Financial Services (DFS) & Financial Technologies as a driver for Financial Inclusion\\, his competencies include Financial Inclusion Thought Leadership\\, Product Development and Delivery and Software Engineering: At 23\\, he founded iDT Labs\\, https://idtlabs.xyz\\, known for its expertise in developing & applying practical and sustainable technology to key sectors such as finance\\, agriculture\\, justice\\, & health through effective leadership\\, team-building\\, project planning\\, and delivery\\, fundraising\\, and multilateral stakeholder management. During the height of West Africa's 2014 Ebola crisis\\, he led the technology component of the Ebola response\\, implementing an innovative program that combined technology\\, inclusive finance\\, public health\\, and responsive governance. We broke new grounds in the use of mobile wallets\\, cloud computing\\, and open source technology to deliver scale\\, efficiency and achieve transparency of payments in crisis\\, playing a critical role in preventing the collapse of the Ebola response and recovery efforts\\, and the health sector as a whole in Sierra Leone. In recognition\\, iDT Labs was awarded the 2016 UPS International Disaster Relief Award\\, and Salton\\, the 2017 Queen's Young Leader Award.\\n</em></p>","categories":"Fineract","url":"https://apachecon.com/acah2020/tracks/fineract.html#R1815"},{"uid":"acah2020-fineract-R1855@apachecon.com","sequence":"1","dtstamp":"20200909T142006Z","dtstart":"20201001T185500Z","dtend":"20201001T193500Z","summary":"Real-Time Payments: Enabling a Connected\\, Cashless\\, and Inclusive Society","location":"@home","x-alt-desc":"<strong>\\nIstvan Molnar\\, Godfrey Kutumela\\n</strong>\\n<p>\\nAcross the globe\\, real-time payment systems are being rolled out providing a higher degree of convenience\\, transparency\\, and efficiency enabling secure\\, cashless\\, and more inclusive economies transforming commerce at all levels - consumers\\, business\\, and governments. The Apache Fineract architecture will need to continue to evolve and keep pace with how value is exchanged. The Mifos Initiative has been aligning its development on Apache Fineract and Mifos to equip governments and institutions to effectively participate in the real-time systems being built in accordance with emerging standards like the Gates Foundations Level One Principles and complementary open source systems like Mojaloop enabling real-time interoperable payments. This session will explore the emerging trends and standards around real-time payment systems including a closer look at the ecosystems in a couple of countries\\, an overview of these guiding principles illustrated by Mojaloop as a reference implementation and a showcase and roadmap of the Payment Hub EE - the open source bridge and microservices orchestration layer Mifos is building to seamlessly enable accounts and wallets managed on Fineract to initiate transactions over modern real-time payment rails via mobile channels and Open APIs.\\n</p>\\n\\n<p><em>\\nIstvan is Partner and Architect for DPC Consulting\\, an enterprise Java consultancy from Budapest\\, Hungary that has decades of experience implementing banking and real-time payment solutions at scale. Istvan has led the deployment of Mifos and Fineract at a bank in Germany as well as banks in SE Asia. Building off of the experience implementing real-time payment systems for Singapore and Hungary\\, Istvan has lead the design and architecture of Payment Hub EE\\, a powerful bridge and microservices workflow orchestration tool spearheaded by the Mifos Initiative\\, for integrating Fineract with real-time payment systems like Mojaloop\\n<br />\\n\\nGodfrey has 20 years of Technology Consulting experience specializing in Fintech\\, Cybersecurity\\, DevSecOps\\\\BizSecOps\\, Cloud Native and General Solution Architect. Extensive background working with large scale\\, high-profile systems integration and development projects that span a customers organization\\, and experience designing robust solutions that bring together multiple platforms. He is passionate about the use of technology and its applications in every aspect of humanity in order to advance the livelihood and economic conditions of the under developed world. My interests lie in helping businesses create and deliver value through innovative application of information and communication technologies with an outside-in approach\\, focusing on consumer and market needs\\n</em></p>","categories":"Fineract","url":"https://apachecon.com/acah2020/tracks/fineract.html#R1855"},{"uid":"acah2020-fineract-R1935@apachecon.com","sequence":"0","dtstamp":"20200924T125613Z","dtstart":"20201001T193500Z","dtend":"20201001T201500Z","summary":"Fineract CN improvement proposal","location":"@home","x-alt-desc":"<strong>\\nKevin Madhu\\, Saransh Sharma\\n</strong>\\n<p>\\nWith most of the projects showing negligible or no activity at all for about an year\\, the fineract-cn project looks so close to being looked upon as an abandoned beast. And because the difficulties one has to go through to tame this beast\\, not many people get to enjoy the real beauty it really is. With our work\\, we hope to make a difference to this current state of the project and to attract more people into getting to know the project and make contributions by making it easier for them so that we can steer the project towards a more mature\\, stable\\, complete and releasable version.\\n</p>\\n\\n<p><em>\\nKevin Madhu:<br />\\nDeveloper implementing Finscale version inspired from fineract CN<br />\\nSaransh Sharma :<br />\\nTechnical writers working for Muellners Foundation\\n</em></p>","categories":"Fineract","url":"https://apachecon.com/acah2020/tracks/fineract.html#R1935"},{"uid":"acah2020-geode-T1615@apachecon.com","sequence":"3","dtstamp":"20200810T143451Z","dtstart":"20200929T161500Z","dtend":"20200929T165500Z","summary":"A Caching Approach to Data Transformation of Legacy RDBMS","location":"@home","x-alt-desc":"<strong>\\nGregory Green\\n</strong>\\n<p>\\nThis session will a test driven development approach to building data domains where the source system of record is a legacy Relation Database Management System. The initial code will focus on mainframe based DB2 migration. The pattern will be applicable to similar solutions using Oracle\\, Sybase and other similar traditional relational databases. The talk will highlight the pros and cons of different styles of data pipelines. For example\\, Day 0 initial loads\\, Change Data Capture\\, Event based streams and scheduled pulled based tasks. The following are the highlighted technologies\\; - Apache Geode - Spring Data Geode - Spring Data/JDBC - Kakfa - Spring Cloud Stream - Spring Task/Spring Batch - Spring Cloud DataFlow\\n</p>\\n\\n<p><em>\\nSenior Consultant with over 23 years of software development and architecture experience. Specializing in application transformation from legacy/monolith systems to microservices cloud-native applications with a focus on scalable\\, highly available and self-healing cloud-native data platforms.\\n</em></p>","categories":"Geode","url":"https://apachecon.com/acah2020/tracks/geode.html#T1615"},{"uid":"acah2020-geode-T1655@apachecon.com","sequence":"2","dtstamp":"20200810T143451Z","dtstart":"20200929T165500Z","dtend":"20200929T173500Z","summary":"How I contributed the transactionality in WAN replication feature to Apache Geode","location":"@home","x-alt-desc":"<strong>\\nAlberto Gomez\\n</strong>\\n<p>\\nIn this talk I will describe the use case that drove me to develop the \\\"Transactionality in WAN replication\\\" feature and then I will sketch the technical solution implemented. In a second part\\, I will walk you through the process I followed to contribute the feature\\, from the point of view of a recent member of the Apache Geode Community.\\n</p>\\n\\n<p><em>\\nSoftware engineer with more than 20 years of experience in the telco world. Husband\\, father of three and stylish tennis contender.\\n</em></p>","categories":"Geode","url":"https://apachecon.com/acah2020/tracks/geode.html#T1655"},{"uid":"acah2020-geode-T1735@apachecon.com","sequence":"2","dtstamp":"20200810T143451Z","dtstart":"20200929T173500Z","dtend":"20200929T181500Z","summary":"Introduction to Apache Geode Through Spring Data","location":"@home","x-alt-desc":"<strong>\\nPatrick Johnson\\n</strong>\\n<p>\\nApache Geode is a distributed in-memory data-grid designed with speed\\, concurrency\\, and scalability in mind. Apache Geode can be used as a system of record\\, cache\\, and much more. Spring Data is an extension of the Spring Framework that adds useful abstractions to make working with data simpler with less boilerplate code. In this presentation\\, Patrick will dive into what Apache Geode is\\, how it works\\, what it does\\, and how to get started using it with Spring Data for Apache Geode.\\n</p>\\n\\n<p><em>\\nA (semi)recent graduate of Oregon Institute of Technology\\, Patrick is employed as a Software Engineer at VMware in Portland\\, Oregon\\, where he works primarily on Spring Data for Apache Geode.\\n</em></p>","categories":"Geode","url":"https://apachecon.com/acah2020/tracks/geode.html#T1735"},{"uid":"acah2020-geode-T1815@apachecon.com","sequence":"2","dtstamp":"20200810T143451Z","dtstart":"20200929T181500Z","dtend":"20200929T185500Z","summary":"Got Javascript Apps? We Can Do That!","location":"@home","x-alt-desc":"<strong>\\nKaren Miller\\, Blake Bender\\n</strong>\\n<p>\\nYour app is written in Javascript\\, and that's not going to change. Your app needs a cache\\, and you want to use Apache Geode for your caching layer. Until now\\, you've been out of luck. VMware has developed a Node.js Client for Apache Geode. The donation to Apache Geode is in progress. Stay tuned! You can keep your Javascript app\\, and use the Node.js Client to interact with an Apache Geode cluster.\\n</p>\\n\\n<p><em>\\nKaren Miller:<br />\\nKaren is the current Apache Geode PMC chair. She writes technical documentation for VMware\\, where she pursues opportunities to promote the understanding of Apache Geode. Prior to VMware\\, Karen taught Computer Sciences courses at the University of Wisconsin-Madison and is also a textbook author.<br />\\nBlake Bender:<br />\\nBlake is the Apache Geode Native project anchor. He works for VMWare in this capacity\\, and in the same role at Pivotal prior to the VMWare acquisition. He previously worked at Intel Architecture Labs in Hillsboro\\, OR\\, and at Microsoft in Redmond\\, WA\\, specializing in media and contributing to Microsoft Silverlight\\, Media Foundation\\, DirectX\\, and several releases of Windows.\\n</em></p>","categories":"Geode","url":"https://apachecon.com/acah2020/tracks/geode.html#T1815"},{"uid":"acah2020-geode-W1615@apachecon.com","sequence":"1","dtstamp":"20200810T213949Z","dtstart":"20200930T161500Z","dtend":"20200930T165500Z","summary":"Running a Apache Geode on Kubernetes","location":"@home","x-alt-desc":"<strong>\\nMichael Oleske\\, Aaron Lindsey\\n</strong>\\n<p>\\nAs application developers move workloads to Kubernetes\\, they expect data services to run on Kubernetes alongside their applications. Kubernetes excels at running stateless workloads\\, but how does it handle complex stateful applications such as Apache Geode\\, a distributed in-memory database? We will describe challenges faced while building a Geode operator for Kubernetes\\, including controlling pod terminations\\, working with a dynamic network\\, and ensuring state management during the lifecycle of the deployment. We will explain the solutions we took to control these challenges\\, and dive into how we tested these solutions. You will leave with an understanding of how we moved a system designed to run on bare metal to a Kubernetes environment\\, uniting your workloads with your data services.\\n</p>\\n\\n<p><em>\\nMichael Oleske:<br />\\nMichael is a software engineer on Apache Geode and Tanzu GemFire. He works on making Geode both run well and easy to deploy for Kubernetes.<br />\\nAaron Lindsey:<br />\\nAaron works as a software engineer on Apache Geode and VMware Tanzu GemFire\\, focusing on making Geode run well on Kubernetes. Outside of work\\, he enjoys hiking and backpacking in the Pacific Northwest mountains\\, and volunteering with his local community and church.\\n</em></p>","categories":"Geode","url":"https://apachecon.com/acah2020/tracks/geode.html#W1615"},{"uid":"acah2020-geode-W1655@apachecon.com","sequence":"1","dtstamp":"20200810T213949Z","dtstart":"20200930T165500Z","dtend":"20200930T173500Z","summary":"Apache Geode: Exposing just one ip for all your gateway receivers in K8s","location":"@home","x-alt-desc":"<strong>\\nAlberto Bustamante\\n</strong>\\n<p>\\nApache Geode uses gateway senders and gateway receivers for events replication in multisite configuration. Each receiver usually has its own ip\\, which has some drawbacks in cloud environments as Kubernetes. In this talk I would like to show why we decided to expose all our gateway receivers with the same ip\\, the problems we found\\, the solution proposed and how I contributed it to Geode.\\n</p>\\n\\n<p><em>\\nAlberto is a Computer Science engineer by Carlos III University of Madrid\\, with a Master on Software Craftsmanship by Polytechnic University of Madrid. He has been working at Ericsson Spain since 2008 mainly as Software Developer. Since 2019 he works as Open Source Developer contributing to Apache Geode\\, a data management platform ( in-memory data grid ) that provides real-time\\, consistent access to data-intensive applications throughout widely distributed cloud architectures.\\n</em></p>","categories":"Geode","url":"https://apachecon.com/acah2020/tracks/geode.html#W1655"},{"uid":"acah2020-geode-W1735@apachecon.com","sequence":"1","dtstamp":"20200810T213949Z","dtstart":"20200930T173500Z","dtend":"20200930T181500Z","summary":"vMotion and Apache Geode: Investigating the impact of live migration of virtual machines on an in-memory data grid","location":"@home","x-alt-desc":"<strong>\\nNabarun Nag\\n</strong>\\n<p>\\nAvoiding downtime during maintenance or unforeseen machine issues is paramount for mission-critical\\, ready and available systems. To achieve this goal\\, VMware vSphere vMotion provides the capability for a zero-downtime live migration of virtual machine workloads from one server to another. During the entire duration of migration\\, all applications continue running and providing access to users. This feature can be also be automated using Dynamic Resource Scheduler which places a virtual machine in an optimal location in the server cluster. Pivotal Cloud Cache is an in-memory key-value store powered by Apache Geode\\, which is responsible for responding to large volume of concurrent read/write requests without compromising throughput and latency. Pivotal Cloud Cache also serves multiple use cases like event processing\\, transaction and session state caching\\, etc. in industries like finance and travel. To evaluate the impact of vMotion migration of virtual machines hosting Cloud Cache servers\\, we devised experiments where we deploy a Cloud Cache cluster using the Pivotal Platform in VMwares Solutions lab. We then continuously migrate the virtual machines using vSphere SDK\\, while the cluster is under read and write workloads. We measure the impact on latency and throughput and also monitor that no members are being kicked out of the distributed system due to lack of response to heartbeat messages during the migration phase. This paper discusses the experiment design and results in detail.\\n</p>\\n\\n<p><em>\\nNabarun has been a code contributor and PMC member for Apache Geode since 2016\\, after graduating from University of Wisconsin-Madison. Prior to that\\, he worked for Samsung Research Institute. In his spare time\\, he likes to explore Portland's food scene and playing Apex Legends and Overwatch\\n</em></p>","categories":"Geode","url":"https://apachecon.com/acah2020/tracks/geode.html#W1735"},{"uid":"acah2020-geode-W1815@apachecon.com","sequence":"1","dtstamp":"20200810T213949Z","dtstart":"20200930T181500Z","dtend":"20200930T185500Z","summary":"Improving the Performance of Apache Geode Persistence Recovery","location":"@home","x-alt-desc":"<strong>\\nJianxia Chen\\n</strong>\\n<p>\\nApache Geode offers super fast write-ahead-logging (WAL) persistence with a shared-nothing architecture that is optimized for fast parallel recovery of nodes or an entire cluster. In this talk\\, we will first introduce how Geode disk stores work. Then we will present the recent work to improve the performance of persistence recovery. With the analysis of Geode logs\\, we find that the performance of persistence recovery can be significantly improved by unblocking some of the server initialization threads and parallelizing the process of disk stores recovery. Our experiments have proved that persistence recovery becomes remarkably more scalable and efficient with the improved process.\\n</p>\\n\\n<p><em>\\nJianxia is a PMC member and committer of Apache Geode. He enjoys working on open source software.\\n</em></p>","categories":"Geode","url":"https://apachecon.com/acah2020/tracks/geode.html#W1815"},{"uid":"acah2020-geospatial-W1615@apachecon.com","sequence":"1","dtstamp":"20200810T143451Z","dtstart":"20200930T161500Z","dtend":"20200930T165500Z","summary":"Apache Geospatial: Open Source and Open Standards","location":"@home","x-alt-desc":"<strong>\\nGeorge Percivall\\n</strong>\\n<p>\\nThe Apache Geospatial Track provides the latest in applying Apache Projects to geospatial data and processing. Beginning in 2016\\, the geospatial track has provided a venue for geospatial applications using open source from Apache and other open source foundations. The Geospatial Track includes a focus on the use of open standards to enable interoperability and code reuse between independent software developments. The Geospatial Track for ApacheCon 2020 will include projects from Apache Software Foundation as well as from LocationTech Technology ( http://locationtech.org) and the Open Source Geospatial Foundation (https://www.osgeo.org/). Open standards will be included from the Open Geospatial Consortium (http://www.ogc.org/). Open standards enables reuse of common elements for geospatial information and processing resulting in increased productivity\\, lower interoperability friction\\, and higher data quality. Standards for Coordinate Reference Systems (CRSs)\\, spatial geometries and data arrays used for projects with geospatial content will be described based on open source projects and open standards. Emphasis is placed on the use of open standards including the emerging baseline of OGC APIs. OGC APIs are being defined for geospatial resources\\, e.g.\\, maps\\, tiles\\, features\\, coverages. Developed using OpenAPI\\, the APIs can be implemented in a number of languages and patterns. The presentation will be describe the state of implementations and plans for standardization. The modular structure enables flexibility for developers to reuse OGC APIs in their APIs. If open source for geospatial is of interest to you\\, join the discussion on geospatial@apache.org.\\n</p>\\n\\n<p><em>\\nGeorge Percivall serves as CTO and Chief Engineer of the Open Geospatial Consortium (OGC). As CTO he works with OGC members on strategic technology across OGC Programs and leads OGC Technology Forecasting. As Chief Engineer\\, the OGC Architecture Board. Prior to OGC\\, Mr. Percivall was Chief Engineer with Hughes Aircraft for NASA's Earth Observing System Data and Information System\\; Principal engineer for NASA's Digital Earth Office\\; he applied Systems Engineering on the General Motors EV1 program and a control system engineer on Hughes satellites. He holds a BS in Engineering Physics and an MSEE from the University of Illinois.\\n</em></p>","categories":"Geospatial","url":"https://apachecon.com/acah2020/tracks/geospatial.html#W1615"},{"uid":"acah2020-geospatial-W1655@apachecon.com","sequence":"2","dtstamp":"20200810T143451Z","dtstart":"20200930T165500Z","dtend":"20200930T173500Z","summary":"Visualize Apache SIS capabilities with raster data","location":"@home","x-alt-desc":"<strong>\\nMartin Desruisseaux\\n</strong>\\n<p>\\nApache SIS is a Java library for helping developers to create their own geospatial application. SIS follows closely international standards published jointly by the Open Geospatial Consortium (OGC) and the International Organization for Standardization (ISO). But the core standards implemented by SIS are abstracts\\, and their practical use are non-obvious for developers unfamiliar with OGC/ISO conceptual models. Recently a JavaFX application is being developed for showing Apache SIS in action. Its main purpose is still to provide building blocks that developers can use in their own applications\\, but the SIS application can also be used as-is for navigating in some raster data. Using that application\\, some ISO 19115 elements (the common metadata structure used by SIS for representing information stored in headers of various data formats) get a visual aspect. Some ISO 19111 concepts (the model for reference systems and operations) can be more easily explored. Jacobian matrices (a SIS feature) can been seen in action in the context of map projections. Apache SIS is an implementation of GeoAPI 3.0.1 interfaces\\, which are developed by OGC. Another GeoAPI implementation created during the year is a binding to PROJ 7 C++ API. We will show how GeoAPI allows the use of alternative implementation such as PROJ 7 in a Java application. GeoAPI interoperability with the Python language (work in progress) will also be demonstrated. Finally recent development of Apache SIS support of raster data (work in progress) will be presented\\, with an introduction to its API. The emphasis will be on Earth observation data.\\n</p>\\n\\n<p><em>\\nDid a Ph.D thesis in oceanography\\, but have continuously developed tools for helping analysis work. Used C/C++ before to switch to Java in 1997. Develop geospatial libraries since that time\\, initially as a personal project then as a GeoTools contributor until 2008. Now contributing to Apache SIS since 2013. Attend to Open Geospatial Consortium (OGC) meetings about twice per year in the hope to follow closely standard developments and improve Apache SIS conformance to those standards. Work in a small IT services company (Geomatys) specialized in development of geoportals. Geomatys is an OGC member and develop a stack of open source software for spatial applications\\, with Apache SIS as the foundation to which Geomatys contributes actively.\\n</em></p>","categories":"Geospatial","url":"https://apachecon.com/acah2020/tracks/geospatial.html#W1655"},{"uid":"acah2020-geospatial-W1735@apachecon.com","sequence":"2","dtstamp":"20200810T143451Z","dtstart":"20200930T173500Z","dtend":"20200930T181500Z","summary":"pygeoapi: an OSGeo community project implementing OGC API standards","location":"@home","x-alt-desc":"<strong>\\nTom Kralidis\\, Francesco Bartoli\\n</strong>\\n<p>\\nThe proliferation of REST as an architectural style as well as OpenAPI has resulted in broader adoption of a leaner service contract and the OGC developing a new generation of API specifications in support of discovery\\, access\\, visualization and processing of geospatial data. These efforts are aimed to lower the barrier to implementation\\, especially for mass-market/non-geospatial developers. pygeoapi is an OGC Reference Implementation compliant with the OGC API - Features specification. Implemented in Python\\, pygeoapi supports many other OGC APIs via the Flask web framework and a fully integrated OpenAPI structure. Lightweight\\, easy to deploy and cloud-ready\\, pygeoapi's architecture facilitates publishing datasets and processes from multiple sources. Implementations of other OGC APIs are in progress for the 1.0 roadmap\\, including gridded/coverage data (OGC API - Coverages)\\, search (OGC API - Records)\\, and vector/map tiles (OGC API - Tiles). pygeoapi is a community project of the Open Source Geospatial Foundation (OSGeo). pygeoapi follows a clear separation structure with a view\\, provider/plugin and entry point module. The view approach allows for easy integration with other Python web frameworks like Starlette and Django. The provider abstracts connectivity to numerous data sources (CSV\\, SQLite3\\, GeoJSON\\, Elasticsearch\\, GDAL/OGR) and provides extensibility to support additional formats\\, databases\\, object storage and more. This presentation will provide an overview of pygeoapi\\, current status and next steps as part of the evolution of the project.\\n</p>\\n\\n<p><em>\\nTom is a Senior Systems Scientist for the Meteorological Service of Canada and is a longtime proponent of spatial data infrastructure\\, interoperability\\, open standards and open source. He is chief architect of the World Ozone and Ultraviolet Radiation Data Centre (WOUDC) and MSC's GeoMet platform of real-time and archive weather\\, climate and water APIs. Tom is active in OGC and is currently co-chair of the OGC API - Records SWG. He is also the chair of the UN World Meteorological Organization Expert Team on Metadata. Tom is has contributed to numerous FOSS4G projects such as QGIS and MapServer. He is the founder of the pycsw and pygeoapi projects. He is a Charter Member of OSGeo and currently serves on their Board of Directors.\\n</em></p>","categories":"Geospatial","url":"https://apachecon.com/acah2020/tracks/geospatial.html#W1735"},{"uid":"acah2020-geospatial-W1815@apachecon.com","sequence":"1","dtstamp":"20200810T143451Z","dtstart":"20200930T181500Z","dtend":"20200930T185500Z","summary":"Enabling geospatial in big data lakes and databases with LocationTech GeoMesa","location":"@home","x-alt-desc":"<strong>\\nJames Hughes\\n</strong>\\n<p>\\nMany of the Apache projects serving the big data space do not come with out of the box support for geospatial data types like points\\, lines\\, and polygons. LocationTech GeoMesa has provided add-on support to Apache database projects such as Accumulo\\, Cassandra\\, HBase\\, and Redis crafting spatial and spatio-temporal keys. In addition to distributed databases\\, GeoMesa has enables spatial storage in many of the popular Apache file format projects such as Arrow\\, Avro\\, Orc\\, and Parquet. This talk will review the basics of big geo data persistence either in a data lake or in a database\\, and provide an overview of the benefits (and limitations) of each technology.\\n</p>\\n\\n<p><em>\\nJim Hughes applies training in mathematics and computer science to build distributed\\, scalable system capable of supporting data science and machine learning. He is a core committer for GeoMesa\\, which leverages HBase\\, Accumulo and other distributed database systems to provide distributed computation and query capabilities. He is also a committer for the LocationTech projects JTS and SFCurve and serves a mentor for other LocationTech and Eclipse projects. He serves on the LocationTech Project Management Committee and Steering Committee. Through work with LocationTech and OSGeo projects like GeoTools and GeoServer\\, he works to build end-to-end solutions for big spatio-temporal problems. Jim received his Ph.D. in Mathematics from the University of Virginia for work studying algebraic topology. He enjoys playing outdoors and swing dancing.\\n</em></p>","categories":"Geospatial","url":"https://apachecon.com/acah2020/tracks/geospatial.html#W1815"},{"uid":"acah2020-geospatial-W1855@apachecon.com","sequence":"1","dtstamp":"20200810T143451Z","dtstart":"20200930T185500Z","dtend":"20200930T193500Z","summary":"Map Serving with Apache HTTPD Tile Server Ecosystem (AHTSE)","location":"@home","x-alt-desc":"<strong>\\nDr. Lucian Plesea\\n</strong>\\n<p>\\nAHTSE is a collection of Open Source Apache httpd modules that can be used independently or combined to implement high performance and scalable tile services. Developed for geospatial applications\\, AHTSE can be used for other domains that need fast pan and zoom access to large datasets. AHTSE source code is available on GitHub\\, licensed under Apache License 2.0 terms. Geospatial web services compatible with the OGC WMTS\\, Esri REST and tiled WMS can be implemented. The tight integration with httpd results in exceptional scalability and reliability. The AHTSE development represents an evolution of the NASA original OnEarth server code. Examples of public services that use AHTSE are NASA's WorldView server (https://worldview.earthdata.nasa.gov/)\\, Esri's Astro server (https://astro.arcgis.com) and Esri's EarthLive server (https://earthlive.maptiles.arcgis.com) This session will describe the core AHTSE concepts\\, demonstrate some of the existing server instances and provide sample server configurations.\\n</p>\\n\\n<p><em>\\nDr. Plesea worked at NASA's JPL\\, where he was a pioneer in developing geospatial imagery using supercomputers and later transitioned to building geospatial web services. He built and maintained multiple generations of the well known JPL Onearth/OnMars/OnMoon geospatial image servers\\, and was involved in the early development of the NASA WorldWind system. Once the OnEarth server technology was adopted and transitioned to the NASA EOSDIS as the core server technology behind the WorldView client\\, Dr. Plesea transitioned to Esri\\, where his responsibilities include developing cloud architecture for the basemap geospatial services and develop cloud raster technologies. Dr. Plesea is also an active GDAL contributor and maintainer\\, and is the principal OnEarth and AHTSE developer.\\n</em></p>","categories":"Geospatial","url":"https://apachecon.com/acah2020/tracks/geospatial.html#W1855"},{"uid":"acah2020-geospatial-W1935@apachecon.com","sequence":"1","dtstamp":"20200810T143451Z","dtstart":"20200930T193500Z","dtend":"20200930T201500Z","summary":"Lizmap to create Web Map ApplicationsEdit proposal","location":"@home","x-alt-desc":"<strong>\\nRen-Luc DHONT\\n</strong>\\n<p>\\nLizmap is an open-source application to create web map application\\, based on a QGIS plugin and a Web Client. The project started in 2011 and the 3rd version has been published in 2016. In 2019\\, the project has to be adapted to QGIS 3. We will present the state of the project\\, the connected projects as mapbuilder module and extension scripts\\, and what is coming in the future.\\n</p>\\n\\n<p><em>\\nFounding 3Liz Open Source GIS Software Editor * Lizmap developer * QGIS Server maintener\\n</em></p>","categories":"Geospatial","url":"https://apachecon.com/acah2020/tracks/geospatial.html#W1935"},{"uid":"acah2020-geospatial-R1615@apachecon.com","sequence":"2","dtstamp":"20200810T213949Z","dtstart":"20201001T161500Z","dtend":"20201001T165500Z","summary":"Apache Spark Accelerated Deep Learning Inference for Large Scale Satellite Image Analytics","location":"@home","x-alt-desc":"<strong>\\nDalton Lunga\\, PhD\\n</strong>\\n<p>\\nWith volumes of acquired remotely sensed imagery growing at an exponential rate\\, there is an ever-increasing burden of research and development challenges associated with processing this data at scale. In particular\\, the application of object detection models across large geographic areas predominantly faces three obstacles: (1) a lack of workflows for gathering representative training data and mitigating data bias\\, (2) the inability of current machine learning algorithms to generalize across diverse sensor and geographic conditions\\, and (3) the deployment and reuse of hundreds of unique models at scale. By considering the above challenges in a joint manner\\, we formulate and present an efficient\\, geographically agnostic framework for remote sensing imagery analysis at a global scale. The framework addresses the problem of bias-free data selection by mapping observed satellite images to a novel metric space rooted in the manifold geometry of the data itself\\, forming natural partitions of similar data. Using these partitions to seed training\\, the framework enables simpler\\, localized models to be developed\\; alleviating the challenge of generalization seen by more complex models for larger geographic extents. In an agile manner the framework further exploits the inherent parallelism for dataflow\\, and harnesses Apache Spark to implement distributed inference and training strategies which are seen to favorably scale. We discuss the challenges and merits of using Spark with current deep learning frameworks\\, providing insight into solutions developed for overall workflow harmonization. As a test case study\\, with no training data gathered for any entire country\\, we deploy the framework to detect buildings and roads\\, over areas that spans thousands of square kilometers and covered by 26TB of satellite image data. Drawing understanding from the results of this study\\, we finally present future directions which this exciting research may take.\\n</p>\\n\\n<p><em>\\nDalton is currently a research scientist in machine learning driven geospatial image analytics at ORNL. In this role he deploys machine learning and computer vision techniques in high performance computing environments\\, focusing on creating imagery-based data layers of interest to various societal problems e.g. enable accurate population distribution estimates and damage mapping for disaster management needs. He currently conducts research and development in machine learning techniques and advanced workflows for handling large volumes of geospatial data. Prior to ORNL\\, Dalton worked as machine learning research scientist at the council for scientific and industrial research in South Africa on a variety of projects. He received his PhD in electrical and computer engineering from Purdue University\\, West Lafayette\\, IN\\, US.\\n</em></p>","categories":"Geospatial","url":"https://apachecon.com/acah2020/tracks/geospatial.html#R1615"},{"uid":"acah2020-geospatial-R1655@apachecon.com","sequence":"2","dtstamp":"20200810T213949Z","dtstart":"20201001T165500Z","dtend":"20201001T173500Z","summary":"AutoRetrain: automated deep learning model training on imagery using Apache Airflow and Apache Nifi.-","location":"@home","x-alt-desc":"<strong>\\nCarlos Caceres\\n</strong>\\n<p>\\nThe ability to automate model training is a complex subject that has recently received much attention in the deep learning community. Multiple workflow management systems have also begun gaining traction\\, and are necessary in order to orchestrate the necessary steps to make auto-retraining feasible. This work tackles model automation by making use of two such technologies: Apache Airflow and Apache Nifi. Since both fields of automatic model training and the overarching field of AutoML are broad and complex\\, this work seeks to show the utility of AutoML approaches on object detection in overhead imagery by a simple approach: integrating cycles of model retraining as data becomes available over time. Not only does this approach match the reality of data acquisition\\, it also seeks to leverage information as it becomes available and in so doing\\, reduces the time lag from acquiring new data to extracting useful intelligence. This work tackles a few problems practitioners often encounter when involved in long-term\\, deep learning projects. Questions include: 1). when to start a new round of training\\, 2). how to minimize the time complexity of training a deep learning network\\, and 3). how to tackle the problem of selection bias\\, which occurs when training sets contain uneven probability across classes. The third and most complex question originates from the uneven distribution that may be present in the data. This bias occurs for a variety of reasons\\, low sampling opportunities chief among them. Selection bias and other forms of dataset bias are only a part of the learning problem as learning through back propagation also allows the model to ignore uncertainty in its predictions. Instead\\, certain scenarios have been helped by other techniques\\, such as curriculum learning\\, active-bias learning\\, and hard example mining that focus training on easy\\, uncertain\\, and hard examples respectively. Retraining as described consists of training cycles\\, where each cycle contains the whole data science pipeline  from data gathering\\, data preparation\\, to training\\, and scoring. In order to automate this process for a production system\\, it is first necessary to establish a reliable method to orchestrate the execution of individual pieces of the pipeline. To this end\\, this work experimented with Apache Nifi and Apache Airflow\\, two popular data flow management tools. By combining them with a tracking tool such as Mlflow\\, both Apache Nifi and Apache Airflow become extremely useful in managing retraining flows in a way that allows for reliable reproducibility.\\n</p>\\n\\n<p><em>\\nCarlos Caceres:<br />\\nMAXAR<br />\\nCloud Computing for Gov & Milsatcom Applications from satellite data.\\n</em></p>","categories":"Geospatial","url":"https://apachecon.com/acah2020/tracks/geospatial.html#R1655"},{"uid":"acah2020-geospatial-R1735@apachecon.com","sequence":"14","dtstamp":"20200810T213949Z","dtstart":"20201001T173500Z","dtend":"20201001T181500Z","summary":"GeoSpark: Manage Big Geospatial Data in Apache Spark","location":"@home","x-alt-desc":"<strong>\\nJia Yu\\, Mohamed Sarwat\\n</strong>\\n<p>\\nThe volume of spatial data increases at a staggering rate. This talk comprehensively studies how GeoSpark extends Apache Spark to uphold massive-scale spatial data. During this talk\\, we first provide a background introduction of the characteristics of spatial data and the history of distributed spatial data management systems. A follow-up section presents the vital components in GeoSpark\\, such as spatial data partitioning\\, index\\, and query algorithms. The third section then introduces the latest updates in GeoSpark including geospatial visualization\\, integration with Apache Zeppelin\\, Python and R wrapper. The fourth part finally concludes this talk to help the audience better grasp the overall content and points out future research directions.\\n</p>\\n\\n<p><em>\\nJia Yu:<br />\\nJia Yu is an Assistant Professor at Washington State University School of EECS. He obtained his Ph.D. in Computer Science from Arizona State University in Summer 2020. Jias research focuses on database systems and geospatial data management. In particular\\, he worked on distributed data management systems\\, database indexing\\, and data visualization. He is the main contributor of several open-sourced research projects such as Apache Sedona (incubating)\\, a cluster computing framework for processing big spatial data.<br />\\nMohamed Sarwat :<br />\\nMohamed is an assistant professor of computer science at Arizona State University. Dr. Sarwat is a recipient of the 2019 National Science Foundation CAREER award. His general research interest lies in developing robust and scalable data systems for spatial and spatiotemporal applications. The outcome of his research has been recognized by two best research paper awards in the IEEE International Conference on Mobile Data Management (MDM 2015) and the International Symposium on Spatial and Temporal Databases (SSTD 2011)\\, a best of conference citation in the IEEE International Conference on Data Engineering (ICDE 2012) as well as a best vision paper award (3rd place) in SSTD 2017. Besides impact through scientific publications\\, Mohamed is also the co-architect of several software artifacts\\, which include GeoSpark (a scalable system for processing big geospatial data) that is being used by major tech companies. He is an associate editor for the GeoInformatica journal and has served as an organizer / reviewer / program committee member for major data management and spatial computing venues. In June 2019\\, Dr. Sarwat has been named an Early Career Distinguished Lecturer by the IEEE Mobile Data Management community.\\n</em></p>","categories":"Geospatial","url":"https://apachecon.com/acah2020/tracks/geospatial.html#R1735"},{"uid":"acah2020-geospatial-R1815@apachecon.com","sequence":"1","dtstamp":"20200810T213949Z","dtstart":"20201001T181500Z","dtend":"20201001T185500Z","summary":"Rethinking Earth Observation using Deep Learning","location":"@home","x-alt-desc":"<strong>\\nSayantan Das\\n</strong>\\n<p>\\nEarth observation is the gathering of information about the physical\\, chemical\\, and biological systems of the planet via remote-sensing technologies. With the advent of better compute\\, deep learning based methods have come up that are optimizing over existing remote sensing algorithms. In this talk\\, we shall go over some examples of Computer Vision tasks on Satellite Images including showcasing of one of my key projects done under the Indian Space Research Organisation. Slides to my abstract: http://bit.ly/sessionzero-geo Talk will be divided into three parts: 1. Coverage of what remote sensing is and how deep learning technology is being leveraged for betterment 2. Project showcase of semantic segmentation and object and land use classification using Tensorflow/Pytorch. 3. Open Source tools and a small example of map visualization .\\n</p>\\n\\n<p><em>\\nI am Sayantan Das\\, a final year undergraduate student. I am mentoring for Google Code-In 2019 in Tensorflow. This summer I completed my research internship at Space Applications Centre\\,ISRO Ahmedabad. Currently doing a research internship at CVPR Unit\\,ISI Kolkata. I am pursuing my bachelors in Computer Science & Engineering from West Bengal University of Technology. I love to read\\,review and reproduce research papers.\\n</em></p>","categories":"Geospatial","url":"https://apachecon.com/acah2020/tracks/geospatial.html#R1815"},{"uid":"acah2020-geospatial-R1855@apachecon.com","sequence":"1","dtstamp":"20200810T213949Z","dtstart":"20201001T185500Z","dtend":"20201001T193500Z","summary":"Bring Satellite and Drone Imagery into your Data Science Workflows","location":"@home","x-alt-desc":"<strong>\\nJason Brown\\n</strong>\\n<p>\\nOverhead imagery from satellites and drones have entered the mainstream of how we explore\\, understand\\, and tell stories about our world. They are undeniable and arresting descriptions of cultural events\\, environmental disasters\\, economic shifts\\, and more. Data scientists recognize that their value goes far beyond anecdotal storytelling. It is unstructured data full of distinctive patterns in a high dimensional space. With machine learning\\, we can extract structured data from the vast set of imagery available. RasterFrames extends Apache Spark SQL with a strong Python API to enable processing of satellite\\, drone\\, and other spatial image data. This talk will discuss the fundamentals ideas to make sense of this imagery data. We will discuss how RasterFrames custom DataSource exploits convergent trends in how public and private providers publish images. Through deep Spark SQL integration\\, RasterFrames lets users consider imagery and other location-aware data sets in their existing data pipelines. RasterFrames builds on Apache licensed tech stack\\, fully supports Spark ML and interoperates smoothly with scikit-learn\\, TensorFlow\\, Keras\\, and PyTorch. To crystallize these ideas\\, we will discuss a practical data science case study using overhead imagery in PySpark.\\n</p>\\n\\n<p><em>\\nJason is a Senior Data Scientist at Astraea\\, Inc. applying machine learning to Earth-observing data to provide actionable insights to clients' and partners' challenges. He brings a background in mathematical modeling and statistics together with an appreciation for data visualization\\, geography\\, and software development.\\n</em></p>","categories":"Geospatial","url":"https://apachecon.com/acah2020/tracks/geospatial.html#R1855"},{"uid":"acah2020-geospatial-R1935@apachecon.com","sequence":"0","dtstamp":"20200828T190237Z","dtstart":"20201001T193500Z","dtend":"20201001T201500Z","summary":"Massively Scalable Real-time Geospatial Anomaly Detection with Apache Kafka and Cassandra","location":"@home","x-alt-desc":"<strong>\\nPaul Brebner\\n</strong>\\n<p>\\nThis presentation will explore how we added location data to a scalable real-time anomaly detection application\\, built around Apache Kafka\\, and Cassandra. Kafka and Cassandra are designed for time-series data\\, however\\, its not so obvious how they can efficiently process spatiotemporal data (space and time). In order to find location-specific anomalies\\, we need ways to represent locations\\, to index locations\\, and to query locations. We explore alternative geospatial representations including: Latitude/Longitude points\\, Bounding Boxes\\, Geohashes\\, and go vertical with 3D representations\\, including 3D Geohashes. For each representation we also explore possible Cassandra implementations including: Clustering columns\\, Secondary indexes\\, Denormalized tables\\, and the Cassandra Lucene Index Plugin. To conclude we measure and compare the query throughput of some of the solutions\\, and summarise the results in terms of accuracy vs. performance to answer the question Which geospatial data representation and Cassandra implementation is best?\\n</p>\\n\\n<p><em>\\nSince learning to program on a VAX 11/780\\, Paul has extensive R&D and consulting experience in distributed systems\\, technology innovation\\, software architecture and engineering\\, software performance and scalability\\, grid and cloud computing\\, and data analytics and machine learning. Paul is the Technology Evangelist at Instaclustr. Hes been learning new scalable technologies\\, solving realistic problems and building applications\\, and blogging about Apache Cassandra\\, Spark\\, Zeppelin\\, Kafka\\, and Elasticsearch. Paul has worked at UNSW\\, several tech start-ups\\, CSIRO\\, UCL (London\\, UK)\\, & NICTA. Paul has helped solve significant software architecture and performance problems for clients including Defence and NBN Co. Paul has an MSc in Machine Learning and a BSc (Computer Science and Philosophy).\\n</em></p>","categories":"Geospatial","url":"https://apachecon.com/acah2020/tracks/geospatial.html#R1935"},{"uid":"acah2020-groovy-T1615@apachecon.com","sequence":"1","dtstamp":"20200810T143451Z","dtstart":"20200929T161500Z","dtend":"20200929T165500Z","summary":"Groovy update: What's new in Groovy 3.0 and coming in 4.0","location":"@home","x-alt-desc":"<strong>\\nPaul King\\n</strong>\\n<p>\\nThis talk looks at the latest features in Groovy from 3.0 and beyond. This includes the Parrot parser and a myriad of other new miscellaneous features. This will be the first ApacheCon talk going into details of the features planned for Groovy 4.0 including numerous large scale reworking effects and Groovy's response to features coming in JDK versions up to JDK 14. The talk outlines a broad roadmap of how the new features are planned to be rolled out and the system requirements for each version.\\n</p>\\n\\n<p><em>\\nPaul King is a JavaOne Rockstar who has been contributing to open source projects for nearly 30 years. He is an active committer on numerous projects including Groovy\\, GPars and Gradle. Paul speaks at international conferences\\, publishes in software magazines and journals\\, and is a co-author of Mannings best-seller: Groovy in Action\\, 2nd Edition. He is also VP Apache Groovy and Chair of the Apache Groovy PMC.\\n</em></p>","categories":"Groovy","url":"https://apachecon.com/acah2020/tracks/groovy.html#T1615"},{"uid":"acah2020-groovy-T1655@apachecon.com","sequence":"1","dtstamp":"20200810T143451Z","dtstart":"20200929T165500Z","dtend":"20200929T173500Z","summary":"What's in Groovy for Functional Programming","location":"@home","x-alt-desc":"<strong>\\nNaresha K\\n</strong>\\n<p>\\nThe directions in which popular programming languages are heading to is clear evidence of the need for multiple programming paradigms. One such programming paradigm that is gaining attention these days is functional programming. Groovy too has embraced functional programming and provides a wide variety of features for a developer to code in the functional style. In this live coding session\\, I demonstrate the functional programming features of Groovy. We start with the higher-order function support in Groovy and see the benefits they offer. From the example\\, we can observe that functional programming is indeed idiomatic in several parts of Groovy. We then step into implementing functional composition\\, currying\\, memoizing tail-call optimization\\, and recursion. We conclude the session by understanding how dependency injection works in functional programming. By the end of the session\\, developers understand how functional programming leads to concise and better maintainable code. Developers using Java learn additional support for functional programming in Groovy.\\nAbout the speaker(s):\\n</p>\\n\\n<p><em>\\nNaresha works as Developer\\, Technical Excellence Coach and Cloud Transformation Catalyst. He works with the developers to improve their professional practices to get better at developing maintainable applications that continuously deliver business value. He also helps teams to architect solutions for the cloud and migrate applications to cloud platforms. He has been developing enterprise software for more than 13 years. TDD\\, Refactoring\\, Programming languages\\, Cloud architecture and Continuous Delivery are his current areas of interest. He is passionate about learning new technologies/ programming paradigms and applying them to solve business problems. Naresha is the founder organiser of Bangalore Groovy User Group. Naresha has been a speaker at several conferences including GR8 Conf EU\\, Functional Conf\\, GR8 Conf India\\, GIDS\\, Java2Days Bulgaria\\, Eclipse Summit\\, Selenium Conf\\, AWS Community Day\\, and FOSSCON India.\\n</em></p>","categories":"Groovy","url":"https://apachecon.com/acah2020/tracks/groovy.html#T1655"},{"uid":"acah2020-groovy-T1735@apachecon.com","sequence":"1","dtstamp":"20200810T143451Z","dtstart":"20200929T173500Z","dtend":"20200929T181500Z","summary":"Apache Groovy's Metaprogramming Options and You","location":"@home","x-alt-desc":"<strong>\\nAndres Almiray\\n</strong>\\n<p>\\nApache Groovy provides several ways to modify and update programs and classes by means of metaprogramming. Some of this options are available at runtime\\, some others at compile time\\, and some are even reachable to other JVM languages. These options allow library and framework authors to design better integrations\\, prototype new language constructs without grammar changes\\, deliver powerful and and gratifying DSLs\\, and more. Come to this talk to discover these options and learn how you can put them to work on your projects.\\n</p>\\n\\n<p><em>\\nAndres is a Java/Groovy developer and a Java Champion with more than 20 years of experience in software design and development. He has been involved in web and desktop application development since the early days of Java. Andres is a true believer in open source and has participated in popular projects like Groovy\\, Griffon\\, and DbUnit\\, as well as starting his own projects (Json-lib\\, EZMorph\\, GraphicsBuilder\\, JideBuilder). Founding member of the Griffon framework and Hackergarten community event.\\n</em></p>","categories":"Groovy","url":"https://apachecon.com/acah2020/tracks/groovy.html#T1735"},{"uid":"acah2020-groovy-T1815@apachecon.com","sequence":"1","dtstamp":"20200810T143451Z","dtstart":"20200929T181500Z","dtend":"20200929T185500Z","summary":"Effective Java with Groovy - How Language Influences Adoption of Good Practices","location":"@home","x-alt-desc":"<strong>\\nNaresha K\\n</strong>\\n<p>\\n'Effective Java' presents the most effective ways of using language. However\\, the adoption of these practices among Java developers is less than satisfactory. In my observation\\, the effort required to implement them can be a barrier to the adoption of these practices. Since Groovy runs on JVM\\, most of the suggestions from Effective Java are equally relevant for Groovy developers. Groovy provides out of the box constructs for many of the recommended practices which can boost developer productivity. In this talk\\, I walk you through code examples that follow these good practices\\, highlighting the Groovy way of implementing the 'Effective Java' suggestions. As a participant\\, you walk away\\, appreciating the simplicity with which Groovy empowers the developers. The talk also provides food for thought - how a language can influence its users to adopt good practices. Java users learn the techniques a language can use to reduce the friction to adoption of good practices\\, instead of coming up with a prescription on how to implement good practices. Developers familiar with Groovy understand the reason behind the design of their favourite language features.\\n</p>\\n\\n<p><em>\\nNaresha works as Developer\\, Technical Excellence Coach and Cloud Transformation Catalyst. He works with the developers to improve their professional practices to get better at developing maintainable applications that continuously deliver business value. He also helps teams to architect solutions for the cloud and migrate applications to cloud platforms. He has been developing enterprise software for more than 13 years. TDD\\, Refactoring\\, Programming languages\\, Cloud architecture and Continuous Delivery are his current areas of interest. He is passionate about learning new technologies/ programming paradigms and applying them to solve business problems. Naresha is the founder organiser of Bangalore Groovy User Group. Naresha has been a speaker at several conferences including GR8 Conf EU\\, Functional Conf\\, GR8 Conf India\\, GIDS\\, Java2Days Bulgaria\\, Eclipse Summit\\, Selenium Conf\\, AWS Community Day\\, and FOSSCON India.\\n</em></p>","categories":"Groovy","url":"https://apachecon.com/acah2020/tracks/groovy.html#T1815"},{"uid":"acah2020-groovy-T1855@apachecon.com","sequence":"1","dtstamp":"20200810T143451Z","dtstart":"20200929T185500Z","dtend":"20200929T193500Z","summary":"What would a new Groovy web console look like?","location":"@home","x-alt-desc":"<strong>\\nGuillaume Laforge\\n</strong>\\n<p>\\nThe venerable Groovy web console helps developers share snippets of groovy code. However\\, it doesnt really look fresh. Furthermore\\, there are certain limitations that could potentially be lifted. What could a redesigned web console look like? Sometimes\\, youd like to pin a specific version of Groovy: you might want to try the new Groovy 3 or are stuck with a 2.5.x version. Perhaps you are using @Grab to take advantage of some dependencies in your script\\, and you would like to test and run this snippet online. Is it possible? Lets see what we can do for the long awaited v2 of the Groovy web console!\\n</p>\\n\\n<p><em>\\nAt Google\\, Guillaume Laforge is Developer Advocate for the Google Cloud Platform\\, where he spreads the word about the rich set of products and services offered for developers wishing to take advantage of the cloud for their projects and businesses. Before joining Google\\, at Restlet\\, Guillaume was taking care of the Product Leadership around the APISpark API management platform\\, the Restlet Studio for crafting Web APIs and the Restlet Framework for authoring restful applications. He is also leading the Developer Advocacy team\\, to interact with developers using those projects. He's also well known for his deep involvement with the Apache Groovy programming language and community over many years.\\n</em></p>","categories":"Groovy","url":"https://apachecon.com/acah2020/tracks/groovy.html#T1855"},{"uid":"acah2020-groovy-T1935@apachecon.com","sequence":"1","dtstamp":"20200810T143451Z","dtstart":"20200929T193500Z","dtend":"20200929T201500Z","summary":"Favouring Composition - The Groovy Way","location":"@home","x-alt-desc":"<strong>\\nNaresha K\\n</strong>\\n<p>\\nMost developers I met agree that composition is better than inheritance. However\\, in most codebases\\, we see the use of inheritance where composition would have been a better design choice. Then why are the Java developers falling into this trap? It is easy to implement inheritance over composition\\, Isnt it? But we end up paying for the consequences in terms of reduced maintainability. Can language offer anything for the developers to implement compositions? In this presentation\\, I walk you through what Groovy has to offer to make sure implementing composition is as easy as inheritance\\, if not simpler. I dive into three techniques for applying compositions in your Groovy applications. We start with the technique of delegation and see how easy it is to implement compositions. We uncover the limitations of this technique and introduce traits. After walking through plenty of code examples covering various aspects of using traits\\, we briefly touch upon functional composition\\, since Groovy also supports functional programming.\\n</p>\\n\\n<p><em>\\nNaresha works as Developer\\, Technical Excellence Coach and Cloud Transformation Catalyst. He works with the developers to improve their professional practices to get better at developing maintainable applications that continuously deliver business value. He also helps teams to architect solutions for the cloud and migrate applications to cloud platforms. He has been developing enterprise software for more than 13 years. TDD\\, Refactoring\\, Programming languages\\, Cloud architecture and Continuous Delivery are his current areas of interest. He is passionate about learning new technologies/ programming paradigms and applying them to solve business problems. Naresha is the founder organiser of Bangalore Groovy User Group. Naresha has been a speaker at several conferences including GR8 Conf EU\\, Functional Conf\\, GR8 Conf India\\, GIDS\\, Java2Days Bulgaria\\, Eclipse Summit\\, Selenium Conf\\, AWS Community Day\\, and FOSSCON India.\\n</em></p>","categories":"Groovy","url":"https://apachecon.com/acah2020/tracks/groovy.html#T1935"},{"uid":"acah2020-groovy-W0900@apachecon.com","sequence":"1","dtstamp":"20200810T213949Z","dtstart":"20200930T090000Z","dtend":"20200930T094000Z","summary":"Groovy Hackathon","location":"@home","x-alt-desc":"<strong>\\nGroovy Community\\n</strong>\\n<p>\\nGroovy Hackathon\\n</p>\\n\\n<p><em>\\n...\\n</em></p>","categories":"Groovy","url":"https://apachecon.com/acah2020/tracks/groovy.html#W0900"},{"uid":"acah2020-groovy-W1615@apachecon.com","sequence":"1","dtstamp":"20200810T213949Z","dtstart":"20200930T161500Z","dtend":"20200930T165500Z","summary":"Grails 4: Leveling Up Your Game","location":"@home","x-alt-desc":"<strong>\\nZachary Klein\\n</strong>\\n<p>\\nGrails 4 takes the powerful and flexibility of the Grails framework to a new level\\, with the latest versions of core frameworks like Spring 5.1\\, Spring Boot 2.1\\, Gradle 5\\, and Groovy 2.5. Additionally\\, Micronaut is now part of the Grails foundation\\, allowing many powerful features from Micronaut to be used natively within your Grails apps. In this talk\\, well look at how you can upgrade your Grails 3 project (with a little aside for Grails 2 projects as well) to Grails 4\\, and get a taste of the new features at your disposal in this exciting new release.\\n</p>\\n\\n<p><em>\\nZachary Klein has been practicing web development since 2010 and front-end development since 2015. He's a contributor to both the Grails and Micronaut frameworks\\, as well as an instructor in Object Computing's training practice.\\n</em></p>","categories":"Groovy","url":"https://apachecon.com/acah2020/tracks/groovy.html#W1615"},{"uid":"acah2020-groovy-W1655@apachecon.com","sequence":"0","dtstamp":"20200828T190237Z","dtstart":"20200930T165500Z","dtend":"20200930T173500Z","summary":"A Groovy Apache Fortress","location":"@home","x-alt-desc":"<strong>\\nShawn McKinney\\n</strong>\\n<p>\\nOf late\\, some experimentation around a couple of API sets\\, specifically AccessMgr and AdminMgr in Apache Fortress\\, a Role-Based Access Control system. The goal: To simplify and enhance the API interactions with Apache Fortress specifically\\, and security authorization systems in general. Admittedly\\, the fortress managers have grown over the years\\, both in number of methods\\, and their complexity. That is\\, as new use cases or features pop up\\, say dynamic constraints placed on roles\\, new APIs must be invented to handle the new patterns. While this is a good thing\\, the systems evolving/adaptable\\, its bad from the standpoint of the number of methods users must learn\\, and having to maintain new method entry points into the system. Lets take the [AdminMgr](https://github.com/apache/directory-fortress-core/blob/master/src/main/java/org/apache/directory/fortress/core/AdminMgr.java) for example. It has over 50 public methods! Perhaps understandable when one considers all of the entities that are being processed. But also highly complicated and confusing for someone who is new to the space. What does groovy got to do with it? Enter Apache Groovy. As most of you know\\, a dynamic scripting language that sits on top of the Java virtual machine. It accelerates development through the elimination of boilerplate and bringing some convenience to longstanding irritations in the platform. Things like checked exceptions\\, brackets\\, even semi-colons are no longer required. Here it brought the means to rapidly iterate over some design patterns\\, in order to find easier to use call/response flows. The new GroovyAdmin manager has really just one method! Basically\\, the doIt' passing an operation and entity names\\, along with a map\\, that contains the model (data). The map maps directly to the fortress model\\, using the same entity names and attributes. Similarly\\, the Access manager can be simplified. No longer do we need many methods to create the session (for example). Again\\, only one method\\, start\\, is needed. Again passing only a map containing the operands pertinent to the requires. One envisions how this API pattern works in a service-based setting. Instead of calling a groovy function\\, the client would invoke a service\\, passing a JSON map with the data. But it's expensive to code API gateways and so Groovy helps in the prototyping phase. It allows us to quickly get up to speed\\, giving more freedom to experiment with new ideas\\, possibly leading to more improvements and growth within the target systems themselves.\\n</p>\\n\\n<p><em>\\nCode Monkey\\n</em></p>","categories":"Groovy","url":"https://apachecon.com/acah2020/tracks/groovy.html#W1655"},{"uid":"acah2020-groovy-W1735@apachecon.com","sequence":"1","dtstamp":"20200810T213949Z","dtstart":"20200930T173500Z","dtend":"20200930T181500Z","summary":"Micronaut + Groovy","location":"@home","x-alt-desc":"<strong>\\nSergio del Amo\\n</strong>\\n<p>\\nIn this talk\\, Micronaut committer\\, Sergio del Amo introduces the framework and demonstrates how you can take your web application development to the next level with Micronaut features and Groovy succinctness to create powerful applications in the most productive way. It showcases how to use Micronaut with Groovy related technologies such as GORM\\, Spock\\, Geb. After this talk you should have an understanding of what Micronaut development with Groovy looks like. No initial knowledge of Micronaut is required.\\n</p>\\n\\n<p><em>\\nSergio del Amo feels genuinely empowered by Grails and how succinct and powerful Groovy is. After 6 years contributing his expertise to Grails applications\\, Guides\\, plugins\\, and other related technologies\\, Sergio assisted the 2GM team in the development of Micronaut. Since April 2015\\, Sergio has been the author of Groovy Calamari\\, a weekly newsletter about the Groovy Ecosystem: Grails\\, Geb\\, Gradle\\, and Ratpack.\\n</em></p>","categories":"Groovy","url":"https://apachecon.com/acah2020/tracks/groovy.html#W1735"},{"uid":"acah2020-groovy-W1815@apachecon.com","sequence":"1","dtstamp":"20200810T213949Z","dtstart":"20200930T181500Z","dtend":"20200930T185500Z","summary":"Getting Groovy with Micronaut & JHipster","location":"@home","x-alt-desc":"<strong>\\nZachary Klein\\n</strong>\\n<p>\\nJHipster is a rapid development platform that makes it easy to build modern JavaScript frontends backed by JVM microservices\\, including support for Micronaut. This allows you to produce microservice or monolith projects quickly\\, with plenty of customization options and a project structure that illustrates best practices when developing with Micronaut. As Micronaut is a JVM framework\\, it is compatible with Groovy\\, making it easy to use the Groovy language for tests (with Spock) and for general purpose application code\\, even within standard Java project. In this talk we'll see how you can add Groovy to your Micronaut project (using JHipster as a starting point\\, but applicable even in your own non-JHipster projects)\\, and still take advantage of Micronaut's powerful Dependency Injection and configuration support.\\n</p>\\n\\n<p><em>\\nZachary Klein has been practicing web development since 2010 and front-end development since 2015. He's a contributor to both the Grails and Micronaut frameworks\\, as well as an instructor in Object Computing's training practice.\\n</em></p>","categories":"Groovy","url":"https://apachecon.com/acah2020/tracks/groovy.html#W1815"},{"uid":"acah2020-groovy-W1855@apachecon.com","sequence":"1","dtstamp":"20200810T213949Z","dtstart":"20200930T185500Z","dtend":"20200930T193500Z","summary":"Taming your browser with Geb","location":"@home","x-alt-desc":"<strong>\\nSergio del Amo\\n</strong>\\n<p>\\nGeb is a Groovy layer on top of Selenium Webdriver. Geb integrates the Page Object patter\\, with CSS selectors and several DSLs to empower you to write browser test in an elegant and succinct way. In this beginner talk\\, Sergio de Amo\\, will introduce you to Geb and show you how to test a real Web. If you ever used Selenium\\, Geb opens a world of possibilities.\\n</p>\\n\\n<p><em>\\nSergio del Amo feels genuinely empowered by Grails and how succinct and powerful Groovy is. After 6 years contributing his expertise to Grails applications\\, Guides\\, plugins\\, and other related technologies\\, Sergio assisted the 2GM team in the development of Micronaut. Since April 2015\\, Sergio has been the author of Groovy Calamari\\, a weekly newsletter about the Groovy Ecosystem: Grails\\, Geb\\, Gradle\\, and Ratpack.\\n</em></p>","categories":"Groovy","url":"https://apachecon.com/acah2020/tracks/groovy.html#W1855"},{"uid":"acah2020-groovy-W1935@apachecon.com","sequence":"1","dtstamp":"20200810T213949Z","dtstart":"20200930T193500Z","dtend":"20200930T201500Z","summary":"Interacting with Ethereum Blockchains using Groovy and web3j","location":"@home","x-alt-desc":"<strong>\\nKevin Wittek\\n</strong>\\n<p>\\nEthereum is currently one of the most exciting technologies in the Blockchain domain\\, providing us with a Turing-complete distributed world-computer and a rich statefulness. But how do you actually interact with such a system from within your applications and your code? Is it like using a database\\, a web service\\, the cloud? The answer is probably yes and no In this talk\\, we want to have a look under the hood and see some real code examples of how to interact with Ethereum. We will use web3j\\, a Java implementation of the quasi-standard Javascript Etheuerem client library web3 and we will use it not only with Java but with Groovy to get the flexibility and ease of a scripting language onto the JVM. And we will get sciency and use Groovy to get some data science work done with data we extract from Ethereum\\, demonstrating that Python and R arent the answer for everything.\\n</p>\\n\\n<p><em>\\nTestcontainers co-maintainer and Testcontainers-Spock author\\, passionate about FLOSS and Linux. Received the Oracle Groundbreaker Ambassador award for his contributions to the Open Source community. Software Craftsman and testing fan. Fell in love with TDD because of Spock. Believes in Extreme Programming as one of the best Agile methodologies. Likes to write MATLAB programs to support his wife in performing behavioural science experiments with pigeons. Plays the electric guitar and is a musician in his second life. After many years working in the industry as an engineer\\, Kevin is now doing his PhD at RWTH Aachen on the topic of verification of Smart Contracts and is leading the Blockchain Research Lab at the Institute for Internet Security in Gelsenkirchen at the Westphalian University of Applied Sciences.\\n</em></p>","categories":"Groovy","url":"https://apachecon.com/acah2020/tracks/groovy.html#W1935"},{"uid":"acah2020-groovy-R0900@apachecon.com","sequence":"2","dtstamp":"20200810T213949Z","dtstart":"20201001T090000Z","dtend":"20201001T094000Z","summary":"Groovy and Data Science Workshop","location":"@home","x-alt-desc":"<strong>\\nPaul King\\n</strong>\\n<p>\\nGroovy is a powerful multi-paradigm programming language for the JVM that offers a wealth of\\nfeatures that make it ideal for many data science and big data scenarios.\\n\\n<ul>\\n\t<li>Groovy has a dynamic nature like Python\\, which means that it is very powerful\\, easy to learn\\, and productive. The language gets out of the way and lets data scientists write their algorithms naturally.</li>\\n\t<li>Groovy has a static nature like Java and Kotlin\\, which makes it fast when needed. Its close alignment with Java means that you can often just cut-and-paste the Java examples from various big data solutions and they'll work just fine in Groovy.</li>\\n\t<li>Groovy has first-class functional support\\, meaning that it offers features and allows solutions similar to Scala. Functional and stream processing with immutable data structures can offer many advantages when working in parallel processing or clustered environments.</li>\\n</ul>\\n\\nThis workshop reviews the benefits of using Groovy to develop data science solutions\\,\\nincluding integration with various JDK libraries commonly used in data science solutions\\nincluding libraries for data manipulation\\, machine learning\\, plotting and various big\\ndata solutions for scaling up these algorithms.\\n\\nMath/Data Science libraries covered include:\\nWeka\\, Smile\\, Tablesaw\\, Apache Commons Math\\, Jupyter/Beakerx notebooks\\, Deep Learning4J.\\n\\nLibraries for scaling/concurrency include:\\nApache Spark\\, Apache Ignite\\, Apache MXNet\\, GPars\\, Apache Beam.\\n</p>\\n\\n<p><em>\\nPaul King is a JavaOne Rockstar who has been contributing to open source projects for nearly 30 years.\\nHe is an active committer on numerous projects including Groovy\\, GPars and Gradle. Paul speaks at\\ninternational conferences\\, publishes in software magazines and journals\\, and is a co-author of\\nMannings best-seller: Groovy in Action\\, 2nd Edition. He is also VP Apache Groovy and Chair of the Apache Groovy PMC.\\n</em></p>","categories":"Groovy","url":"https://apachecon.com/acah2020/tracks/groovy.html#R0900"},{"uid":"acah2020-httpd-W1615@apachecon.com","sequence":"0","dtstamp":"20200921T142837Z","dtstart":"20200930T161500Z","dtend":"20200930T165500Z","summary":"Apache's 25th Anniversary: a timeline of The Apache HTTP Server","location":"@home","x-alt-desc":"<strong>\\nJim Jagielski\\, Nick Vidal\\n</strong>\\n<p>\\nThis year\\, the Apache HTTP Server Project celebrates its 25th Anniversary. In February 1995\\, a small group of webmasters known as the Apache Group came together with the goal of releasing a common distribution based on multiple \\\"patches\\\" to the NCSA HTTPd Server. The first public release of Apache was in April 1995 and\\, after a major re-architecture\\, Apache 1.0 was officially released in December 1995. Apache rapidly grew to become the most popular server on the Internet\\, playing a key role in the growth of the World Wide Web and Open Source. The goal of this talk to present a timeline of the Apache HTTP Server Project\\, highlighting the most important milestones of this amazing software and community.\\n</p>\\n\\n<p><em>\\nJim Jagielski:<br />\\nJim is a well known and acknowledged expert and visionary in Open Source and IT\\, an accomplished coder (in numerous languages) and frequent presenter/interviewee/consultant on all things Web and Cloud related. He is best known as one of the developers and co-founders of the Apache Software Foundation and has served as President and Chairman. He also served on the board\\, as well as President\\, for the Outercurve Foundation and was a director for the Open Source Initiative (OSI). Jim works for Uber as their Head of Open Source\\, after stints at ConsenSys\\, Capital One\\, Red Hat\\, VMware\\, and others.<br />\\nNick Vidal:<br />\\nNick Vidal has been an open source advocate for over 15 years. He helped the Open Source Initiative to celebrate the \\\"20th Anniversary of Open Source\\\" by organizing 100 activities across 40 major open source events worldwide.\\n</em></p>","categories":"httpd and the Web","url":"https://apachecon.com/acah2020/tracks/httpd.html#W1615"},{"uid":"acah2020-httpd-R1655@apachecon.com","sequence":"0","dtstamp":"20200914T143859Z","dtstart":"20201001T165500Z","dtend":"20201001T173500Z","summary":"Apache httpd and TLS/SSL certificates validation","location":"@home","x-alt-desc":"<strong>\\nJean-Frederic Clere\\n</strong>\\n<p>\\nWe will look to 2 different things here\\, validation of the server certificate and validation of the client certificates. For the server certificate we will show Let's encrypt and mod_md and speak about the new ACMEv2 protocol and OCSP stapling. For the client certificates we look to OCSP and other validations. Demo and quick start example will provided during the tal\\n</p>\\n\\n<p><em>\\nJean-Frederic has spent more than 20 years writing client/server software. His knowledges range from Cobol to Java\\, BS2000 to Linux and /390 to i386 but with preference to the later \\;). He is committer in Httpd and Tomcat and he likes complex projects where different languages and machines are involved. Borne in France\\, Jean-Frederic lived in Barcelona (Spain) for 14 years. Since May 2006 he lives in Neuchatel (Switzerland) where he works for RedHat in the JBoss division on Tomcat\\, httpd and cloud/cluster related topics.\\n</em></p>","categories":"httpd and the Web","url":"https://apachecon.com/acah2020/tracks/httpd.html#R1655"},{"uid":"acah2020-httpd-R1735@apachecon.com","sequence":"0","dtstamp":"20200914T143859Z","dtstart":"20201001T173500Z","dtend":"20201001T181500Z","summary":"GraphQL in Apache Sling - but isn't it the opposite of REST?","location":"@home","x-alt-desc":"<strong>\\nBertrand Delacretaz\\n</strong>\\n<p>\\nGraphQL is often presented as the opposite of REST\\, but how could a query language be the opposite of an architectural style? Opposing technologies and tools is rarely productive\\, and although Sling is firmly based on REST principles\\, it makes absolute sense to take advantage of GraphQL's rich query language and \\\"one request does it all\\\" interaction model in Sling. In this talk we'll present a GraphQL scripting engine for Sling\\, which enables GraphQL queries either \\\"hidden\\\" on the server side\\, for more control\\, or provided by the clients in the more traditional way to provide the full flexibility of the query language. Generating GraphQL schemas dynamically\\, based on Sling Resource Types\\, Sling Models and scripted schemas\\, provides a lot of flexibility in mapping Sling content to the outside world and makes the query subsystem modular and flexible. This talk will will help you make the best use of this new and exciting query language\\, without compromising on the principles of adaptable and discoverable Web applications.\\n</p>\\n\\n<p><em>\\nBertrand Delacretaz works as a Principal Scientist for Adobe in Basel\\, Switzerland. He's involved in software design and development for Adobe Experience Cloud products\\, which use many open source modules\\, mostly from Apache projects to which his teams contribute extensively. Bertrand is a currently (2020-2021) on his eleventh term on the Apache Software Foundation's Board of Directors and has been active in the Foundation for about 20 years.\\n</em></p>","categories":"httpd and the Web","url":"https://apachecon.com/acah2020/tracks/httpd.html#R1735"},{"uid":"acah2020-httpd-R1815@apachecon.com","sequence":"0","dtstamp":"20200914T143859Z","dtstart":"20201001T181500Z","dtend":"20201001T185500Z","summary":"Apache Web Server Security Hardening","location":"@home","x-alt-desc":"<strong>\\nAndrew Carr\\n</strong>\\n<p>\\nIn my 2017 presentation I discussed hardening Apache Web server with Apache Tomcat behind it. There was a lot of interest in hardening Apache and recommendations. We will review possible exploits and how proper mitigation can prevent breaches. Apache has security holes\\, especially in older versions. While upgrading fixes a lot of problems\\, there will always be exploits. We want to demonstrate a system that is reliable and robust\\, with the least amount of information exposed to the public. Additionally\\, there will be a review of some standard configurations you can build from to protect your environment\\n</p>\\n\\n<p><em>\\nAbout: Andrew has been working in the I.T. industry since 1996 developing hardware\\, network and software solutions to suit business needs and requirements. Leveraging open source software\\, he has implemented enterprise software solutions for a number of large corporations while delivering training to staff\\, both entry-level and expert. Currently\\, Andrew works as a Consulting Enterprise Architect at Perforce.\\n\\n</em></p>","categories":"httpd and the Web","url":"https://apachecon.com/acah2020/tracks/httpd.html#R1815"},{"uid":"acah2020-httpd-R1855@apachecon.com","sequence":"0","dtstamp":"20200914T143859Z","dtstart":"20201001T185500Z","dtend":"20201001T193500Z","summary":"Hardware-protected Keys for TLS: the httpd Angle","location":"@home","x-alt-desc":"<strong>\\nSander Temme\\n</strong>\\n<p>\\nUsing hardware-protected cryptographic modules (Hardware Security Modules or HSMs) is a requirement in many applications for governments\\, banking and financial environments\\, and others. This session will discuss these requirements\\, provide an update on how the Apache HTTP Server's mod_ssl integrate with HSMs\\, and demonstrate how to configure httpd to use hardware-based keys for TLS.\\n</p>\\n\\n<p><em>\\nA long time contributor to the Apache HTTP Server\\, in his copious spare time Sander Temme is the product manager at nCipher Security\\, an Entrust Datacard company\\, for the nShield as a Service Cloud-accesslble Hardware Security Modules.\\n</em></p>","categories":"httpd and the Web","url":"https://apachecon.com/acah2020/tracks/httpd.html#R1855"},{"uid":"acah2020-ignite-T1615@apachecon.com","sequence":"1","dtstamp":"20200810T143451Z","dtstart":"20200929T161500Z","dtend":"20200929T165500Z","summary":"In-Memory Computing Essentials For Software Engineers","location":"@home","x-alt-desc":"<strong>\\nDenis Magda\\n</strong>\\n<p>\\nAttendees will be introduced to the fundamental capabilities of in-memory computing platforms that are proven to boost application performance and solve scalability problems by storing and processing unlimited data sets distributed across a cluster of interconnected machines. The session is tailored for software engineers and architects seeking practical experience with in-memory computing technologies. You'll be given an overview (including code samples in Java) of in-memory concepts such as caches\\, databases\\, and data grids combined with a technical deep-dive based on Apache Ignite in-memory computing platform. In particular\\, we'll cover the following essentials of distributed in-memory systems: * Data partitioning: utilizing all memory and CPU resources of the cluster * Affinity co-location: avoiding data shuffling over the network and using highly-performant distributed SQL queries * Co-located processing: eliminating network impact on the performance of our applications\\n</p>\\n\\n<p><em>\\nDenis Magda is an open-source enthusiast who started his journey in Sun Microsystems as a developer advocate and presently settled down at Apache Software Foundation in the roles of Apache Ignite committer and PMC member. He is an expert in distributed systems and platforms who actively contributes to Apache Ignite and helps companies to build successful open-source projects. You can be sure to come across Denis at conferences\\, workshops and other events sharing his knowledge about the open-source\\, community building\\, distributed systems.\\n</em></p>","categories":"Ignite","url":"https://apachecon.com/acah2020/tracks/ignite.html#T1615"},{"uid":"acah2020-ignite-T1655@apachecon.com","sequence":"1","dtstamp":"20200810T143451Z","dtstart":"20200929T165500Z","dtend":"20200929T173500Z","summary":"Data Streaming using Apache Flink and Apache Ignite","location":"@home","x-alt-desc":"<strong>\\nSaikat Maitra\\n</strong>\\n<p>\\nApache Ignite is a powerful in-memory computing platform. The Apache IgniteSink streaming connector enables users to inject Flink data into the Ignite cache. Join Saikat Maitra to learn how to build a simple data streaming application using Apache Flink and Apache Ignite. This stream processing topology will allow data streaming in a distributed\\, scalable\\, and fault-tolerant manner\\, which can process data sets consisting of virtually unlimited streams of events. Apache IgniteSink offers a streaming connector to inject Flink data into the Ignite cache. The sink emits its input data to the Ignite cache. The key feature to note is the performance and scale both Apache Flink and Apache Ignite offer. Apache Flink can process unbounded and bounded data sets and has been designed to run stateful streaming applications at scale. Application computation is distributed and concurrently executed in clusters. Apache Flink is also optimized for local state access for tasks and does checkpointing of local state for durability. Apache Ignite provides streaming capabilities that allow data ingestion at a high scale in its in-memory data grid.\\n</p>\\n\\n<p><em>\\nSaikat Maitra is Lead Engineer at Target and Apache Ignite Committer and PMC Member. Prior to Target\\, he worked for Flipkart and AOL (America Online) to build retail and e-commerce systems. Saikat received his Master of Technology in Software Systems from BITS\\, Pilani.\\n</em></p>","categories":"Ignite","url":"https://apachecon.com/acah2020/tracks/ignite.html#T1655"},{"uid":"acah2020-incubator-T1615@apachecon.com","sequence":"1","dtstamp":"20200810T143451Z","dtstart":"20200929T161500Z","dtend":"20200929T165500Z","summary":"How to Slide Your Release Pass the Incubator","location":"@home","x-alt-desc":"<strong>\\nJustin Mclean\\n</strong>\\n<p>\\nAll podling releases need to be voted on by the incubator PMC before being released to the world. I'll go through what the incubator PMC looks for in every release and what you can do to make it pass that IPMC vote and get your project one step closer to graduation. More importantly I'll cover where you can get help if you need it. In this talk\\, I'll describe current incubator and ASF policy\\, recent changes that you may not be aware of\\, and go into detail the legal requirements of common open source licenses and the best way to assemble your NOTICE and LICENSE files. Where possible I describe the reasons behind why things are done a certain which may not always be obvious from our documentation. I'll show how I review a release and the simple tools I use. I'll go through a worked example or two\\, including a fictional project called Apache Wombat\\, and cover common mistakes I've seen in releases. \\n</p>\\n\\n<p><em>\\nJustin Mclean has more than 25 years experience in developing web-based applications and is heavily involved in open source hardware and software. He runs his own consulting company Class Software and has spoken at numerous conferences in Australia and overseas. In his free time\\, he's active in several Apache Software Foundation projects\\, including the Apache Incubator\\, and is a mentor for a number of their projects. He's currently the chair of the Apache Incubator and on the ASF board. He also teaches at an online college and runs the IoT meetup in Sydney. \\n</em></p>","categories":"Incubator","url":"https://apachecon.com/acah2020/tracks/incubator.html#T1615"},{"uid":"acah2020-incubator-T1655@apachecon.com","sequence":"1","dtstamp":"20200810T143451Z","dtstart":"20200929T165500Z","dtend":"20200929T173500Z","summary":"Apache IoTDB: Growing a bilingual community","location":"@home","x-alt-desc":"<strong>\\nJulian Feinauer\\n</strong>\\n<p>\\nOpen Source in general but also the ASF gets more and attention in Asia and especially in China. Many projects with initial chinese communities joined the incubator in the last years. This is a very positive development but over the last years we experienced that chinese communities often have different needs and challenges when learning to adopt to the Apache Way. One very important example is the language barrier which is present as many chinese developers are not fluent in english or not as fluent as most developers from western countries. We feel that the IoTDB community was really successful in adopting the Apache Way and in this talk we want to share our approaches and our learnings.\\n</p>\\n\\n<p><em>\\nJulian Feinauer joined the IoTDB Community as one of the first external Contributors.\\n</em></p>","categories":"Incubator","url":"https://apachecon.com/acah2020/tracks/incubator.html#T1655"},{"uid":"acah2020-incubator-T1755@apachecon.com","sequence":"1","dtstamp":"20200810T143451Z","dtstart":"20200929T173500Z","dtend":"20200929T181500Z","summary":"Past\\, now and future about Apache YuniKorn (incubating): Cloud-Native resource scheduler","location":"@home","x-alt-desc":"<strong>\\nWilfred Spiegelenburg\\nWangda Tan\\n</strong>\\n<p>\\nApache YuniKorn (Incubating) is a light-weight\\, universal resource scheduler for container orchestrator systems. It was created to achieve fine-grained resource sharing for various workloads efficiently on a large scale\\, multi-tenant\\, and cloud-native environment. YuniKorn brings a unified\\, cross-platform\\, scheduling experience for mixed workloads that consist of stateless batch workloads and stateful services. YuniKorn now supports K8s and can be deployed as a custom K8s scheduler. YuniKorn's architecture design also allows adding different shim-layer and adapt to different ResourceManager implementation including Apache Hadoop YARN\\, or any other systems. For this talk\\, we will talk about gaps in resource scheduling in Cloud-Native environment\\, and how YuniKorn can support running big data applications (like Spark/Flink/Tensorflow\\, etc.) on K8s. We will talk about existing and upcoming features of YuniKorn (including hierarchical of queues\\, resource fairness\\, gang scheduling support\\, integration with K8s features\\, quota management\\, autoscaling\\, etc.). We will also share how YuniKorn being used in community partners such as Alibaba\\, Cloudera\\, Lyft. \\n</p>\\n\\n<p><em>\\nWilfred is a Staff Software Engineer from Cloudera in Australia. Hes also PMC member of Apache YuniKorn (incubating)\\, Apache Hadoop committer. He has worked on Hadoop for 6 years mainly on YARN\\, MapReduce\\, and Spark. Before Cloudera\\, he has worked for SUN Microsystems and Oracle as part of the Identity Management teams as a developer and consultant for over 10 years. Wilfred started his career as a lecturer at the Amsterdam University of Applied Science\\; teaching\\, designing\\, and implementing multiple IT systems. Wilfred holds a Master's in Decision Support Systems from Sunderland University.\\n\\nWangda Tan is Sr. Manager of Compute Platform engineering team @ Cloudera\\, responsible for all engineering efforts related to Kubernetes\\, Apache Hadoop YARN\\, Resource Scheduling\\, and internal container cloud. In the open-source world\\, he's a member of Apache Software Foundation (ASF)\\, PMC Chair of Apache Submarine project\\, He is also project management committee (PMC) members of Apache Hadoop\\, Apache YuniKorn (incubating). Before joining Cloudera\\, he leads High-performance-computing on Hadoop related work in EMC/Pivotal. Before that\\, he worked in Alibaba Cloud and participated in the development of a distributed machine learning platform (later became ODPS XLIB). \\n</em></p>","categories":"Incubator","url":"https://apachecon.com/acah2020/tracks/incubator.html#T1755"},{"uid":"acah2020-incubator-T1815@apachecon.com","sequence":"1","dtstamp":"20200810T143451Z","dtstart":"20200929T181500Z","dtend":"20200929T185500Z","summary":"Apache Superset - A data visualization platform","location":"@home","x-alt-desc":"<strong>\\nMaxime Beauchemin\\n</strong>\\n<p>\\nThis talk explores Apache Superset through a live demo\\, and provides a high level understanding of what it offers as a product. We'll also share our journey thus far\\, and explore what it takes to grow an open source project\\, a community and a movement. We'll look at a retrospective of the design decisions\\, technology choices and engineering challenges that have shaped Superset. We'll also take a deep look into the current challenges the community is currently facing\\, and peak at what is ahead.\\n</p>\\n\\n<p><em>\\nMax Beauchemin has worked at the leading edge of data and analytics his entire career\\, helping shape the discipline in influential roles at data-dependent companies like Facebook\\, Airbnb\\, Lyft and Yahoo!. A leader in the open-source community\\, Max is the creator of Apache Airflow\\, an open-source tool for orchestrating complex computational workflows and data processing pipelines and Apache Superset\\, a popular open-source data visualization\\, exploration and analytics platform. More recently he founded Preset\\, a company devoted to building upon Superset to offer next generation analytics as a service.\\n</em></p>","categories":"Incubator","url":"https://apachecon.com/acah2020/tracks/incubator.html#T1815"},{"uid":"acah2020-incubator-W0900@apachecon.com","sequence":"0","dtstamp":"20200828T190237Z","dtstart":"20200930T090000Z","dtend":"20200930T094000Z","summary":"Apache Incubator\\, & How incubator communities are built ? (in Hindi language) [ALC Indore]","location":"@home","x-alt-desc":"<strong>\\nAditya Sharma\\n</strong>\\n<p>\\nThis talk will be part of Track prepared by ALC Indore\\, and *language for the talk will be Hindi*. The talk will include the details on Apache Incubator\\, how to it works\\, and the important topic will be how to build a community around your incubating project. ## What is Apache Incubator? Apache Incubator is the gateway for open-source projects intended to become fully-fledged Apache Software Foundation projects. -- History -- Incubation process -- Current incubating projects -- How to contribute to the incubating project? ## How to build a community for your incubating project? The community is core for the success of any open source project\\, in this topic I will share some tips which can help you Incubating project to grow its community. After all\\, it is Community over Code.  \\n</p>\\n\\n<p><em>\\nProject Management Committee (PMC) member at Apache OFBiz and Apache Roller\\, Apache Local Community (ALC) Indore Chapter Lead\\n</em></p>","categories":"Incubator","url":"https://apachecon.com/acah2020/tracks/incubator.html#W0900"},{"uid":"acah2020-incubator-W0940@apachecon.com","sequence":"0","dtstamp":"20200828T190237Z","dtstart":"20200930T094000Z","dtend":"20200930T102000Z","summary":"Apache APISIX: How to implement plugin orchestration in API GatewayEdit proposal","location":"@home","x-alt-desc":"<strong>\\nMing Wen\\n</strong>\\n<p>\\nApache APISIX is a cloud-native API gateway that provides the same plugin mechanism as other gateways. However\\, in Apache APISIX\\, plugin orchestration is also provided that allows users to control the conditions and order for running plugins. Apache APISIX uses DAG(Directed Acyclic Graph) to implement this feature. In this share\\, we'll introduce Apache APISIX\\, and use a few examples to explain the advantages of plugin orchestration\\, and the specific implementation. \\n</p>\\n\\n<p><em>\\nPPMC member of Apache APISIX CEO of ZhiLiu Technology Co.\\, Ltd\\, China Speaker of ApcheCon EU 2016\\n</em></p>","categories":"Incubator","url":"https://apachecon.com/acah2020/tracks/incubator.html#W0940"},{"uid":"acah2020-incubator-W1020@apachecon.com","sequence":"0","dtstamp":"20200828T190237Z","dtstart":"20200930T102000Z","dtend":"20200930T110000Z","summary":"ECharts: a storyteller of visualization evolution","location":"@home","x-alt-desc":"<strong>\\nWenli Zhang\\n</strong>\\n<p>\\nWith the increasing demand for data visualization and a deeper understanding of theories\\, the role of data visualization tools has changed dramatically over the years. Previously\\, the main expectation was to help users understand abstract data through a static chart. Later\\, interactive tools were introduced to help users better understand the relationships between data. Today\\, another important aspect of our expectations is the ability to tell the stories. We expect visualization tools to help users explore and think about the story behind the data and get inspired or motivated to take further steps after reading the charts. In this sharing\\, we will introduce why and how Apache ECharts (incubating) has evolved to adapt to the changing needs and formed a modern visualization tool as you see today. \\n</p>\\n\\n<p><em>\\nWenli is a data visualization developer and PPMC of Apache ECharts (incubating). She has open-sourced many data visualization projects on GitHub (ID: Ovilia) and is enthusiastic about open-source community.\\n</em></p>","categories":"Incubator","url":"https://apachecon.com/acah2020/tracks/incubator.html#W1020"},{"uid":"acah2020-incubator-W1615@apachecon.com","sequence":"2","dtstamp":"20200810T143451Z","dtstart":"20200930T161500Z","dtend":"20200930T165500Z","summary":"Hatching the Clutch - A Guide to the Apache Incubator","location":"@home","x-alt-desc":"<strong>\\nDave Fisher\\n</strong>\\n<p>\\nPodlings are said to be part of the Clutch. On a daily basis the status of podlings is evaluated from available information. This talk will describe the clutch evaluation process and how that feeds into the Incubator website. Additional topics: Whimsy and Podling information. Bootstrapping a Podling. Updating podling status. \\n</p>\\n\\n<p><em>\\nApache Software Foundation Member and Incubator Mentor.\\n</em></p>","categories":"Incubator","url":"https://apachecon.com/acah2020/tracks/incubator.html#W1615"},{"uid":"acah2020-incubator-W1655@apachecon.com","sequence":"2","dtstamp":"20200810T143451Z","dtstart":"20200930T165500Z","dtend":"20200930T173500Z","summary":"Advice to Incubator Mentors","location":"@home","x-alt-desc":"<strong>\\nJustin Mclean\\n</strong>\\n<p>\\nSo you signed up to become a mentor for a project? Do you know what it entails or what is expected of you? Your project is relying on you to help guild it to graduation. In this talk\\, I'll be giving an overview of the ASF incubation process\\, the pitfalls to watch out for\\, and how projects become successful. I'll focus on everyday situations and challenges that podlings face and what's a good way to deal for mentors to deal with them. This talk is is for anyone who is thinking of being a mentor\\, is currently a mentor or for projects wanting a smooth path to graduation. \\n</p>\\n\\n<p><em>\\nJustin Mclean has more than 25 years experience in developing web-based applications and is heavily involved in open source hardware and software. He runs his own consulting company Class Software and has spoken at numerous conferences in Australia and overseas. In his free time\\, he's active in several Apache Software Foundation projects\\, including the Apache Incubator\\, and is a mentor for a number of their projects. He's currently the chair of the Apache Incubator and on the ASF board. He also teaches at an online college and runs the IoT meetup in Sydney. \\n</em></p>","categories":"Incubator","url":"https://apachecon.com/acah2020/tracks/incubator.html#W1655"},{"uid":"acah2020-incubator-W1735@apachecon.com","sequence":"1","dtstamp":"20200810T143451Z","dtstart":"20200930T173500Z","dtend":"20200930T181500Z","summary":"Daffodil - Kill the Data Format Problem","location":"@home","x-alt-desc":"<strong>\\nMichael Beckerle\\n</strong>\\n<p>\\nDaffodil is an incubator project. Its goal is killing the data format problem by providing an implementation of DFDL (Data Format Description Language - an emerging standard from the Open Grid Forum) that we can all use and extend\\, and integrate into all our data-consuming frameworks and applications. This talk will use numerous compelling examples of Daffodil parsing\\, and unparsing (reconstructing) data in a variety of data formats - textual and binary\\, industry standard formats\\, and ad-hoc one-of-a-kind formats as well\\, and using both XML and JSON to make the data tangible and visible. \\n</p>\\n\\n<p><em>\\nApache commiter since 2017. Currently applying knowledge of scalable computing systems and data format issues at Owl Cyber Defense Solutions (formerly Tresys Technology). Since 2002\\, Co-chair DFDL Workgroup of Open Grid Forum - working towards a standard for data format description so we can all stop solving this problem over and over again. Former life as a CTO of a few small/startup companies. Likes to program in Scala and to do data archeology to figure out data from just the bits. \\n</em></p>","categories":"Incubator","url":"https://apachecon.com/acah2020/tracks/incubator.html#W1735"},{"uid":"acah2020-incubator-W1935@apachecon.com","sequence":"0","dtstamp":"20200921T175745Z","dtstart":"20200930T193500Z","dtend":"20200930T201500Z","summary":"Teaclave: A Universal Secure Computing Platform","location":"@home","x-alt-desc":"<strong>\\nMingshen Sun\\n</strong>\\n\\n<p>\\nApache Teaclave (incubating) is a universal secure computing platform to make computation on privacy-sensitive data safe and secure. The platform adopts multiple security technologies to enable secure computing\\, in particular\\, Teaclave uses Intel SGX to serve the most security-sensitive tasks with hardware-based isolation\\, memory encryption and attestation. Teaclave is provided as a function-as-a-service platform and with many built-in functions\\, it supports a wide variety of tasks on sensitive data\\, such as privacy preserving machine learning\\, private set intersection\\, and cryptographic computation. More importantly\\, unlike traditional FaaS\\, Teaclave supports both general secure computing tasks and flexible single- and multi-party secure computation. Last but not least\\, Teaclave is written in Rust to prevent memory-safety issues. Teaclave entered the Apache Incubator in August 2019. In this talk\\, we would like to introduce this project to the whole community for the first time. We will discuss some motivation and background of the secure computing ecosystem. Then\\, we will present highlights of the Teaclave platform and its internal design\\, talk about the roadmap of incubating and current progress\\, and finally\\, introduce current status of Teaclave community. \\n</p>\\n\\n<p><em>\\nMingshen Sun works at Baidu and is a member of Apache Teaclave (incubating) PPMC (Podling Project Management Committee). He leads\\, maintains and actively contributes to several open source projects including Teaclave\\, MesaPy\\, Rust OP-TEE TrustZone SDK\\, etc. Please visit his homepage (https://mssun.me) for more information.\\n</em>\\n</p>","categories":"Incubator","url":"https://apachecon.com/acah2020/tracks/incubator.html#W1935"},{"uid":"acah2020-iot-T1615@apachecon.com","sequence":"1","dtstamp":"20200810T143451Z","dtstart":"20200929T161500Z","dtend":"20200929T165500Z","summary":"IndustryFusion: The democratization of Industry 4.0","location":"@home","x-alt-desc":"<strong>\\nKonstantin Kernschmidt\\, Matt Mikulina\\, Marcel Wagner\\n</strong>\\n<p>\\nSmall and mediumsized machine manufacturers (SMEs) currently are undergoing a tremendous transformation process. For decades high quality machines were the main development focus and precision on the scale of a human hair made the engineers' hearts beat faster. However\\, growing global competition\\, changed customer requirements and the wish to extend innovation leadership require that additional data-driven services are offered to the cutsomers in addition to selling the machines. Implementing an IIoT-solution for their machines confronts the companies with two major challenges: 1. SMEs do not have the ressources to experiment with proprietary IIoT-solutions\\, pushing them into an undesirable vendor lock-in. 2. The solution has to be interoperable with the solutions from other manufacturers\\, as usually up to 100 different machines are present in a factory and the customers only want to have one transparent Smart Factory and not 100 different digital solutions. In order to achieve an IIoT-solution that fits these needs of SMEs\\, a growing group of innovative machine manufacturers teamed up with IT- and Open-Source experts to implement IndustryFusion\\, a cross-manufacturer interoperable open source solution for Industry 4.0. IndustryFusion is an Apache 2.0 licensed\\, fully deployable End-2-End IIoT-solution covering all required layers - i.e. perception\\, network\\, middleware\\, application - for implementing a digital ecosystem. The architecture intergrates several Apache projects(PLC4X\\, Kafka\\, Cassandra\\, Beam\\, Flink) and can either run entirley on premises\\, using StarlingX\\, or be deployed in any cloud environment.\\n</p>\\n\\n<p><em>\\nKonstantin Kernschmidt:<br />\\nKonstantin Kernschmidt is passionate about the digital transformation of small and medium-sized enterprises. He has a broad experience in mechanical engineering\\, IT and smart factory solutions. Konstantin is head of Research & Development / Industry 4.0 at MicroStep Europa GmbH and the technical lead of IndustryFusion. Prior to his current position\\, he was general manager of a cross-disciplinary research center focusing on innovation processes and new business models in the context of Industry 4.0. He holds a PhD in automation and information systems as well as a diploma in mechanical engineering and management from the Technical University of Munich (TUM).<br />\\nMatt Mikulina:<br />\\nAt MicroStep Europa - a manufacturer of high-end CNC cutting systems - he accompanied the digital transformation of key business processes. In his new role within the IndustryFusion Team\\, besides the brand communication\\, he passionately takes care of the user experience & application design\\, streamlining processes and building a scalable solution.<br />\\nMarcel Wagner:<br />\\nMarcel Wagner is Software Application Engineer in Intel's IoT Group. In this role\\, he works with cusomters on Open Source Edge-Cloud platforms and Cloud Native architectures\\, with focus on Industrial IoT. He contributed to open source projects like StarlingX\\, the OpenStack open source Edge-Cloud\\, and Open IoT Service Platform\\, an open source cloud platform which is based on Apache projects like Kafka\\, Beam\\, Flink\\, and Casssandra. Before joining Intel\\, Marcel was researching at Siemens Corporate Technology and Nokia Networks on video transmission protocols and distributes applications. Marcel holds a Dr. rer. nat from the University of Freiburg\\, Germany\\, and a master of science (Dipl. Inform.) from the Karlsruhe Institute of Technology.\\n</em></p>","categories":"IoT","url":"https://apachecon.com/acah2020/tracks/iot.html#T1615"},{"uid":"acah2020-iot-T1655@apachecon.com","sequence":"1","dtstamp":"20200810T143451Z","dtstart":"20200929T165500Z","dtend":"20200929T173500Z","summary":"Apache StreamPipes  Flexible Industrial IoT Management","location":"@home","x-alt-desc":"<strong>\\nPatrick Wiener\\n</strong>\\n<p>\\nEmerging data-driven use cases in the manufacturing business often require continuous integration and analysis of sensor data to identify time-critical situations. Apache StreamPipes is a new project in the Apache Incubator which aims at providing a self-service industrial IoT toolbox to enable non-technical users to connect\\, analyze and explore IoT data streams. It provides many connectors for industrial communication protocols and a library of reusable algorithms to analyze sensor measurements or camera images based on simple rules up to machine learning methods. A variety of data sinks allow for easy exchange with third party systems\\, including many Apache IoT and Big Data projects (including Apache PLC4X\\, Apache Kafka\\, Apache IoTDB). In this talk\\, we give an overview of Apache StreamPipes (incubating) and interactively show how to extend the IoT toolbox and create a custom data processor using the integrated Software Development Kit.\\n</p>\\n\\n<p><em>\\nPatrick Wiener currently works at the FZI Research Center for Information Technology in Karlsruhe. His research interests include Distributed Computing (Cloud\\, Edge/Fog Computing)\\, IoT\\, and Stream Processing. Patrick is an expert for infrastructure management such as containers and container orchestration frameworks. He has worked in several public-funded research projects related to Big Data Management and Stream Processing in domains such as manufacturing\\, logistics and geographical information systems.\\n</em></p>","categories":"IoT","url":"https://apachecon.com/acah2020/tracks/iot.html#T1655"},{"uid":"acah2020-iot-T1735@apachecon.com","sequence":"1","dtstamp":"20200810T143451Z","dtstart":"20200929T173500Z","dtend":"20200929T181500Z","summary":"Analyzing IIoT data with PLC4X and StreamPipes","location":"@home","x-alt-desc":"<strong>\\nPhilipp Zehnder\\, Christofer Dutz\\n</strong>\\n<p>\\nThe adoption of the Industrial Internet of Things (IIoT) in manufacturing companies is constantly increasing. Apache software and other open source efforts play a key role in creating value from such data\\, from connecting machines to processing streaming data and storing it in databases. There are several successful projects within the Apache Foundation that can be used as building blocks to create a tailor-made IIoT solution for your company. In this presentation\\, we will show how Apache StreamPipes (incubating) can be used as a solution that already provides a flexible infrastructure for IIoT data analytics. It is an out of the box solution\\, consisting of several microservices\\, which allow domain experts to easily analyze data streams. Therefore\\, it is closely integrated with several other Apache projects\\, e.g. PLC4X\\, Flink or IoTDB. We present how we have implemented this integration and show the advantages of the cooperation of different Apache projects. The aim of our demonstration is to detect faulty parts on the basis of sensor values. First\\, we show how to realize machine connectivity with Apache PLC4X. Then we pre-process data with StreamPipes pipelines and store it in a time-series database (Apache IoTDB). After that\\, we introduce how domain knowledge can be used to define a rule for classifying parts to detect quality deviations. In addition\\, for cases where a simple rule cannot be defined\\, we will use a machine learning model\\, trained on the previously collected data.\\n</p>\\n\\n<p><em>\\nPhilipp Zehnder:<br />\\nPhilipp Zehnder is a research scientist at the FZI Research Center of Information Technology. His current research interests are in the areas of Distributed Stream Processing and Streaming Machine Learning. He is very interested in open source software\\, especially in the field of IIoT\\, and is involved in the Apache StreamPipes (incubation) project.<br />\\nChristofer Dutz:<br />\\nFull blooded Apache and Open-Source enthusiast. Invests all of his work and private time in multiple Apache Projects. Deeply interested in the IoT Area he is currently VP of the Apache PLC4X project and deeply involved in Apache Edgent (incubator) as well as mentor to the Apache IoTDB (incubating) podling.\\n</em></p>","categories":"IoT","url":"https://apachecon.com/acah2020/tracks/iot.html#T1735"},{"uid":"acah2020-iot-T1815@apachecon.com","sequence":"1","dtstamp":"20200810T143451Z","dtstart":"20200929T181500Z","dtend":"20200929T185500Z","summary":"Using the Mm FLaNK Stack for Edge AI (Apache MXNet\\, Apache Flink\\, Apache NiFi\\, Apache Kafka\\, Apache Kudu)","location":"@home","x-alt-desc":"<strong>\\nTimothy Spann\\n</strong>\\n<p>\\nToday\\, data is being generated from devices and containers living at the edge of networks\\, clouds and data centers. We need to run business logic\\, analytics and deep learning at the edge before we start our real-time streaming flows. Fortunately using the all Apache Mm FLaNK stack we can do this with ease! Streaming AI Powered Analytics From the Edge to the Data Center is now a simple use case. With MiNiFi we can ingest the data\\, do data checks\\, cleansing\\, run machine learning and deep learning models and route our data in real-time to Apache NiFi and/or Apache Kafka for further transformations and processing. Apache Flink will provide our advanced streaming capabilities fed real-time via Apache Kafka topics. Apache MXNet models will run both at the edge and in our data centers via Apache NiFi and MiNiFi. Our final data will be stored in Apache Kudu via Apache NiFi for final SQL analytics. We can now solve IoT problems with a scalable all Apache solution that incorporates real-time streaming\\, analytics and AI. Tools Apache Flink\\, Apache Kafka\\, Apache NiFi\\, MiNiFi\\, Apache MXNet\\, Apache Kudu\\, Apache Impala\\, Apache HDFS References https://www.datainmotion.dev/2019/08/rapid-iot-development-with-cloudera.html https://www.datainmotion.dev/2019/09/powering-edge-ai-for-sensor-reading.html https://www.datainmotion.dev/2019/05/dataworks-summit-dc-2019-report.html https://www.datainmotion.dev/2019/03/using-raspberry-pi-3b-with-apache-nifi.html\\n</p>\\n\\n<p><em>\\nTim Spann is a Field Engineer at Cloudera in the Data in Motion Team where he works with Apache NiFi\\, MiniFi\\, Kafka\\, Kafka Streams\\, Edge Flow Manager\\, MXNet\\, TensorFlow\\, Apache Spark\\, Big Data\\, IoT\\, Cloud\\, Machine Learning\\, and Deep Learning. Tim has over a decade of experience with the IoT\\, big data\\, distributed computing\\, streaming technologies\\, and Java programming. Previously\\, he was a senior solutions architect at AirisData and a senior field engineer at Pivotal. He blogs for DZone\\, where he is the Big Data Zone leader\\, and runs a popular meetup in Princeton on big data\\, IoT\\, deep learning\\, streaming\\, NiFi\\, blockchain\\, and Spark. Tim is a frequent speaker at conferences such as IoT Fusion\\, Strata\\, ApacheCon\\, Data Works Summit Berlin\\, DataWorks Summit Sydney\\, DataWorks Summit DC\\, DataWorks Summit Barcelona and Oracle Code NYC. He holds a BS and MS in computer science.\\n</em></p>","categories":"IoT","url":"https://apachecon.com/acah2020/tracks/iot.html#T1815"},{"uid":"acah2020-iot-T1935@apachecon.com","sequence":"1","dtstamp":"20200810T143451Z","dtstart":"20200929T193500Z","dtend":"20200929T201500Z","summary":"Use cases and optimizations of IoTDB","location":"@home","x-alt-desc":"<strong>\\nJialin Qiao\\n</strong>\\n<p>\\nApache IoTDB is a high performance database for time-series data management on the edge and cloud for Internet of Things. This talk will introduce some use cases of IoTDB\\, including Meteorological station data management\\, Subway data management and power plants monitoring applications. The read/write performance optimization and database tunning are also involved.\\n</p>\\n\\n<p><em>\\nPh.D student of school of software\\, Tsinghua University. Expert in IoTDB's storage engine\\, query engine and application implementation on IoTDB.\\n</em></p>","categories":"IoT","url":"https://apachecon.com/acah2020/tracks/iot.html#T1935"},{"uid":"acah2020-iot-W1615@apachecon.com","sequence":"1","dtstamp":"20200810T213949Z","dtstart":"20200930T161500Z","dtend":"20200930T165500Z","summary":"How to Become an IoT Developer (and Have Fun!)","location":"@home","x-alt-desc":"<strong>\\nJustin Mclean\\n</strong>\\n<p>\\nI started off my life as a developer writing machine code and C and working on some low-level hardware projects. Then this thing called the internet come along and I moved into the web application space for a couple of decades. More recently I've moved back into commercial IoT development and not unexpectedly a lot has changed over that time. In this talk\\, I'll cover what it's like developing IoT projects. I'll go over the tools you need and the protocols you need to be familiar with. I'll look at how the C language has evolved to what it is today and how to write code that works well on memory constrained devices. I'll go over producing prototypes\\, rapid development\\, debugging and testing embedded applications and what and how much electronics you should learn. In short\\, everything you need to know in becoming an IoT developer and have fun doing it.\\n</p>\\n\\n<p><em>\\nJustin Mclean has more than 25 years experience in developing web-based applications and is heavily involved in open source hardware and software. He runs his own consulting company Class Software and has spoken at numerous conferences in Australia and overseas. In his free time\\, he's active in several Apache Software Foundation projects\\, including the Apache Incubator\\, and is a mentor for a number of their projects. He is also current the chair of the Apache Incubator and on the ASF board. He also teaches at an online college and runs the IoT meetup in Sydney.\\n</em></p>","categories":"IoT","url":"https://apachecon.com/acah2020/tracks/iot.html#W1615"},{"uid":"acah2020-iot-W1655@apachecon.com","sequence":"1","dtstamp":"20200810T213949Z","dtstart":"20200930T165500Z","dtend":"20200930T173500Z","summary":"Home automation with Apache","location":"@home","x-alt-desc":"<strong>\\nChristofer Dutz\\n</strong>\\n<p>\\nEven if Apache PLC4X was initiated in order to communicate with industrial hardware\\, in the last years it has grown to also allow communication with building and home-automation systems. In this talk I'd like to demonstrate how I use Apache PLC4X to communicate with the KNX\\, Modbus and Luxtronic2 drivers to talk to my house and how easy it is to store this data in Apache IoTDB (Incubating) to process it with Apache Camel\\, Apache NiFi\\, Apache Edgent (Incubating) (RIP) or others and to create a Frontend using Apache Royale (perhaps even with some nifty Apache ECharts (incubating) diagrams).\\n</p>\\n\\n<p><em>\\nFull blooded Apache Member\\, who likes to think out of the box. If others say something's impossible\\, that's when Chris starts to become interested. He's involved in numerous Apache and even more non-Apache projects and currently serving as the VP of Apache PLC4X which he had the pleasure and the luck to write the first lines of code for (ok ... and a \\\"few\\\" after that).\\n</em></p>","categories":"IoT","url":"https://apachecon.com/acah2020/tracks/iot.html#W1655"},{"uid":"acah2020-iot-W1735@apachecon.com","sequence":"1","dtstamp":"20200810T213949Z","dtstart":"20200930T173500Z","dtend":"20200930T181500Z","summary":"Apache PLC4X or: How I Learned to Stop Worrying and Love the Industrial IoT","location":"@home","x-alt-desc":"<strong>\\nJulian Feinauer\\n</strong>\\n<p>\\nThe Apache PLC4X project left the incubator last year and is one of the younger projets of the ASF. It is a set of libraries for communicating with industrial programmable logic controllers (PLCs) using a variety of protocols but with a shared API. At pragmatic minds we had the first (known) productive deployments of PLC4X in industrial projects. As many may know\\, there still is a gap between the very IT affine Open Source world and the OT or shop floor world in the industry. So\\, at the beginning\\, these projects sometimes felt like Alices adventures when she fell down the rabbit hole. During these projects we entered a world that is completely different\\, sometimes strange but very exciting.\\n</p>\\n\\n<p><em>\\nJulian Feinauer studied mathematics at the university of Stuttgart and received his PhD in mathematics at Ulm University. Besides his interest in open source and big data he had many contacts with timeseries data\\, storage and evaluation. In 2016 he founded the company pragmatic industries GmbH with focus on industrial iot and industry data processing.\\n</em></p>","categories":"IoT","url":"https://apachecon.com/acah2020/tracks/iot.html#W1735"},{"uid":"acah2020-iot-W1815@apachecon.com","sequence":"1","dtstamp":"20200810T213949Z","dtstart":"20200930T181500Z","dtend":"20200930T185500Z","summary":"Solving IoT and Edge connectivity with Apache projects","location":"@home","x-alt-desc":"<strong>\\nDejan Bosanac\\, Hugo Guerrero\\n</strong>\\n<p>\\nIoT and Edge solutions are all about connecting distributed systems together. But different use cases need different kinds of communication technologies. Luckily\\, Apache Software Foundation hosts multiple projects in this domain that can solve even the most challenging problems. Bonus point? They work great together as well\\, providing a great foundation layer for all your needs. In this session we'll discuss common communication patterns and where they fit IoT and Edge solutions. We'll dig into the Apache projects that enable them\\, such as Kafka\\, Qpid dispatch router and ActiveMQ. We'll discuss the differences and show where different approaches make the most sense. Finally\\, we'll explore how these projects can work together and provide a foundation layer for a wider ecosystem targeting specifically IoT and Edge use cases. We'll give a brief architecture of Eclipse Hono\\, EnMasse\\, Strimzi and Skupper projects. All based on Apache technologies. We'll see their benefits and place in the wider cloud IoT and Edge ecosystems.\\n</p>\\n\\n<p><em>\\nDejan Bosanac:<br />\\nDejan Bosanac is an engineer at Red Hat with broad expertise in messaging and integration technologies. Hes been an active member of open source communities for many years and a contributor to various projects. His latest interests revolve around open source IoT cloud and Edge computing solutions.<br />\\nHugo Guerrero:<br />\\nHugo Guerrero works at Red Hat as an APIs and messaging developer advocate. In this role\\, he helps the marketing team with technical overview and support to create\\, edit\\, and curate product content shared with the community through webinars\\, conferences\\, and other activities. With more than 15 years of experience as a developer\\, consultant\\, architect\\, and software development manager\\, he also works on open source software with major private and federal public sector clients in Latin America.\\n</em></p>","categories":"IoT","url":"https://apachecon.com/acah2020/tracks/iot.html#W1815"},{"uid":"acah2020-iot-W1855@apachecon.com","sequence":"2","dtstamp":"20200810T213949Z","dtstart":"20200930T185500Z","dtend":"20200930T193500Z","summary":"Utilizing Apache NiFi and MiNiFi for EdgeAI IoT at Scale","location":"@home","x-alt-desc":"<strong>\\nTimothy Spann\\, Sunile Manjee\\n</strong>\\n<p>\\nA hands-on deep dive on using Apache NiFi + Edge Flow Manager + MiniFi Agents with Apache MXNet\\, OpenVino\\, TensorFlow Lite\\, and other Deep Learning Libraries on the actual edge devices including Raspberry Pi with Movidius 2\\, Google Coral TPU\\, NVidia Jetson Xavier\\, and NVidia Jetson Nano. We run deep learning models on the edge devices and send images\\, capture real-time GPS and sensor data. With our low coding IoT applications providing easy edge routing\\, transformation\\, data acquisition and alerting before we decide what data to stream real-time to our data space. These edge applications classify images and sensor readings real-time at the edge and then send Deep Learning results to Apache NiFi for transformation\\, parsing\\, enrichment\\, querying\\, filtering and merging data to various Apache data stores including Apache Kudu and Apache HBase. https://www.datainmotion.dev/2019/08/updating-machine-learning-models-at.html\\n</p>\\n\\n<p><em>\\nTim Spann is a Principal Field Engineer at Cloudera in the Data in Motion Team where he works with Apache NiFi\\, MiniFi\\, Kafka\\, Kafka Streams\\, Edge Flow Manager\\, MXNet\\, TensorFlow\\, Apache Spark\\, Big Data\\, IoT\\, Cloud\\, Machine Learning\\, and Deep Learning. Tim has over a decade of experience with the IoT\\, big data\\, distributed computing\\, streaming technologies\\, and Java programming. Previously\\, he was a senior solutions architect at AirisData and a senior field engineer at Pivotal. He blogs for DZone\\, where he is the Big Data Zone leader\\, and runs a popular meetup in Princeton on big data\\, IoT\\, deep learning\\, streaming\\, NiFi\\, blockchain\\, and Spark. Tim is a frequent speaker at conferences such as IoT Fusion\\, Strata\\, ApacheCon\\, Data Works Summit Berlin\\, DataWorks Summit Sydney\\, DataWorks Summit DC\\, DataWorks Summit Barcelona and Oracle Code NYC. He holds a BS and MS in computer science.<br />\\nSunile: As a open source first champion in the Data of Anything space\\, I have lead and enabled unreasonably successful data strategies for several premier fortune 100s. I simplify technical solutions through ubiquitous language for complex business challenges. Evangelism of open source adoption with a maniacal business centric solutions approach earned me the Hortonworks 2016 Technical Leadership Award. With paramount passion\\, I am a business enabler who has built from soup to nuts habitually secure\\, dynamic\\, scalable\\, distributed\\, versatile\\, and remunerative enterprise grade analytic and transactional solutions.\\n\\n\\n</em></p>","categories":"IoT","url":"https://apachecon.com/acah2020/tracks/iot.html#W1855"},{"uid":"acah2020-jena-W0940@apachecon.com","sequence":"0","dtstamp":"20200812T151913Z","dtstart":"20200930T094000Z","dtend":"20200930T102000Z","summary":"Apache Jena GeoSPARQL","location":"@home","x-alt-desc":"<strong>\\nMarco Neumann\\n</strong>\\n<p>\\nThis presentation will discuss an implementation of GeoSPARQL for Apache Jena and a Fuseki integration. GeoSPARQL adds spatial functions to the SPARQL query language and enables the processing of spatial data with the popular Apache Jena project. In this presentation basic filter and property functions will be discussed in context of spatial relations and geometry shapes or types for the use with the Resource Description Framework (RDF) and SPARQL. Apache Jena GeoSPARQL spatial filters can be a applied to Well-known text (WKT) representation of geometry objects and datasets using the WGS84 Geo predicates for latitude and longitude. The goal for the latest release of Apache Jena GeoSPARQL module was to follow generally the 11-052r4 OGC GeoSPARQL standard where possible while providing an easy to use extension for Apache Jena users.\\n</p>\\n\\n<p><em>\\nMarco Neumann is an Information Scientist with keen interest in distributed information syndication and contexts for the Semantic Web\\, dynamic schema evolution in structured data\\, information visualisation\\, ontology based knowledge management\\, reputation based ranking in Semantic Social Networks (augmented collaborative online communities such as http://www.lotico.com)\\, and last but not least the Semantic GeoSpatial Web. Since 2005 Marco applies his experiencing to large-scale information management projects in international cultural heritage institutions and the private sector.\\n</em></p>","categories":"Jena","url":"https://apachecon.com/acah2020/tracks/jena.html#W0940"},{"uid":"acah2020-jena-W1735@apachecon.com","sequence":"0","dtstamp":"20200812T151913Z","dtstart":"20200930T173500Z","dtend":"20200930T181500Z","summary":"XML -> JSON -> RDF : Another iteration in data format evolution","location":"@home","x-alt-desc":"<strong>\\nClaude Warren\\n</strong>\\n<p>\\nStarting with a brief history of web and micro server data serialization formats\\, this talk looks at the advantages of using RDF as the data format for web and micros service processing. An example of the processing as performed in a live application is presented. The talk demonstrates how RDF processed by Jena can deliver a clean\\, extensible data format with simple merge characteristics and mechanisms for reasoning.\\n</p>\\n\\n<p><em>\\nClaude Warren is a Senior Software Engineer with over 30 years experience. He currently lives in Galway\\, Ireland where he works on innovative solutions to technical problems. He is also a Comitter and Project Management Committee member on the Apache Jena project and has several small open source projects on Github. He has presented papers at several conferences and has several papers published both in the popular IT press and in refereed journals. He is a founding member of the Denver Mad Scientists Club and winner of the original Critter Crunch competition.\\n</em></p>","categories":"Jena","url":"https://apachecon.com/acah2020/tracks/jena.html#W1735"},{"uid":"acah2020-jena-W1815@apachecon.com","sequence":"0","dtstamp":"20200812T151913Z","dtstart":"20200930T181500Z","dtend":"20200930T185500Z","summary":"SHACL in Apache Jena","location":"@home","x-alt-desc":"<strong>\\nAndy Seaborne\\n</strong>\\n<p>\\nRDF databases holds data in a schema-less fashion. Adding new RDF data from new sources does not require existing data to be reorganised or redesigned\\, nor do applications using the existing data need to be changed. Information about what the data looks like is not part of the database and not enforced\\, leaving it to the application to deal with data mistakes such as bad formats\\, or missing information needed by the application. This in turn makes writing applciations more cumbersome because checking data is fit for the applications purpose needs to be performed. An approach that is gaining ground is data shapes\\; higher level descriptions of the RDF data that say which RDF triples are expected\\, and what the format of data values is required to be. SHACL is the W3C standard for expressing data shapes for such validation tasks This talk will introduce the SHACL standard and show how it can be used with Apache Jena.\\n</p>\\n\\n<p><em>\\nAndy works on infrastructure for RDF data systems. He has been an specification editor in the SPARQL standardization process at W3C for both the original SPARQL 1.0 and also SPARQL 1.1 standards. Within the Apache Jena project\\, he contributes to the query engine and SPARQL server\\, ensuring that complete implementations of standards are available.\\n</em></p>","categories":"Jena","url":"https://apachecon.com/acah2020/tracks/jena.html#W1815"},{"uid":"acah2020-jena-W1855@apachecon.com","sequence":"0","dtstamp":"20200812T151913Z","dtstart":"20200930T185500Z","dtend":"20200930T193500Z","summary":"Buddhist Digital Archives (BUDA)\\, RDF and jena-text","location":"@home","x-alt-desc":"<strong>\\nChris Tomlinson\\, lie Roux\\n</strong>\\n<p>\\nBUDA is s Linked Data Platform built on Jena-Fuseki using RDF and Jenas Lucene integration\\, jena-text. The platform enables collaboration in digital humanities among a variety of partners and leverages RDF and IIIF to provide open-access to a vast collection of textual materials and cultural heritage metadata about these materials.\\n</p>\\n\\n<p><em>\\nChris Tomlinson:<br />\\nSenior Technologist\\, working with BDRC (tbrc.org) for 18 years developing systems for the preservation\\, access and distribution of Buddhist texts and their cultural context. Developed contributions to Jena in support of the multilingual needs of BUDA.<br />\\nlie Roux:<br />\\nProject Lead\\, working with BDRC for 4 years\\, with experience in open-source cultural preservation projects (such as Gregorian Chant score engraving software . Developed contributions to IIIF and other open-source activities for use in BUDA.\\n</em></p>","categories":"Jena","url":"https://apachecon.com/acah2020/tracks/jena.html#W1855"},{"uid":"acah2020-jena-W1935@apachecon.com","sequence":"0","dtstamp":"20200812T151913Z","dtstart":"20200930T193500Z","dtend":"20200930T201500Z","summary":"Semantic Graph BoF hosted by Apache Jena","location":"@home","x-alt-desc":"<strong>\\nCommunity Participation\\n</strong>\\n<p>\\nBirds of a feather meeting to discuss all things semantic graph. Who is using them? What issues have projects encountered and how have they overcome them? Could the Jena help in making transition to semantic graphs easier?\\n</p>\\n\\n<p><em>\\n...\\n</em></p>","categories":"Jena","url":"https://apachecon.com/acah2020/tracks/jena.html#W1935"},{"uid":"acah2020-karaf-T1615@apachecon.com","sequence":"1","dtstamp":"20200903T153024Z","dtstart":"20200929T161500Z","dtend":"20200929T165500Z","summary":"Apache Karaf\\, coming features and roadmap","location":"@home","x-alt-desc":"<strong>\\nJB Onofr\\n</strong>\\n<p>\\nApache Karaf is a great runtime\\, rock solid. If the user experience is appreciated\\, the developer experience could be much better. This talk will introduce the current state of the projects\\, including the subprojects\\, and will show the roadmap\\, especially about tooling and developer experience.\\n</p>\\n\\n<p><em>\\nJB is member of the Apache Software Foundation. He's PMC Chair for Apache Karaf and is working on about 20 projects (Apache Camel\\, ActiveMQ\\, ...).\\n</em></p>","categories":"Karaf","url":"https://apachecon.com/acah2020/tracks/karaf.html#T1615"},{"uid":"acah2020-karaf-T1655@apachecon.com","sequence":"1","dtstamp":"20200903T153024Z","dtstart":"20200929T165500Z","dtend":"20200929T173500Z","summary":"Apache Karaf\\, multi purpose runtime","location":"@home","x-alt-desc":"<strong>\\nJB Onofr\\n</strong>\\n<p>\\nApache Karaf is a perfect runtime for the cloud supporting several kind of programming model. While OSGi is supported for a while\\, now Karaf evolved to support new kind of framework like CDI\\, or even Spring Boot. Thanks for that\\, Karaf is a perfect multi purpose and multi tenant runtime\\, providing bunch of ready to use features.\\n</p>\\n\\n<p><em>\\nJB is ASF member\\, PMC Chair for Apache Karaf and involve in about 20 Apache projects.\\n</em></p>","categories":"Karaf","url":"https://apachecon.com/acah2020/tracks/karaf.html#T1655"},{"uid":"acah2020-karaf-T1735@apachecon.com","sequence":"1","dtstamp":"20200903T153024Z","dtstart":"20200929T173500Z","dtend":"20200929T181500Z","summary":"Design Resilient Microservices using Apache Karaf and CXF: practical experience","location":"@home","x-alt-desc":"<strong>\\nAndrei Shakirin\\n</strong>\\n<p>\\nDevelopment team just has finished the last feature after months of hard work. Does this mean the software is production ready now? What aspects need to be considered before deployment your Microservices to the production environment? What should you do in emergency situations on production? How to make your software more reliable and resilient by deployment in Cloud? What are the stability and resiliency patterns and anti-patterns? All these questions will be addressed in the talk. Based on practical experience\\, presenter will demonstrate the best engineering practices to design resilient software using Apache Karaf\\, CXF\\, Kafka\\, ActiveMQ and illustrate them with real life cases. The following topics will be covered in presentation:  Stability anti-patterns (chain reactions\\, cascading failures\\, blocked threads)  Stability patterns (Timeouts\\, Circuit Breaker\\, Bulkheads\\, Fail Fast\\, Async)  Clustering and Load Balancing  Logging and Monitoring  Production Diagnostic  Pooling and Caching  Load and Stress Testing\\n</p>\\n\\n<p><em>\\nAndrei is a software architect in the Talend team developing the open source Application Integration platform based on Apache projects. The areas of his interest are REST API design\\, Microservices\\, Cloud\\, resilient distributed systems\\, security and agile development. Andrei is PMC and committer of Apache CXF and committer of Syncope projects. He is member of OASIS S-RAMP Work Group and speaker at Java and Apache conferences. Last speaking experience:  DOAG 2019\\, Nov 2019\\, Nurnberg\\, Design Production-Ready Software  Karlsruhe Entwickertag 2017\\, Mai 2017\\, Karlsruhe\\, Microservices with OSGi  ApacheCon Europe 2016\\, Nov 2016\\, Seville\\, Microservices with Apache Karaf and Apache CXF: Practical Experience  ApacheCon Europe 2015\\, Oct 2015\\, Budapest\\, Create and Secure Your REST API with Apache CXF  ApacheCon Europe 2014\\, Nov 2014\\, Budapest\\, Design REST Services With CXF JAX-RS Implementation: Lessons Learned  WJAX 2011\\, Nov 2011\\, Munich\\, Apache Days\\, Enabling Services with Apache CXF\\n</em></p>","categories":"Karaf","url":"https://apachecon.com/acah2020/tracks/karaf.html#T1735"},{"uid":"acah2020-karaf-T1815@apachecon.com","sequence":"1","dtstamp":"20200903T153024Z","dtstart":"20200929T181500Z","dtend":"20200929T185500Z","summary":"Will it blend? Java agents and OSGi","location":"@home","x-alt-desc":"<strong>\\nRobert Munteanu\\n</strong>\\n<p>\\nJava agents are a little-known but extremely powerful part of the Java ecosystem. Agents are able to transform existing classes at runtime\\, allowing scenarios such as logging and monitoring\\, hot reload or gathering code coverage. However\\, their usage presents a number of pitfalls as well. In this talk we will present the steps of writing a java agent from scratch\\, indicate various common mistakes and pain points and draw conclusions on best practices. Special care will be taken to discuss how running in an OSGi environment affects Java agents and how we can best approach integration testing in a modular environment. After this talk participants will have a better understanding of the Java instrumentation API\\, how it fits in with OSGi runtimes and about should / should not be done with it.\\n</p>\\n\\n<p><em>\\nWorking as a Senior Computer Scientist in the AEM Cloud Foundation team at Adobe\\, Robert Munteanu is a software developer with a passion for open source. He is a member of the Apache Software Foundation and frequent contributor to many open source projects\\, notably Apache Sling and Apache Jackrabbit. Robert is a frequent conference speaker\\, having spoken at Devoxx\\, ApacheCon and EclipseCon\\, amongst others.\\n</em></p>","categories":"Karaf","url":"https://apachecon.com/acah2020/tracks/karaf.html#T1815"},{"uid":"acah2020-karaf-T1855@apachecon.com","sequence":"1","dtstamp":"20200903T153024Z","dtstart":"20200929T185500Z","dtend":"20200929T193500Z","summary":"Netflix: Finding middle ground between monolithic and microservice architectures.","location":"@home","x-alt-desc":"<strong>\\nDmitry Vasilyev\\, Saeid Mirzaei\\, George Ye\\n</strong>\\n<p>\\nThe world of business applications is evolving. Monolithic applications are being split up into smaller microservices and deployed into virtualized environments. Engineers are striving to achieve responsiveness\\, resilience and elasticity at the same time improving separation of concerns and deployments via CI. The tradeoffs are usually more complex operations\\, harder dependency testability\\, lower developer productivity in some cases and cognitive overhead as well as infrastructure costs. We will discuss how our team at Netflix is settling in the middle between monolithic and microservice architectures getting the best of the two worlds. Well go through different phases of the application development lifecycle from initiation to production deployment as well as well talk how Apache Karaf enables us to achieve the aforementioned properties of the systems we build.\\n</p>\\n\\n<p><em>\\nDmitry Vasilyev\\, Saeid Mirzaei\\, George Ye\\n</em></p>","categories":"Karaf","url":"https://apachecon.com/acah2020/tracks/karaf.html#T1855"},{"uid":"acah2020-keynotes-T1500@apachecon.com","sequence":"0","dtstamp":"20200928T121458Z","dtstart":"20200929T150000Z","dtend":"20200929T154000Z","summary":"The State of the Feather","location":"@home","x-alt-desc":"<strong>\\nDavid Nalley\\, President\\, The Apache Software Foundation\\n</strong>\\n<p>\\nThe annual report from the Apache Software Foundation\\n</p>\\n\\n<p><em>\\nDavid Nalley is the current President of the Apache Software Foundation\\n</em></p>\\n\\n\\n\\n<img src=\\\"/acah2020/images/keynote_huang.jpg\\\" width=\\\"150\\\" style=\\\"float: left\\; padding-right: 30px\\; padding-left: 30px\\;\\\" />","categories":"Keynotes","url":"https://apachecon.com/acah2020/tracks/keynotes.html#T1500"},{"uid":"acah2020-keynotes-T1515@apachecon.com","sequence":"5","dtstamp":"20200828T190548Z","dtstart":"20200929T151500Z","dtend":"20200929T155500Z","summary":"Why Build a Castle When You Can Create a Community<br />\\nAdvancing Satellite Data Analysis through Professional Open Source","location":"@home","x-alt-desc":"<strong>Thomas Huang\\, NASA Jet Propulsion Laboratory</strong><br />\\n\\n<p>Thomas Huang is a Technical Group Supervisor for the JPLs Data Product Generation Software group. He is also the Strategic Lead for Interactive Analytics for the JPL's National Space Technology Applications Program Office\\, the Principal Investigator on several NASA Cloud-based big data analytic projects\\, and the System Architect for the NASAs Sea Level Change Portal. As an expert in large-scale\\, distributed intelligent data systems\\, Thomas led both planetary and earth data system projects. Thomas was the Project Technologist for NASA's Physical Oceanography Distributed Active Archive Center (PO.DAAC).  As an advocate for free and open source software\\, Thomas led the open sourcing of many NASA-funded technologies. He is the architect and founder of the Apache Science Data Analytics Platform (SDAP) as a community-driven\\, Cloud-based Analytic Center Framework. As an expert in data management and big data architecture\\, Thomas is a frequent invited speaker and panelist at various Earth and Space Informatics and Open Source events. He recently delivered keynote addresses at the ESAs Conference on Big Data From Space (BiDS19) and the Australasian eResearch Organisations (AeRO)s Collaborative Conference on Computational & Data Intensive Science (C3DIS 2019). Thomas is a member of the NOAAs Data Archive and Access Requirements Working Group (DAARWG) of the NOAAs Science Advisory Board (SAB). As an educator\\, Thomas is also a Computer Science lecturer at the California State Polytechnic University\\, Pomona\\, and member of its Industry Advisory Board.</p>\\n\\n\\n<img src=\\\"/acah2020/images/keynote_shengwu.png\\\" width=\\\"150\\\" style=\\\"float: left\\; padding-right: 30px\\; padding-left: 30px\\;\\\" />","categories":"Keynotes","url":"https://apachecon.com/acah2020/tracks/keynotes.html#T1515"},{"uid":"acah2020-keynotes-T0900@apachecon.com","sequence":"1","dtstamp":"20200914T171859Z","dtstart":"20200929T090000Z","dtend":"20200929T094000Z","summary":"Apache grows in China","location":"@home","x-alt-desc":"<strong>\\nSheng Wu\\, Founding Engineer\\, Tetrate.io\\n</strong>\\n<p>\\nIn the Apache FY2020 report\\, China is on the top of the download statistics. More China initiated projects joined the incubator\\, and some of them graduated as the Apache TLP.\\nSheng joined the Apache community since 2017\\, in the past 3 years\\, he witnessed the growth of the open-source culture and Apache way in China.<br />\\nMany developers have joined the ASF as new contributors\\, committers\\, foundation members. Chinese enterprises and companies paid more attention to open source contributions\\, rather than simply using the project like before.\\nIn the keynote\\, he would share the progress about China embracing the Apache culture\\, and willing of enhancing the whole Apache community.\\n</p>\\n\\n<p><em>\\nSheng Wu is a founding engineer at tetrate.io\\, leads the observability for service mesh and hybrid cloud. A searcher\\, evangelist\\, and developer in the observability\\, distributed tracing\\, and APM.\\nHe is a member of the Apache Software Foundation. Love open source software and culture. Created the Apache SkyWalking project and being its VP and PMC member. Co-founder and PMC member of Apache ShardingSphere. \\nAlso as a PMC member of Apache Incubator and APISIX. He is awarded as Microsoft MVP\\, Alibaba Cloud MVP\\,  Tencent Cloud TVP.\\n</em></p>\\n\\n\\n\\n\\n<H3>Wednesday\\, September 30th</h3>\\n\\n<img src=\\\"/acah2020/images/keynote_fournier.jpg\\\" width=\\\"150\\\" style=\\\"float: left\\; padding-right: 30px\\; padding-left: 30px\\;\\\" />","categories":"Keynotes","url":"https://apachecon.com/acah2020/tracks/keynotes.html#T0900"},{"uid":"acah2020-keynotes-W1515@apachecon.com","sequence":"1","dtstamp":"20200828T191038Z","dtstart":"20200930T151500Z","dtend":"20200930T155500Z","summary":"Camille Fournier","location":"@home","x-alt-desc":"<strong>Two Sigma</strong><br />\\n\\n<p>\\nCamille Fournier is the head of Platform Engineering at Two Sigma\\, a financial company in New York City. Prior to joining Two Sigma she was theChief Technology Officer ofRent the Runway\\, a transformative brand that offers unprecedented access to designer fashion\\, disrupting the way millions of women get dressed.</p>\\n<p>She is an open source contributor and project committee member for both Apache ZooKeeper and the Dropwizard web framework. Prior to working for Rent the Runway\\, Camille served as a software engineer at Microsoft\\, and most recently\\, spent several years as a technical specialist at Goldman Sachs\\, creating distributed systems for managing risk analysis and firmwide infrastructure.</p>\\n<p>She has a BS in Computer Science from Carnegie Mellon University and an MS in Computer Science from the University of Wisconsin-Madison.Camille is a well-respected voice within the tech community\\, speaking on a variety of topics such as engineering leadership\\, distributed systems\\, scaling teams\\, and technical architecture. In 2017 she released her book\\, \\\"<a href=\\\"https://www.amazon.com/dp/B06XP3GJ7F/\\\">The Managers Path: A Guide for Tech Leaders Navigating Growth and Change</a>.\\\"\\n</p>\\n\\n<br clear=\\\"all\\\" />\\n\\n<H3>Thursday\\, October 1st</h3>\\n<img src=\\\"/acah2020/images/keynote_begoli.jpg\\\" width=\\\"150\\\" style=\\\"float: left\\; padding-right: 30px\\; padding-left: 30px\\;\\\" />","categories":"Keynotes","url":"https://apachecon.com/acah2020/tracks/keynotes.html#W1515"},{"uid":"acah2020-keynotes-R1515@apachecon.com","sequence":"2","dtstamp":"20200828T191038Z","dtstart":"20201001T151500Z","dtend":"20201001T155500Z","summary":"Edmon Begoli","location":"@home","x-alt-desc":"<strong>High Performance Computing with Apache Spark and Parquet on Mission Critical Tasks</strong>\\n\\n<p>Oak Ridge National Laboratory (ORNL) is known for its deployment of some of the world's fastest supercomputers. This legacy brings us opportunities to work on some of the most challenging societal problems. Often\\, these problems require approaches that are more comprehensive than what specific high-performance computing solutions can solve. In this talk\\, we will talk about the essential role that Apache Spark and Parquet played in solving some of these problems.  We will illustrate Apache Spark and Parquet's uses with a case study related to suicide and overdose risk where prevention. The result is a 300x speedup in processing from 75+ hours for the original algorithm to 15 minutes with a new one. We will discuss specific techniques behind this accomplishment and the lessons learned.</p>\\n\\n<p>Edmon Begoli\\, PhD works at Oak Ridge National Laboratory (ORNL)\\, where he leads research and development programs aimed at scaling and improving the resilience of critical decision making.</p>\\n\\n<p>Edmon is a committer with Apache Software Foundation\\, and is a joint faculty professor of Computer Science at the University of Tennessee\\, EECS department.</p>\\n\\n<h2>Sponsored Keynotes</h2>\\n\\n<h3>Tuesday</h3>\\n\\n\\n<img src=\\\"/acah2020/images/keynote_ibm_lightstone.jpg\\\" width=\\\"150\\\" style=\\\"float: left\\; padding-right: 30px\\; padding-left: 30px\\;\\\" />","categories":"Keynotes","url":"https://apachecon.com/acah2020/tracks/keynotes.html#R1515"},{"uid":"acah2020-keynotes-T1545@apachecon.com","sequence":"2","dtstamp":"20200903T153024Z","dtstart":"20200929T154500Z","dtend":"20200929T162500Z","summary":"Double inflection point: Open Source meets AI","location":"@home","x-alt-desc":"<strong>Sam Lightstone:\\nChief Technology Officer for AI Strategy\\, IBM</strong><br />\\n\\n<p>\\nAbstract: Machine Learning is almost as old as the electronic computer\\, but the domain has experienced a massive infusion of energy and investment over the past 8 years.  During this time the open source community has simultaneously developed a wide landscape of rich\\, sophisticated OSS packages for machine learning and deep learning such as Apache Marvin-AI\\, DLlab\\, Spark\\, MLlib\\, MADlib and OpenNLP. In this talk IBM CTO for AI Strategy\\, Sam Lightstone\\, will explore the confluence of these two disruptions and the possibilities that lie ahead for dramatic advances in AI\\, computation power\\, distributed computing\\, and a sea-change in computer science.  \\n\\n<p><em>\\nSam Lightstone is IBM Chief Technology Officer for AI Strategy\\, IBM\\nFellow and a Master Inventor in the IBM Data and AI group. He is also\\nchair of the Data and AI Technical Team\\, the working group of IBMs\\ntechnical executives in the division. He has been the founder and\\nco-founder of several large-scale initiatives including AI databases\\,\\nnext generation data warehousing\\, data virtualization\\, autonomic\\ncomputing for data systems\\, serverless cloud SQL query\\, and cloud native\\ndatabase services. He co-founded the IEEE Data Engineering Workgroup on\\nSelf-Managing Database Systems. Sam has more than 65 patents issued and\\npending and has authored 4 books and over 30 papers. Sams books have\\nbeen translated into Chinese\\, Japanese and Spanish.  In his spare time\\nhe is an avid guitar player and fencer. His Twitter handle is\\n<a href=\\\"https://twitter.com/samlightstone\\\">@samlightstone</a>.</em>\\n</p>\\n\\n\\n<img src=\\\"/acah2020/images/keynote_datastax_ellis.jpg\\\" width=\\\"150\\\" style=\\\"float: left\\; padding-right: 30px\\; padding-left: 30px\\;\\\" />","categories":"Keynotes","url":"https://apachecon.com/acah2020/tracks/keynotes.html#T1545"},{"uid":"acah2020-keynotes-T1600@apachecon.com","sequence":"3","dtstamp":"20200903T153024Z","dtstart":"20200929T160000Z","dtend":"20200929T164000Z","summary":"DataStax Astra and Apache Cassandra: Sustainable Open Source in the Cloud Era","location":"@home","x-alt-desc":"<strong>Jonathan Ellis\\, Co-founder and CTO\\, DataStax</strong><br />\\n\\n<p>\\nApache Cassandra solves database performance at scale better than any other system in the world\\, but it was designed for a world of self-managed infrastructure.  This created a lot of rough edges for the level of automation DataStax needed to build its Astra managed service for Cassandra.  Building Astra also exposed some gaps in Cassandras feature set that modern developers want and expect from a database-as-a-service.</p>\\n\\n<p>DataStax believes that developers and businesses shouldnt have to give up ownership of their data to take advantage of the benefits of cloud infrastructure.  We want everyone to have the freedom to deploy anywhere\\, without lock-in.  This talk will explain how were bringing the enhancements we made for Astra back to Apache Cassandra.  Following the Cassandra Enhancement Proposal process\\, we are showing that cloud and open source are not mutually exclusive.\\n</p>\\n\\n<p><em>\\nJonathan Ellis is a co-founder of DataStax. Before DataStax\\, Jonathan was Project Chair of Apache Cassandra for six years\\, where he built the Cassandra project and community into an open-source success. Previously\\, Jonathan built an object storage system based on Reed-Solomon encoding for data backup provider Mozy that scaled to petabytes of data and gigabits per second throughput.\\n</em></p>\\n\\n\\n<img src=\\\"/acah2020/images/keynote_redhat_huang.png\\\" width=\\\"150\\\" style=\\\"float: left\\; padding-right: 30px\\; padding-left: 30px\\;\\\" />","categories":"Keynotes","url":"https://apachecon.com/acah2020/tracks/keynotes.html#T1600"},{"uid":"acah2020-keynotes-W1545@apachecon.com","sequence":"4","dtstamp":"20200909T160425Z","dtstart":"20200930T154500Z","dtend":"20200930T162500Z","summary":"Rethinking Language: Why Now\\, Whats Next","location":"@home","x-alt-desc":"<strong>Kim Huang\\, Content Strategist\\, Red Hat</strong>\\n\\n<p>\\nAt the core of open source is the idea that we continually change and adapt as we learn new information or discover better ways of doing things. We welcome ideas from anyone to help us make the best software available. Adapting the language we use in our code to become more welcoming for all current and future community members is part of that ethos. Learn about why Red Hat is taking steps to rethink the language of our code and documentation\\, and the impact this work will have.\\n</p>\\n\\n<p><em>\\n</em></p>\\n\\n<img src=\\\"/acah2020/images/keynote_vmware_mcgarvey.jpg\\\" width=\\\"150\\\" style=\\\"float: left\\; padding-right: 30px\\; padding-left: 30px\\;\\\" />","categories":"Keynotes","url":"https://apachecon.com/acah2020/tracks/keynotes.html#W1545"},{"uid":"acah2020-keynotes-W1600@apachecon.com","sequence":"2","dtstamp":"20200909T170516Z","dtstart":"20200930T160000Z","dtend":"20200930T164000Z","summary":"Fostering strong\\, open source communities that benefit all of us","location":"@home","x-alt-desc":"<strong>\\nCatherine McGarvey\\, VP Engineering\\, VMWare\\n</strong>\\n<p>\\nWe all desire strong open source communities\\, but what does that even mean? What are health metrics that you can track and measure to see that you are making an impact here. Let's explore the different open source communities approaches as case studies. What actions can you take to help make your community more inclusive? \\n</p>\\n\\n<p><em>\\nCatherine McGarvey is the VP of Engineering at VMware\\, leading engineering for developer facing communities. She has had the privilege of being involved in a number of OS communities including Apache Geode\\, RabbitMQ\\, Kubernetes\\, cloud foundry and knative.  \\n\\n \\n</em></p>\\n\\n<img src=\\\"/acah2020/images/keynote_instaclustr_inamdar.jpg\\\" width=\\\"150\\\" style=\\\"float: left\\; padding-right: 30px\\; padding-left: 30px\\;\\\" />","categories":"Keynotes","url":"https://apachecon.com/acah2020/tracks/keynotes.html#W1600"},{"uid":"acah2020-keynotes-R1545@apachecon.com","sequence":"3","dtstamp":"20200909T174449Z","dtstart":"20201001T154500Z","dtend":"20201001T162500Z","summary":"A Rising Tide Lifts All Boats: Working With Contributors of All Sizes","location":"@home","x-alt-desc":"<strong>\\nAnil Inamdar\\, Head of US Consulting and Delivery\\, Instaclustr\\n</strong>\\n<p>\\nThe Open source development model has changed significantly since its heydays in the 1990s. Today there are more projects\\, more contributors\\, additional financing and vendors of various kinds  support\\, add-ons\\, cloud providers. The Apache community still continues to play a pivotal role in promoting open source projects\\, setting community standards\\, providing framework for arbitrations and ensuring quality for the projects. \\n</p>\\n<p>\\nParticipations from contributors of all sizes  individual\\, company affiliated\\, and vendor supported act as the rising tide and help expand the open source market. The key however is to ensure that we contribute back to the project and the foundation. Working together creates a rising tide lifting its participants. As the saying goes  If you want to go fast\\, go alone\\; but if you want to go far\\, go together.\\n</p>\\n\\n<p><em>\\n<!-- BIO -->\\n</em></p>\\n\\n<img src=\\\"/acah2020/images/keynote_imply_merlino.jpg\\\" width=\\\"150\\\" style=\\\"float: left\\; padding-right: 30px\\; padding-left: 30px\\;\\\" />","categories":"Keynotes","url":"https://apachecon.com/acah2020/tracks/keynotes.html#R1545"},{"uid":"acah2020-keynotes-R1600@apachecon.com","sequence":"1","dtstamp":"20200914T162505Z","dtstart":"20201001T160000Z","dtend":"20201001T164000Z","summary":"The heat is on: architecting for hot analytics","location":"@home","x-alt-desc":"<strong>\\nGian Merlino\\,\\nCTO and Co-Founder\\, Imply and Apache Druid PMC Chair\\n</strong>\\n<p>\\nToday\\, the industry offers numerous systems for the analysis of large amounts of data. Under the hood\\, they span a variety of interesting and unique architectures. In this talk\\, we'll discuss why you can never seem to find that single perfect system\\, and how to think about and evaluate the capabilities of various systems through the prism of a temperature-based spectrum of use cases\\, from cold to hot analytics.\\n</p>\\n\\n<p><em>\\n<!-- BIO -->\\n</em></p>","categories":"Keynotes","url":"https://apachecon.com/acah2020/tracks/keynotes.html#R1600"},{"uid":"acah2020-mahout-R1615@apachecon.com","sequence":"0","dtstamp":"20200921T182718Z","dtstart":"20201001T161500Z","dtend":"20201001T165500Z","summary":"A Data Scientist First-Time Mahout Experience: Tips and Takeaways (Talk in Spanish)","location":"@home","x-alt-desc":"<strong>\\nJose Francisco Hernandez Santa Cruz\\n</strong>\\n<p>\\nEl constante incremento en la disponibilidad de la data y su crecimiento exponencial crean una oportunidad perfecta para descubrir los detalles ms reveladores y predicciones ms precisas que los datos pueden entregar. Desafortunadamente\\, esto viene\\, a veces\\, a expensas de una alta compejidad computacional: la cada vez ms grande ingesta de datos requiere un mayor poder de cmputo\\, creando limitaciones en un proyecto. Una solucin planteada es el empleo de computacin distribuida: sistema distribuido de computadores ejecutando tareas en paralelo. Un framework que rpidamente se volvi popular en este mbito es Apache-Spark. Sin embargo\\, a medida que el aprendizaje automtico se volvi\\, no solo ms popular\\, pero ms demandante de poder de cmputo\\, Apache Mahout nos trajo un framework enfocado a estadistica y aprendizaje automtico. Como cientfico de datos\\, y primera vez como usuario de Apache Mahout\\, mis experiencias proveen de detalles y contenido desde un punto de vista de nuevo usuario\\, que por primera vez experimenta con Apache Mahout\\, proveyendo lecciones aprendidas especialmente para usuarios de Python con poca o ninguna experiencia en Scala o aprendizaje distribuido.<br />(English Translation\\, Talk will be in Spanish) The constant increase of data availability and exponential growth makes an excellent opportunity to uncover the most revealing insights and most accurate predictions data can give us. Unfortunately this comes\\, sometimes\\, at the expense of highly complex computation. One framework which quickly became popular is Apache-Spark: distributed computing for big data processing. However\\, as machine learning became\\, not only more popular\\, but more demanding of distributive computation\\, Apache Mahout brought us a nice framework with statisticians and machine learning practitioners in mind. As a data scientist for IBM and first time user of Apache Mahout\\, my experiences provide an insight from a first-time user point-of-view\\, providing takeaways and lessons learned\\, specially for Python and R users with no or little experience in Scala or distributed learning.\\n</p>\\n\\n<p><em>\\nGraduated as Industrial Engineer in the city of Lima\\, Peru\\, I started pursuing the data scientist career at the age of 24\\, focused in Machine Learning algorithms and Artificial Neural Networks research. Certified by IBM and Open Group as level 1 data scientist\\, I'm currently finishing MIT's Micromaster in Statistics and Data Science and preparing my application for a master's program in Machine Learning. With two research papers under review for publication\\, and leading for one year a Machine Learning mentoring program at IBM\\, I'm starting my giveback period\\, trying to contribute to open source technology as well as the scientific community with articles published as independent researcher.\\n</em></p>","categories":"Mahout","url":"https://apachecon.com/acah2020/tracks/mahout.html#R1615"},{"uid":"acah2020-mahout-R1655@apachecon.com","sequence":"0","dtstamp":"20200921T182718Z","dtstart":"20201001T165500Z","dtend":"20201001T173500Z","summary":"Modern Recommenders with Mahout","location":"@home","x-alt-desc":"<strong>\\nPatrick (Pat) Ferrel\\n\\n</strong>\\n<p>\\nMahout in years past was known for being the place to go for premium OSS recommenders. Time passed and recommender technology moved on. With Mahout 0.13+ Mahout is once contains a state-of-the-art modern recommender targeting broad use. This talk covers the the 3rd generation Correlated Cross Occurrence Algorithm as it is implemented in Spark-based Mahout. CCO will be explained via the mathematics and theory behind it as well as optimizations made in Mahout to produce a production worthy implementation. We call CCO a 3rd generation algorithm since it comes after Cooccurrence and Matrix Factorization and is fully multimodal\\, making it possible to use many indicators of user behavior as well as contextual and content or metadata based indicators. While Mahout implements the core of the algorithm we will discuss how Mahout can be integrated into a full end-to-end data ingestion and serving architecture. We will also review some comparative performance data.\\n</p>\\n\\n<p><em>\\nPat has worked in startups building apps based on Machine Learning since 2000. He has worked in NLP/NER\\, text mining\\, and recommenders. He became a committer to Apache Mahout in 2012\\, and Apache PredictionIO in 2017. He is currently the Chief Consultant at the OSS and ML consultancy ActionML where he has led nesarly 100 deployments of their Harness ML Server which makes use of Apache Mahout and Apache Spark.\\n</em></p>","categories":"Mahout","url":"https://apachecon.com/acah2020/tracks/mahout.html#R1655"},{"uid":"acah2020-mahout-R1735@apachecon.com","sequence":"0","dtstamp":"20200921T182718Z","dtstart":"20201001T173500Z","dtend":"20201001T181500Z","summary":"Mahout and Kubeflow Together At Last","location":"@home","x-alt-desc":"<strong>\\nTrevor Grant\\n</strong>\\n<p>\\nKubeflow is an exciting and fashionable new platform for Data Science. In this talk we will discuss how to use Apache Mahout (and Apache Spark) on it.\\n</p>\\n\\n<p><em>\\nSomeday he will be the Chief Mugwug. Not today\\, but someday.\\n</em></p>","categories":"Mahout","url":"https://apachecon.com/acah2020/tracks/mahout.html#R1735"},{"uid":"acah2020-mahout-R1815@apachecon.com","sequence":"0","dtstamp":"20200921T182718Z","dtstart":"20201001T181500Z","dtend":"20201001T185500Z","summary":"Apache Mahout on Zeppelin","location":"@home","x-alt-desc":"<strong>\\nAndrew Musselman\\n</strong>\\n<p>\\nThis talk will demonstrate adding a Mahout interpreter to the Zeppelin notebook system. Zeppelin is an extensible notebook project which allows users to add interpreters which will understand and run a wide variety of code\\, ranging from Python\\, to Spark-flavored Scala\\, to SQL dialects\\, to other domain-specific languages (DSLs). In our case we will add an interpreter which understands the Mahout DSL called Samsara\\, which focuses on matrix math at scale. The activities in this tutorial will span: (1) Getting the latest software releases (2) Setting environment variables (3) Creating and configuring the Samsara interpreter (4) Starting a notebook and importing a data set (5) Doing some data manipulation and calculation (6) Producing some plots and charts (7) Showing some ways to publish dashboards and individual cells The audience should be prepared with an operating system which has a recent version of Java (>= jdk 1.8)\\, and an installation script will be provided for people who would like to set a computer up in advance to follow along. This talk is for anyone with an interest in data science and analytics. Blog post with similar previous work/style: https://mahout.apache.org/docs/latest/tutorials/misc/mahout-in-zeppelin\\n</p>\\n\\n<p><em>\\nAndrew Musselman runs business and data operations in North America for 24i\\, chairs the Apache Mahout Project\\, and hosts the Adversarial Learning podcast. He loves distributed matrix math and lives in Seattle with his wife and kids.\\n</em></p>","categories":"Mahout","url":"https://apachecon.com/acah2020/tracks/mahout.html#R1815"},{"uid":"acah2020-mahout-R1855@apachecon.com","sequence":"1","dtstamp":"20200921T182718Z","dtstart":"20201001T185500Z","dtend":"20201001T193500Z","summary":"The Long and Winding Road to Becoming A Mahout Committer","location":"@home","x-alt-desc":"<strong>\\nTrevor Grant\\, Andrew Musselman\\, Pat Ferrel\\n</strong>\\n<p>\\nJk! We want you to be a committer. In this panel discussion various PMC members from (past and?) present will discuss how someone who knows very little or maybe nothing about Apache can go about getting involved with our community\\, what parts of the project we need help on\\, how we operate and more. If we can get some PMC members from Mahout of Yesteryear we will listen their stories of the Mahout of the Past. If we're really hurting for time\\, AKM will freeform about the joys of being a HAM Radio operator.\\n</p>\\n\\n<p><em>\\nTrevor Grant:<br />\\nTrevor is a former data scientist who has given it all up to pursue the app game\\, however will have probably given that up to pursue some other game by the time the conference rolls around.<br />\\nAndrew Musselman:<br />\\nAndrew Musselman runs business and data operations in North America for 24i\\, chairs the Apache Mahout Project\\, and hosts the Adversarial Learning podcast. He loves distributed matrix math and lives in Seattle with his wife and kids.\\n</em></p>","categories":"Mahout","url":"https://apachecon.com/acah2020/tracks/mahout.html#R1855"},{"uid":"acah2020-mahout-R1935@apachecon.com","sequence":"0","dtstamp":"20200921T182718Z","dtstart":"20201001T193500Z","dtend":"20201001T201500Z","summary":"Mahout: State of the Matrix","location":"@home","x-alt-desc":"<strong>\\nTrevor Grant\\n</strong>\\n<p>\\nIn this talk we will go over recent developments\\, discuss upcoming changes\\, and share the PMC's vision for Mahout over the next 12 months and beyond.\\n</p>\\n\\n<p><em>\\nPMC of Mahout\\n</em></p>","categories":"Mahout","url":"https://apachecon.com/acah2020/tracks/mahout.html#R1935"},{"uid":"acah2020-mandarin-T0930@apachecon.com","sequence":"0","dtstamp":"20200903T153024Z","dtstart":"20200929T093000Z","dtend":"20200929T101000Z","summary":"New Apache Members from China\\, responsibilities and obligations","location":"@home","x-alt-desc":"<strong>\\nSheng Wu\\, Juan Pan\\, Ning Jiang\\, Duo Zhang\\n</strong>\\n<p>\\nThere are 11 of 35 new ASF members from China. With more and more project initialized from China and graduated from Incubator as new TLPs\\, China has more people involved in the Apache. In this panel\\, we invited Chinese Apache Members to talk about their open source journey and their responsibilities and obligations for the Apache Software Foundation and open source world.\\n\\n</p>\\n\\n<p><em>\\nSheng Wu:<br />\\nHe is an Apache Member\\, the Apache SkyWalking VP\\, and a PMC member. Also be a member of Apache ShardingSphere\\, APISIX\\, and Incubator PMC. He mentors several China initialized incubator project. Talked a lot about the open source in many conferences.<br />\\nJuan Pan:<br />\\nAs a senior DBA worked at JD.com\\, the responsibility is to develop the distributed database and middleware\\, and the automated management platform for database clusters. As a PMC of Apache ShardingSphere\\, I am willing to contribute to the OS community and explore the area of distributed databases and NewSQL.<br />\\nNing Jiang:<br />\\nWillem Jiang is the technical expert of Huawei\\, a member of the Apache Software Foundation\\, he worked on many Apache projects like Camel\\, CXF\\, ServiceMix and ServiceComb. Before joining Huawei\\, Willem was the principal engineer of RedHat working on Fuse ESB\\, he also worked for FuseSource\\, IONA and Travelsky.inc. Willem gave talks on micro-services\\, distributed systems and open source in several conferences\\, like QCon Beijing\\, ArchSummit etc.<br />\\nDuo Zhang:<br />\\nDuo Zhang is a principal software engineer at Xiaomi\\, works for the cloud platform department. He is a member of the Apache Software Foundation\\, and also the chair of the Apache HBase PMC. Besides HBase\\, he also works on several other Apache projects like Hadoop\\, Yetus\\, etc. He is a mentor of several Apache incubator projects such as NuttX and Pegasus.\\n</em></p>","categories":"Mandarin","url":"https://apachecon.com/acah2020/tracks/mandarin.html#T0930"},{"uid":"acah2020-mandarin-T1010@apachecon.com","sequence":"0","dtstamp":"20200903T153024Z","dtstart":"20200929T101000Z","dtend":"20200929T105000Z","summary":"From Web Engineer to Apache APISIX PMC","location":"@home","x-alt-desc":"<strong>\\nZhiyuan Ju\\n</strong>\\n<p>\\nThe open source project Apache HTTP Server\\, carry the data connectivity between many terminals. Without the help of open source projects\\, today's Internet will be much inferior. Therefore\\, we encourage developers to actively participate in open source projects in order to better maintain the community ecology. In this meeting\\, I will share my experience from a Web engineer to continuous participation in open source projects\\, as well as the cultural differences between the Apache community and others\\, so that more developers can understand\\, embrace and participate in open source projects.\\n</p>\\n\\n<p><em>\\nPMC member of Apache APISIX The core member of freeCodeCamp China\\, an organization involving to help people to learn web technologies Web and Security are also my favorites.\\n</em></p>","categories":"Mandarin","url":"https://apachecon.com/acah2020/tracks/mandarin.html#T1010"},{"uid":"acah2020-mandarin-T1050@apachecon.com","sequence":"0","dtstamp":"20200903T153024Z","dtstart":"20200929T105000Z","dtend":"20200929T113000Z","summary":"New Features of Apache CarbonData 2.0","location":"@home","x-alt-desc":"<strong>\\nCai Qiang\\n</strong>\\n<p>\\nApache CarbonData is an indexed columnar data format for fast analytics on big data platform. The latest version 2.0 is a milestone version. Compared with the 1.x version\\, the data loading and index capabilities are greatly improved. The CDC capability is improved to support the update\\, delete\\, and merge functions. The reconstructed MV supports multiple formats.\\n\\n</p>\\n\\n<p><em>\\nCai Qiang\\, Apache CarbonData PMC\\, Committer\\, more 10 years code experience in big data domain\\, has deep understanding for Hadoop\\, Spark\\, Hive etc. As CarbonDatas initial member\\, who was responsible for core architecture design of data loading and index features.\\n\\n</em></p>","categories":"Mandarin","url":"https://apachecon.com/acah2020/tracks/mandarin.html#T1050"},{"uid":"acah2020-mandarin-T1130@apachecon.com","sequence":"0","dtstamp":"20200903T153024Z","dtstart":"20200929T113000Z","dtend":"20200929T121000Z","summary":"ECharts: could the customization be both easy and highly personalized?","location":"@home","x-alt-desc":"<strong>\\nShuang Su\\n</strong>\\n<p>\\nThe major task of a charting library is to find out some appropriate ways to abstract the data visualization programing. Usually\\, common cases\\, easy-to-use\\, \\\"flexibility\\\" and \\\"maintainability\\\" should be considered to come up with some concepts and API for users to learn and express their requirements. In this designing\\, is it possible to both satisfy the easy-to-use and highly personalized? This topic will share the understanding of these abstraction in the evolution of echarts program\\, and illustrate the cases that benefited from the concepts like \\\"custom series\\\"\\, \\\"series/coordinate system combination\\\".\\n\\n</p>\\n\\n<p><em>\\nApache ECharts (incubating) PPMC member\\n</em></p>","categories":"Mandarin","url":"https://apachecon.com/acah2020/tracks/mandarin.html#T1130"},{"uid":"acah2020-mandarin-T1210@apachecon.com","sequence":"1","dtstamp":"20200903T153024Z","dtstart":"20200929T121000Z","dtend":"20200929T125000Z","summary":"New Feature of Apache ShardingSphere 5.x","location":"@home","x-alt-desc":"<strong>\\nLiang Zhang\\n</strong>\\n<p>\\nThe first version of Apache ShardingSphere 5.x will be released soon. In version 5.x\\, Apache ShardingSphere has made significant innovations from architecture design to product scope. Apache ShardingSphere 5.x follow pluggable architecture design concept to build a flexible\\, embeddable and extensible project. Apache ShardingSphere 5. X no longer takes data sharding as kernel\\, but turns to building distributed database ecosystem. In the new version\\, core functions such as data sharding\\, distributed transaction and database governance are completely separated from the kernel and become a part of its pluggable component. Through SPI\\, the ecosystem is fully opened\\, and the functions of data migration\\, elastic scheduling\\, data encryption\\, shadow table are fully integrated into the product ecology. This presentation will comprehensively introduce the new features of Apache ShardingSphere 5.x.\\n</p>\\n\\n<p><em>\\nLiang Zhang\\, Architecture expert of Technical Center\\, JD Digital Technology(JD.com)\\, Apache ShardingSphere PMC Chair. Passionate to open source\\, and advocate clean code. He recently focuses on building distributed database middleware Apache ShardingSphere as the first-rate data solution in the finance industry. Liang Zhang has published a book named \\\"Future Architecture: from SOA to Cloud Native\\\" on March\\, 2019. GitHub: https://github.com/terrymanu\\, communications are always welcomed.\\n</em></p>","categories":"Mandarin","url":"https://apachecon.com/acah2020/tracks/mandarin.html#T1210"},{"uid":"acah2020-mandarin-W0900@apachecon.com","sequence":"0","dtstamp":"20200903T153024Z","dtstart":"20200930T090000Z","dtend":"20200930T094000Z","summary":"How does Apache Dolphin Scheduler (Incubator) support 100\\,000-level data task scheduling?","location":"@home","x-alt-desc":"<strong>\\nLidong Dai\\n</strong>\\n<p>\\nFirst I will introduce the development of the DolphinScheduler community\\, and then introduce why we had to reinvent the wheel to rebuild the scheduling of big data tasks\\, the overall design ideas of DolphinScheduler\\, considerations\\, and the features and capabilities of DolphinScheduler. Next\\, I will introduce evolution process of DolphinScheduler architecture. In this share\\, I will also talk about the challenges and accumulated experience we have encountered in the scheduling of big data tasks. then\\, I will share some user cases and usage scenarios. Finally\\, I will share the history of open source.\\n</p>\\n\\n<p><em>\\nHe is currently the director of Analysys Big Data Platform & Apache DolphinScheduler PPMC\\, responsible for the data process architecture\\, technology selection\\, and technical breakthroughs of the daily 30 billion-level data processing chain. Focusing on the research and development of data platform architecture for 10 years\\, he good at data platform construction\\, cluster performance tuning\\, and data warehouse construction. He has served as a data architect for many big data companies and has some experience in retail business\\, olap data analysis\\, and mining.\\n</em></p>","categories":"Mandarin","url":"https://apachecon.com/acah2020/tracks/mandarin.html#W0900"},{"uid":"acah2020-mandarin-W0940@apachecon.com","sequence":"0","dtstamp":"20200903T153024Z","dtstart":"20200930T094000Z","dtend":"20200930T102000Z","summary":"OSS.Chat - A bridge to the Apache Way in China","location":"@home","x-alt-desc":"<strong>\\nHuan\\n</strong>\\n<p>\\nThe mission of the OSS.Chat project is to bridge the three-way communication and translation barriers between WeChat and other social platforms (future) and GitHub Issues and mailing lists to the open source development community\\, making ASF's cultural\\, technical\\, and collaborative processes acceptable quickly and easily\\, rather than stumbling from the start. With Chatbot\\, an automated process mechanism\\, developers can more easily share and communicate information about the development of open source projects. In particular\\, the archiving and secondary induction of open information to the community is one of the things that we think is very meaningful. Through OSS.Chat project\\, we hope to further promote\\, disseminate\\, and even optimize the culture\\, technology\\, and collaboration of the Apache project community.\\n</p>\\n\\n<p><em>\\nHuan\\, PreAngel Partner\\, Author of Wechaty\\, an Angel Investor\\, Serial Entrepreneur\\, Machine Learning PhD Student\\, Microsoft AI MVP\\, Google ML GDE\\, Tencent Chatbot TVP\\, Conversational AI Coder with passion\\n</em></p>","categories":"Mandarin","url":"https://apachecon.com/acah2020/tracks/mandarin.html#W0940"},{"uid":"acah2020-mandarin-W1020@apachecon.com","sequence":"0","dtstamp":"20200903T153024Z","dtstart":"20200930T102000Z","dtend":"20200930T110000Z","summary":"Apache TubeMQ: a new choice of MQ in big data scenarios","location":"@home","x-alt-desc":"<strong>\\nGosonzhang\\n</strong>\\n<p>\\n\\nThis paper introduces the challenges faced by Message Queue (MQ) when data transmission changes from 10 billion to trillions in the big data scenario\\, and how TubeMQ solves such problems to meet business needs.\\n</p>\\n\\n<p><em>\\nTubeMQ project PPMC member\\, working in the data storage group of Tencent Data Platform Department.\\n\\n</em></p>","categories":"Mandarin","url":"https://apachecon.com/acah2020/tracks/mandarin.html#W1020"},{"uid":"acah2020-mandarin-W1100@apachecon.com","sequence":"0","dtstamp":"20200903T153024Z","dtstart":"20200930T110000Z","dtend":"20200930T114000Z","summary":"Apache Doris - A fast MPP database for all modern analytics on big data","location":"@home","x-alt-desc":"<strong>\\nMingyu Chen\\n</strong>\\n<p>\\nDoris is an analytical database project that entered the Apache incubator in 2018. The design goal of Doris is to provide users with an interactive analysis system that responds to massive amounts of data in sub-second levels through an elegant and simple system architecture\\, effectively supporting real-time data analysis. Doris's distributed architecture is very simple\\, easy to operate and maintain\\, and can support very large data sets of more than 10PB. Doris can also meet a variety of data analysis needs\\, including history data reports\\, real-time data analysis\\, interactive data analysis\\, and exploratory data analysis. Make data analysis easier. The speech mainly introduced the development history of Doris\\, architecture design\\, key features and classic use cases.\\n</p>\\n\\n<p><em>\\nBaidu senior R&D engineer\\, Apache Doris(incubating) PPMC\\, Bachelor of University of Science and Technology of China\\, Master of Institute of Computing Technology\\, Chinese Academy of Sciences\\, 6 years of big data research and development experience.\\n\\n</em></p>","categories":"Mandarin","url":"https://apachecon.com/acah2020/tracks/mandarin.html#W1100"},{"uid":"acah2020-ml-T1615@apachecon.com","sequence":"1","dtstamp":"20200810T143451Z","dtstart":"20200929T161500Z","dtend":"20200929T165500Z","summary":"TVM: An End to End Deep Learning Compiler Stack","location":"@home","x-alt-desc":"<strong>\\nTianqi Chen\\n</strong>\\n<p>\\nApache(incubating) TVM is an open deep learning compiler stack for CPUs\\, GPUs\\, and specialized accelerators. It aims to close the gap between the productivity-focused deep learning frameworks\\, and the performance- or efficiency-oriented hardware backends. TVM provides the following main features: - Compilation of deep learning models in Keras\\, MXNet\\, PyTorch\\, Tensorflow\\, CoreML\\, DarkNet into minimum deployable modules on diverse hardware backends. - Infrastructure to automatic generate and optimize tensor operators on more backend with better performance. In this talk\\, I will cover the new developments in TVM in the past year around the areas of more backend\\, automation and model support.\\n</p>\\n\\n<p><em>\\nTianqi Chen received his PhD. from the Paul G. Allen School of Computer Science & Engineering at the University of Washington\\, working with Carlos Guestrin on the intersection of machine learning and systems. He has created three major learning systems that are widely adopted: XGBoost\\, TVM\\, and MXNet(co-creator). He is a recipient of the Google Ph.D. Fellowship in Machine Learning. He is currently the CTO of OctoML.\\n</em></p>","categories":"Machine Learning","url":"https://apachecon.com/acah2020/tracks/ml.html#T1615"},{"uid":"acah2020-ml-T1655@apachecon.com","sequence":"1","dtstamp":"20200810T143451Z","dtstart":"20200929T165500Z","dtend":"20200929T173500Z","summary":"Apache Submarine: State of the union","location":"@home","x-alt-desc":"<strong>\\nWangda Tan\\, Zhankun Tang\\n</strong>\\n<p>\\nApache Submarine is the ONE PLATFORM to allow Data Scientists to create end-to-end machine learning workflow. ONE PLATFORM means it supports Data Scientists to finish their jobs on the same platform without frequently switching their toolsets. From dataset exploring data pipeline creation\\, model training (experiments)\\, and push model to production (model serving and monitoring). All these steps can be completed within the ONE PLATFORM. In this talk\\, well start with the current status of Apache Submarine  how it is used today in deployments large and small. We'll then move on to the exciting present & future of Submarine  features that are further strengthening Submarine as the ONE PLATFORM for data scientists to train/manage machine learning models. Well discuss highlight of the newly released 0.4.0 version\\, and new features 0.5.0 release which is planned in 2020 Q3: - New features to run model training (experiments) on K8s\\, submit mode training job by using easy-to-use Python/REST API or UI. - Integration to Jupyter notebook\\, and allows Data-Scientists to provision\\, manage notebook session\\, and submit offline machine learning jobs from notebooks. - Integration with Conda kernel\\, Docker images to make hassle-free experiences to manage reusable notebook/mode-training experiments within a team/company. - Pre-packaged Training Template for Data-Scientists to focus on domain-specific tasks (like using DeepFM to build a CTR prediction model). We will also share mid-term/long-term roadmap for Submarine\\, including Model management for model-serving/versioning/monitoring\\, etc.\\n</p>\\n\\n<p><em>\\nWangda Tan:<br />\\nWangda Tan is Sr. Manager of Compute Platform engineering team @ Cloudera\\, responsible for all engineering efforts related to Kubernetes\\, Apache Hadoop YARN\\, Resource Scheduling\\, and internal container cloud. In open-source world\\, he's a member of Apache Software Foundation (ASF)\\, PMC Chair of Apache Submarine project\\, He is also project management committee (PMC) members of Apache Hadoop\\, Apache YuniKorn (incubating). Before joining Cloudera\\, he leads High-performance-computing on Hadoop related work in EMC/Pivotal. Before that\\, he worked in Alibaba Cloud and participated in the development of a distributed machine learning platform (later became ODPS XLIB).<br />\\nZhankun Tang:<br />\\nZhankun Tang is Staff Software Engineer @Cloudera. Hes interested in big data\\, cloud computing\\, and operating system. Now focus on contributing new features to Hadoop as well as customer engagement. Zhankun is PMC member of Apache Hadoop/Submarine\\, prior to Cloudera/Hortonworks\\, he works for Intel.\\n</em></p>","categories":"Machine Learning","url":"https://apachecon.com/acah2020/tracks/ml.html#T1655"},{"uid":"acah2020-ml-T1725@apachecon.com","sequence":"0","dtstamp":"20200812T151913Z","dtstart":"20200929T173500Z","dtend":"20200929T181500Z","summary":"Apache MXNet 2.0: Bridging the Gap between DL and ML","location":"@home","x-alt-desc":"<strong>\\nSheng Zha\\n</strong>\\n<p>\\nDeep learning community has largely evolved independently from the prior community of data science and machine learning community in NumPy. While most deep learning frameworks now provide NumPy-like math and array library\\, they differ in the definition of the operations which creates a steeper learning curve of deep learning for machine learning practitioners and data scientists. This creates a chasm not only in the skillsets of the two different communities\\, but also hinders the exchange of knowledge. The next major version\\, 2.0\\, of Apache MXNet (incubating) seeks to bridge the fragmented deep learning and machine learning ecosystem. It provides NumPy-compatible programming experiences and simple enhancements to NumPy for deep learning with the new Gluon 2.0 interface. The NumPy-compatible array API also brings the advances in GPU acceleration\\, auto-differentiation\\, and high-performance one-click deployment to the NumPy ecosystem.\\n</p>\\n\\n<p><em>\\nSheng Zha is an Applied Scientist at Amazon AI. Hes also a committer and PPMC member of Apache MXNet (Incubating)\\, steering committee member of Linux AI Foundation ONNX\\, and maintainer of the GluonNLP project. In his research\\, Sheng focuses on the intersection between deep learning-based natural language processing and computing systems\\, with the aim of enabling learning from large-scale language data and making it accessible.\\n</em></p>","categories":"Machine Learning","url":"https://apachecon.com/acah2020/tracks/ml.html#T1725"},{"uid":"acah2020-ml-T1815@apachecon.com","sequence":"1","dtstamp":"20200810T143451Z","dtstart":"20200929T181500Z","dtend":"20200929T185500Z","summary":"Streaming Machine Learning with Apache Kafka and TensorFlow (without a Data Lake)","location":"@home","x-alt-desc":"<strong>\\nKai Waehner\\n</strong>\\n<p>\\nMachine Learning (ML) is separated into model training and model inference. ML frameworks typically load historical data from a data store like HDFS or S3 to train models. This talk shows how you can avoid such a data store by ingesting streaming data directly via Apache Kafka from any source system into TensorFlow for model training and model inference using the capabilities of TensorFlow I/O. The talk compares this modern streaming architecture to traditional batch and big data alternatives and explains benefits like the simplified architecture\\, the ability of reprocessing events for training different models\\, and the possibility to build a scalable\\, mission-critical\\, real time ML architecture with muss less headaches and problems\\n</p>\\n\\n<p><em>\\nKai Waehner is a Technology Evangelist at Confluent. He works with customers across Europe\\, US\\, Middle East and Asia and internal teams like engineering and marketing. Kais main area of expertise lies within the fields of Big Data Analytics\\, Machine Learning\\, Hybrid Cloud Architectures\\, Event Stream Processing and Internet of Things. He is regular speaker at international conferences such as ApacheCon and Kafka Summit\\, writes articles for professional journals\\, and shares his experiences with new technologies on his blog: www.kai-waehner.de.\\n</em></p>","categories":"Machine Learning","url":"https://apachecon.com/acah2020/tracks/ml.html#T1815"},{"uid":"acah2020-ml-W1615@apachecon.com","sequence":"0","dtstamp":"20200812T151913Z","dtstart":"20200930T161500Z","dtend":"20200930T165500Z","summary":"Deep Learning in Java","location":"@home","x-alt-desc":"<strong>\\nQing Lan\\n</strong>\\n<p>\\nAI is evolving rapidly\\, and is used widely in a variety of industries. Machine learning (ML) applications ranging from basic text classification to complex applications such as object detection and pose estimation are being developed to be used in enterprise applications. Currently\\, software engineers using Java have a large barrier to entry when they try to adopt Deep Learning (DL) for their applications. Python being the de-facto programming language for ML adds additional gradient to an already steep learning curve. This tutorial will introduce an open-source\\, framework-agnostic Java library  Deep Java Library (DJL) for high-performance training and inference in production. DJL supports a variety of Deep Learning engines (including but not limited to Apache MXNet\\, TensorFlow and PyTorch) and provides a simple and clean Java API that will work the same with each engine. Additionally\\, DJL offers the DJL Model Zoo - a repository of models that makes it easy to share models across teams. This tutorial will walk software engineers through the core features of DJL and demonstrate how it can be used to simplify experience of serving models. By the end of the session\\, users will be able to train and deploy DL models from a variety of DL frameworks into production environments and serve user requests using Java. Website: https://djl.ai/\\n</p>\\n\\n<p><em>\\nQing is a SDE II in the AWS Deep Learning Toolkits team. He is one of the co-authors of DJL (djl.ai) and PPMC member of Apache MXNet. He graduated from Columbia University in 2017 with a MS degree in Computer Engineering and has worked on model training and inference. Qing has presented a workshop about Apache MXNet in ApacheCon 2019(Las Vegas) about using Java for Deep Learning inference.\\n</em></p>","categories":"Machine Learning","url":"https://apachecon.com/acah2020/tracks/ml.html#W1615"},{"uid":"acah2020-ml-W1655@apachecon.com","sequence":"0","dtstamp":"20200812T151913Z","dtstart":"20200930T165500Z","dtend":"20200930T173500Z","summary":"Running ML algorithms with ML tools available in Apache Ecosystem","location":"@home","x-alt-desc":"<strong>\\nShekhar Prasad Rajak\\n</strong>\\n<p>\\nIn these days\\, having libraries to get abstract methods to use machine learning algorithm in the application is important but to train our model effectively in lesser time & resources\\; for our own customize algorithm is more important. Machine learning technology is changing every single day\\, so let's spend time on how Researchers and Software Developers can leverage the powerful features provided by Apache libraries & frameworks. In this talk we will focus on Apache libraries/frameworks available for distributed training\\, large scale & less costly data transfer during the whole Model training life cycle. Fundamentals and motive behind following Apache Projects: * Apache Spark MLlib: Simplifies large scale machine learning pipelines\\, using distributed memory-based Spark architecture. The best for building & experimenting new algorithms. * Apache MxNet: A lean\\, flexible\\, and ultra-scalable deep learning framework that supports state of the art in deep learning models * Apache Singa: It provides intelligent database system\\, distributed deep learning by partitioning the model and data onto nodes in a cluster and parallelize the training. * Apache Ignite: A distributed database \\, caching and processing platform designed to store and compute on large volumes of data across a cluster of nodes - which can be super useful to perform distributed training and inference instantly without massive data transmissions * Apache Mahout : A distributed linear algebra framework that support multiple distributed backends like Apache Spark\\, to use by data scientists to quickly implement algorithms and statistics analysis of data. Practical guide for above Apache projects\\, focusing following points: * Data processing\\, implementing existing & customised own ML algorithms\\, tuning\\, scaling up and finally deploying to optimising it using Apache cluster management tools and(or) Kubernetes. Performance and benchmark with Kubernetes. * Handling large-scale batch\\, streaming data & realtime processing. * Caching data or in-memory for faster ML predictions\\n</p>\\n\\n<p><em>\\nShekhar is passionate about Open Source Softwares and active in various Open Source Projects. During college days he has contributed SymPy - Python library for symbolic mathematics \\, Data Science related Ruby gems like: daru\\, dart-view(Author)\\, nyaplot - which is under Ruby Science Foundation (SciRuby)\\, Bundler: a gem to bundle gems\\, NumPy & SciPy for creating the interactive website and documentation website using sphinx and Hugo framework\\, CloudCV for migrating the Angular JS application to Angular 8\\, and few others. He has successfully completed Google Summer of Code 2016 & 2017 and mentored students after that on 2018\\, 2019. Shekhar also talked about daru-view gem in RubyConf India 2018 and PyCon India 2017 on SymPy & SymEngine.\\n</em></p>","categories":"Machine Learning","url":"https://apachecon.com/acah2020/tracks/ml.html#W1655"},{"uid":"acah2020-ml-W1735@apachecon.com","sequence":"0","dtstamp":"20200812T151913Z","dtstart":"20200930T173500Z","dtend":"20200930T181500Z","summary":"Apache Deep Learning 301","location":"@home","x-alt-desc":"<strong>\\nTimothy Spann\\n</strong>\\n<p>\\nIn my talk I will discuss and show examples of using Apache Hadoop\\, Apache Kudu\\, Apache Flink\\, Apache Hive\\, Apache MXNet\\, Apache OpenNLP\\, Apache NiFi and Apache Spark for deep learning applications. This is the follow up to previous talks on Apache Deep Learning 101 and 201 at ApacheCon\\, Dataworks Summit\\, Strata and other events. As part of my talk I will walk through using Apache MXNet Pre-Built Models\\, integrating new open source Deep Learning libraries with Python and Java\\, as well as running real-time AI streams from edge devices to servers utilizing Apache NiFi and Apache NiFi - MiNiFi. This talk is geared towards Data Engineers interested in the basics of architecting Deep Learning pipelines with open source Apache tools in a Big Data environment. I will walk through source code examples available in github and run the code live on Apache NiFi and Apache Flink clusters.\\n</p>\\n\\n<p><em>\\nTim Spann is a Principal Field Engineer at Cloudera in the Data in Motion Team where he works with Apache NiFi\\, MiniFi\\, Kafka\\, Kafka Streams\\, Edge Flow Manager\\, MXNet\\, TensorFlow\\, Apache Spark\\, Big Data\\, IoT\\, Cloud\\, Machine Learning\\, and Deep Learning. Tim has over a decade of experience with the IoT\\, big data\\, distributed computing\\, streaming technologies\\, and Java programming. Previously\\, he was a senior solutions architect at AirisData and a senior field engineer at Pivotal. He blogs for DZone\\, where he is the Big Data Zone leader\\, and runs a popular meetup in Princeton on big data\\, IoT\\, deep learning\\, streaming\\, NiFi\\, blockchain\\, and Spark. Tim is a frequent speaker at conferences such as IoT Fusion\\, Strata\\, ApacheCon\\, Data Works Summit Berlin\\, DataWorks Summit Sydney\\, DataWorks Summit DC\\, DataWorks Summit Barcelona and Oracle Code NYC. He holds a BS and MS in computer science.\\n</em></p>","categories":"Machine Learning","url":"https://apachecon.com/acah2020/tracks/ml.html#W1735"},{"uid":"acah2020-ml-W1815@apachecon.com","sequence":"1","dtstamp":"20200812T151913Z","dtstart":"20200930T181500Z","dtend":"20200930T185500Z","summary":"Edge to AI: Analytics from Edge to Cloud with Efficient Movement of Machine Data","location":"@home","x-alt-desc":"<strong>\\nTimothy Spann\\, Paul Vidal\\n</strong>\\n<p>\\nIn this talk\\, we will walk you through the simple steps to build and deploy machine learning for sentiment analysis and YOLO object detection as part of an IoT application that starts from devices collecting sensor data and camera images with MiNiFi. This data is streamed to Apache NiFi which integrates with Cloudera Data Science Workbench for classification with models in real-time as part of the real-time event stream. We parse\\, filter\\, fork\\, sort\\, query with SQL\\, dissect\\, enrich\\, transform\\,utilizing TensorFlow and MXNet processors in NiFi\\, join and aggregate data as it is ingested. The data is landed in Big Data stores in the cloud for batch and interactive analytics with Apache Flink\\, Apache Spark\\, Apache Hive\\, Apache Kudu and Apache Impala. Utilizing Intel Movidius\\, NVidia Jetson Xavier\\, NVidia Jetson Nano and Google Coral Edge processors as part of a real-time streaming deep learning flow that includes Deep Learning Classification at the edge\\, at the gateway\\, in the cloud and at every step along the way. Reference: https://blog.cloudera.com/blog/2019/02/integrating-machine-learning-models-into-your-big-data-pipelines-in-real-time-with-no-coding/ https://community.cloudera.com/t5/Community-Articles/Edge-to-AI-IoT-Sensors-and-Images-Streaming-Ingest-and/ta-p/249474 https://community.cloudera.com/t5/Community-Articles/Using-Cloudera-Data-Science-Workbench-with-Apache-NiFi-and/ta-p/249469 https://github.com/tspannhw/nifi-cdsw\\n</p>\\n\\n<p><em>\\nTim Spann is a Principal DataFlow Field Engineer at Cloudera\\, the Big Data Zone leader and blogger at DZone and an experienced data engineer with 15 years of experience. He runs the Future of Data Princeton meetup as well as other events. He has spoken at Philly Open Source\\, ApacheCon in Montreal\\, Strata NYC\\, Oracle Code NYC\\, IoT Fusion in Philly\\, meetups in Princeton\\, NYC\\, Philly\\, Berlin and Prague\\, DataWorks Summits in San Jose\\, Washington DC\\, Barcelona\\, Berlin and Sydney. https://www.youtube.com/watch?v=bOfSnNVum_M&t=397s\\n</em></p>","categories":"Machine Learning","url":"https://apachecon.com/acah2020/tracks/ml.html#W1815"},{"uid":"acah2020-observability-T0930@apachecon.com","sequence":"0","dtstamp":"20200828T190237Z","dtstart":"20200929T093000Z","dtend":"20200929T101000Z","summary":"Improve Apache APISIX observability with Apache Skywalking","location":"@home","x-alt-desc":"<strong>\\nYuansheng Wang\\n</strong>\\n<p>\\nApache APISIX is a cloud-native microservices API gateway\\, delivering the ultimate performance\\, security\\, open-source and scalable platform for all your APIs and microservices. Apache SkyWalking: an APM(application performance monitor) system\\, especially designed for microservices\\, cloud-native and container-based (Docker\\, Kubernetes\\, Mesos) architectures. Through the powerful plug-in mechanism of Apache APISIX\\, Apache Skywalking is quickly supported\\, so that we can see the complete life cycle of requests from the edge to the internal service. Monitor and manage each request in a visual way\\, and improve the observability of the service.\\n</p>\\n\\n<p><em>\\nOpen source enthusiasts\\, participated in and contributed to many open source projects\\, and wrote some open source e-books. Apache APISIX ppmc.\\n</em></p>","categories":"Observability","url":"https://apachecon.com/acah2020/tracks/observability.html#T0930"},{"uid":"acah2020-observability-T1010@apachecon.com","sequence":"0","dtstamp":"20200828T190237Z","dtstart":"20200929T101000Z","dtend":"20200929T105000Z","summary":"Another backend storage solution for the APM system","location":"@home","x-alt-desc":"<strong>\\nJuan Pan\\n</strong>\\n<p>\\nThe APM system provides the tracing or metrics for distributed systems or microservice architectures. Back to APM themselves\\, they always need backend storage to store the necessary massive data. What are the features required for backend storage? Simple\\, fewer dependencies\\, widely used query language\\, and the efficiency could be into your consideration. Based on that\\, traditional SQL databases (like MySQL) or NoSQL databases would be better choices. However\\, this topic will present another backend storage solution for the APM system viewing from NewSQL. Taking Apache Skywalking for instance\\, this talking will share how to make use of Apache ShardingSphere\\, a distributed database middleware ecosystem to extend the APM system's storage capability.\\n</p>\\n\\n<p><em>\\nAs a senior DBA worked at JD.com\\, the responsibility is to develop the distributed database and middleware\\, and the automated management platform for database clusters. As a PMC of Apache ShardingSphere\\, I am willing to contribute to the OS community and explore the area of distributed databases and NewSQL.\\n</em></p>","categories":"Observability","url":"https://apachecon.com/acah2020/tracks/observability.html#T1010"},{"uid":"acah2020-observability-T1050@apachecon.com","sequence":"0","dtstamp":"20200828T190237Z","dtstart":"20200929T105000Z","dtend":"20200929T113000Z","summary":"Distributed Tracing in Microservices with Apache Karaf and CXF","location":"@home","x-alt-desc":"<strong>\\nAndrei Shakirin\\n</strong>\\n<p>\\nMicroservice Architectural Pattern suggests splitting of the business domain to several independent bounded contexts exposed as microservices. It brings a lot of benefits for teams working on microservices independently\\, but\\, from other side\\, complicates the problem analysis and monitoring. Sometimes it is very hard to detect which component causes slowdown and failure\\, to analyse what happens with request spans multiple services. The solutions for this challenge are distributed tracing and monitoring. Talk will introduce and explain basic tracing terminology: span\\, trace and context. Presenter will show common approaches to distributed tracing using Apache Karaf\\, CXF and Zipkin\\, SpringBoot and Sleuth frameworks. Talk contains some real project examples and demos.\\n</p>\\n\\n<p><em>\\nThe areas of his interest are REST API design\\, Microservices\\, Cloud\\, resilient distributed systems\\, security and agile development. Andrei is PMC and committer of Apache CXF and committer of Syncope projects. He is member of OASIS S-RAMP Work Group and speaker at Java and Apache conferences. Last speaking experience:  DOAG 2019\\, Nov 2019\\, Nurnberg\\, Design Production-Ready Software  Karlsruhe Entwickertag 2017\\, Mai 2017\\, Karlsruhe\\, Microservices with OSGi  ApacheCon Europe 2016\\, Nov 2016\\, Seville\\, Microservices with Apache Karaf and Apache CXF: Practical Experience  ApacheCon Europe 2015\\, Oct 2015\\, Budapest\\, Create and Secure Your REST API with Apache CXF  ApacheCon Europe 2014\\, Nov 2014\\, Budapest\\, Design REST Services With CXF JAX-RS Implementation: Lessons Learned  WJAX 2011\\, Nov 2011\\, Munich\\, Apache Days\\, Enabling Services with Apache CXF\\n</em></p>","categories":"Observability","url":"https://apachecon.com/acah2020/tracks/observability.html#T1050"},{"uid":"acah2020-observability-T1130@apachecon.com","sequence":"0","dtstamp":"20200828T190237Z","dtstart":"20200929T113000Z","dtend":"20200929T121000Z","summary":"The history of distributed tracing storage","location":"@home","x-alt-desc":"<strong>\\nHongtao Gao\\n</strong>\\n<p>\\nOver the past few years\\, and coupled with the growing adoption of microservices\\, distributed tracing has emerged as one of the most commonly used monitoring and troubleshooting methodologies. New tracing tools are increasingly being introduced\\, driving adoption even further. One of these tools is Apache SkyWalking\\, a popular open-source tracing\\, and APM platform. This talk explores the history of the SkyWalking storage module\\, shows the evolution of distributed tracing storage layers\\, from the traditional relational database to document-based search engine. I hope that this talk contributes to the understanding of history and also that it helps to clarify the different types of storage that are available to organizations today.\\n</p>\\n\\n<p><em>\\nHongtao Gao is the engineer of tetrate.io and the former Huawei Cloud expert. One of PMC members of Apache SkyWalking and participates in some popular open-source projects such as Apache ShardingSphere and Elastic-Job. He has an in-depth understanding of distributed databases\\, container scheduling\\, microservices\\, ServicMesh\\, and other technologies.\\n</em></p>","categories":"Observability","url":"https://apachecon.com/acah2020/tracks/observability.html#T1130"},{"uid":"acah2020-observability-T1210@apachecon.com","sequence":"0","dtstamp":"20200828T190237Z","dtstart":"20200929T121000Z","dtend":"20200929T125000Z","summary":"SourceMarker - Continuous Feedback for Developers","location":"@home","x-alt-desc":"<strong>\\nBrandon Fergerson\\n</strong>\\n<p>\\nToday's monitoring solutions are geared towards operational tasks\\, displaying behavior as time-series graphs inside dashboards and other abstractions. These abstractions are immensely useful but are largely designed for software operators\\, whose responsibilities require them to think in systems\\, rather than the underlying source code. This is problematic given that an ongoing trend of software development is the blurring boundaries between building and operating software. This trend makes it increasingly necessary for programming environments to not just support development-centric activities\\, but operation-centric activities as well. Such is the goal of the feedback-driven development approach. By combining IDE and APM technology\\, software developers can intuitively explore multiple dimensions of their software simultaneously with continuous feedback about their software from inception to production.\\n</p>\\n\\n<p><em>\\nBrandon Fergerson is an open-source software developer who does not regard himself as a specialist in the field of programming\\, but rather as someone who is a devoted admirer. He discovered the beauty of programming at a young age and views programming as an art and those who do it well to be artists. He has an affinity towards getting meta and combining that with admiration of programming\\, has found source code analysis to be exceptionally interesting. Lately\\, his primary focus involves researching and building AI-based pair programming technology.\\n</em></p>","categories":"Observability","url":"https://apachecon.com/acah2020/tracks/observability.html#T1210"},{"uid":"acah2020-observability-T1250@apachecon.com","sequence":"0","dtstamp":"20200828T190237Z","dtstart":"20200929T125000Z","dtend":"20200929T133000Z","summary":"Why averages lie and how to truely monitor your systems","location":"@home","x-alt-desc":"<strong>\\nFilipe Costa Oliveira\\n</strong>\\n<p>\\nWe spend most of our time looking at the reported averages of our monitoring systems\\, completely disregarding the painful truth that the numbers that we look at and present to our bosses\\, to our business and make decisions based upon\\, do not represent our user experience. This simple fact seems to surprise many people. It feels good looking at steady state monitoring charts. In this session\\, you will be told why is it important to pay to the \\\"higher end\\\" of the percentile spectrum in most application monitoring\\, benchmarking\\, and tuning environments and how you can make better usage of the open-source tooling we have at our disposal ( giving examples on both OSS HDR and T-Digest Histograms ).\\n</p>\\n\\n<p><em>\\nPerformance Engineer\\, RedisLabs High-performance scientist\\, low-level C++ grid and distributed computing. Open Source Contributor.\\n</em></p>","categories":"Observability","url":"https://apachecon.com/acah2020/tracks/observability.html#T1250"},{"uid":"acah2020-openoffice-W1615@apachecon.com","sequence":"0","dtstamp":"20200828T190237Z","dtstart":"20200930T161500Z","dtend":"20200930T165500Z","summary":"Apache OpenOffice the Schrdinger App - Quo vadis?","location":"@home","x-alt-desc":"<strong>\\nPeter (petko) Kovacs\\n</strong>\\n<p>\\nThis talk will be a general talk on the situation on the Apache OpenOffice project. I will outline the situation where we are today. Outline a bit the difficulties we have moving to the future. To understand the title see Clabuurn's article at https://www.theregister.com/2018/10/10/apache_open_office_not_dead/ ) Measure position - A short summary on current project state Messure the speed - what is underway and where we go. the project blur effect - A short view on our difficulties and Gaps that we have\\n</p>\\n\\n<p><em>\\nJoined AOO in Sept 2016 Commiter in 2017\\, PMC Memeber in 2017 Chairman at end of 2017 ASF Memember in 2018 ASF Emeritus in end of 2019 Freetime Volunteer\\n</em></p>","categories":"OpenOffice","url":"https://apachecon.com/acah2020/tracks/openoffice.html#W1615"},{"uid":"acah2020-openoffice-W1655@apachecon.com","sequence":"0","dtstamp":"20200828T190237Z","dtstart":"20200930T165500Z","dtend":"20200930T173500Z","summary":"Streiflichter eines langen Weges","location":"@home","x-alt-desc":"<strong>\\nMichael Stehmann\\n</strong>\\n<p>\\nSome highlights of the long history of OpenOffice\\, a nearly 20 years old project\\, which is an Apache top level project since nearly 8 years. The focus is on the german community and the language of the talk will be also german.\\n</p>\\n\\n<p><em>\\nmember of the germanophone community of OpenOffice.org\\, initial committer of Apache OpenOffice\\, member of the PMC of Apache OpenOffice\\, author of an extention for Apache OpenOffice for german lawyers\\, former fellow of the FSFE\\, \\,member of the legal network of FSFE\\, cofounder of Freie Software Freunde e. V.\\, recent chair person of this charitable organisation\\, Debian user since 2002\\n</em></p>","categories":"OpenOffice","url":"https://apachecon.com/acah2020/tracks/openoffice.html#W1655"},{"uid":"acah2020-openoffice-W1735@apachecon.com","sequence":"49","dtstamp":"20200828T190237Z","dtstart":"20200930T173500Z","dtend":"20200930T181500Z","summary":"Building Apache OpenOffice on MacOS","location":"@home","x-alt-desc":"<strong>\\nJim Jagielski\\n</strong>\\n<p>\\nA talk about building Apache OpenOffice on MacOS.\\n</p>\\n\\n<p><em>\\nJim Jagielski is a well-known and acknowledged expert and visionary in open source\\, an accomplished coder\\, and frequent engaging presenter on all things open\\, web\\, and cloud related. As a developer\\, hes made substantial code contributions to just about every core technology behind the internet and web and in 2012 was awarded the OReilly Open Source Award. In 2015\\, he received the Innovation Luminary Award from the EU. He is likely best known as one of the developers and cofounders of the Apache Software Foundation\\, where he has previously served as both chairman and president and where hes been on the board of directors since day one. Hes served as president of the Outercurve Foundation and was also a director of the Open Source Initiative (OSI). He works at Uber in their Open Source Program Office. He credits his wife Eileen with keeping him sane.\\n</em></p>","categories":"OpenOffice","url":"https://apachecon.com/acah2020/tracks/openoffice.html#W1735"},{"uid":"acah2020-openoffice-W1735-2@apachecon.com","sequence":"0","dtstamp":"20200928T220723Z","dtstart":"20200930T173500Z","dtend":"20200930T181500Z","summary":"Translation of an application - How does it work","location":"@home","x-alt-desc":"<strong>\\nMechtilde Stehmann\\n</strong>\\n<p>\\nIn this talk I will show the steps which are need for the translation of Apache OpenOffice. This is an example to do a translation of a very big project\\, too. The translation is done at a Pootle Server hosted by ASF. Most steps are automated by scripts to handle the translation. Nearly 550000 strings in more than 60 languages\\n</p>\\n\\n<p><em>\\nPart of the project since 2005 (former OpenOffice.org) Working on translation\\, qa and the base modul.\\n</em></p>","categories":"OpenOffice","url":"https://apachecon.com/acah2020/tracks/openoffice.html#W1735-2"},{"uid":"acah2020-openoffice-W1815@apachecon.com","sequence":"0","dtstamp":"20200828T190237Z","dtstart":"20200930T181500Z","dtend":"20200930T185500Z","summary":"OpenOffice UNO Programming with Groovy","location":"@home","x-alt-desc":"<strong>\\nCarl Marcum\\n</strong>\\n<p>\\nThe talk will discuss using the Apache Groovy programming language with Apache OpenOffice UNO API's and some associated projects that allow this to happen. Projects include the Groovy UNO Extension that adds convenience methods to the OpenOffice API's allowing less coding\\, an OpenOffice Extension that adds Groovy as a macro language to the office\\, and an associated extension to add sample macros to the office written in Groovy. Examples of usages like Groovy scripts as OpenOffice client applications\\, OpenOffice macros in Groovy\\, and a compiled OpenOffice extension application in Groovy.\\n</p>\\n\\n<p><em>\\nSoftware developer and Open Source enthusiast. Owner of Code Builders\\, LLC specializing in Java-based technologies including Apache Groovy language and the Grails framework for web applications. Apache OpenOffice committer and PMC member. Currently serving as VP OpenOffice. Sun Certified Java Programmer.\\n</em></p>","categories":"OpenOffice","url":"https://apachecon.com/acah2020/tracks/openoffice.html#W1815"},{"uid":"acah2020-openoffice-R1615@apachecon.com","sequence":"0","dtstamp":"20200828T190237Z","dtstart":"20201001T161500Z","dtend":"20201001T165500Z","summary":"OpenOffice on the Web","location":"@home","x-alt-desc":"<strong>\\nDave Fisher\\n</strong>\\n<p>\\nA talk about our Website Infrastructure. - openoffice.org - forum.openoffice.org - wiki.openoffice.org - confluence wikis - bugzilla - mailing lists\\n</p>\\n\\n<p><em>\\nDave has been a member of the Apache OpenOffice PMC since OpenOffice.org started Incubation at the ASF. He ported ported the OpenOffice.org website to the Apache CMS and has helped with Sysadmin tasks with the Forums.\\n</em></p>","categories":"OpenOffice","url":"https://apachecon.com/acah2020/tracks/openoffice.html#R1615"},{"uid":"acah2020-openoffice-R1655@apachecon.com","sequence":"0","dtstamp":"20200828T190237Z","dtstart":"20201001T165500Z","dtend":"20201001T173500Z","summary":"Building and porting Apache OpenOffice","location":"@home","x-alt-desc":"<strong>\\nDamjan Jovanovic\\n</strong>\\n<p>\\nApache OpenOffice currently suffers from a terrible build system and limited portability between platforms and compilers. This talk will cover efforts to switch to a faster and more correct build system using SCons\\, the automated converter that has been developed to help with that\\, and the status of ports to 64 bit Windows\\, AArch64\\, newer MSVC compilers\\, Clang\\, Python 3\\, and newer versions of Java.\\n</p>\\n\\n<p><em>\\nAn open source contributor since 2006\\, and contributor to OpenOffice since it was in incubation\\, Damjan has been its most prolific developer.\\n</em></p>","categories":"OpenOffice","url":"https://apachecon.com/acah2020/tracks/openoffice.html#R1655"},{"uid":"acah2020-pulsar-T0930@apachecon.com","sequence":"1","dtstamp":"20200828T190237Z","dtstart":"20200929T093000Z","dtend":"20200929T101000Z","summary":"Transactional event streaming with Apache Pulsar (Mandarin)","location":"@home","x-alt-desc":"<strong>\\nran gao\\n</strong>\\n<p>\\nTransactional event streaming with Apache Pulsar The highest message delivery guarantee that Apache Pulsar provides is `exactly-once`\\, producing at a single partition via Idempotent Producer. Users are guaranteed that every message produced to a single partition via an Idempotent Producer will be persisted exactly once\\, without data loss. However\\, there is no `atomicity` when a producer attempts to produce messages to multiple partitions. From the consumer side\\, acknowledgement is a best-effort operation\\, which results in message redelivery\\, hence consumer will receive duplicate messages. Pulsar only guarantees `at-least-once` consumption for consumers. It creates inconvenience and brings in complexity when you use Pulsar to build mission critical services (such as billing services). Pulsar introduces transaction support in 2.7.0 version\\, to simplify the process of building reliable and fault resilient services using Apache Pulsar and Pulsar Functions. It only provides the capability to achieve end-to-end exactly-once for streaming jobs in other stream processing engines. This presentation deep dives into the details of Pulsar transaction and how Pulsar transaction is applied to Pulsar Functions and other processing engines to achieve transactional event streaming. How does Pulsar transaction work? How do Pulsar Functions offer transaction support using Pulsar transaction?\\n</p>\\n\\n<p><em>\\nRan Gao is a software engineer at StreamNative. Prior to StreamNative\\, he worked at Zhaopin.com and JD Logistics\\, responsible for the development of the front-end and back-end of the business system. Being interested in open source and messaging systems\\, Ran is an Apache Pulsar contributor.\\n</em></p>","categories":"Pulsar/Bookkeeper","url":"https://apachecon.com/acah2020/tracks/pulsar.html#T0930"},{"uid":"acah2020-pulsar-T1010@apachecon.com","sequence":"1","dtstamp":"20200828T190237Z","dtstart":"20200929T101000Z","dtend":"20200929T105000Z","summary":"Using Apache Pulsar in China Mobile billing system (Mandarin)","location":"@home","x-alt-desc":"<strong>\\nSong Xue\\n</strong>\\n<p>\\nTelecommunication is a complex system. Weve adopted Apache Kafka\\, RocketMQ and other messaging systems in our business. However\\, they hardly meet our requirements. When we met Apache Pulsar\\, we found it was an ideal solution for us. Apache Pulsar has multi-layer and segment-centric architecture\\, and supports geo-replication. We can query data with Pulsar SQL\\, and create complex processing logic without deploying other systems with Pulsar Functions.\\n</p>\\n\\n<p><em>\\nSong Xue is a senior software engineer. He is experienced in telecommunication\\, big data and stream processing.\\n</em></p>","categories":"Pulsar/Bookkeeper","url":"https://apachecon.com/acah2020/tracks/pulsar.html#T1010"},{"uid":"acah2020-pulsar-T1050@apachecon.com","sequence":"1","dtstamp":"20200828T190237Z","dtstart":"20200929T105000Z","dtend":"20200929T113000Z","summary":"Pulsar application in Ksyun cloud log service (Mandarin)","location":"@home","x-alt-desc":"<strong>\\nBin Liu\\n</strong>\\n<p>\\nOur log service is a one-stop service for logging data. The services cover log collection\\, log storage\\, log retrieval and analysis\\, real-time consumption\\, log delivery and so on. Currently\\, our service supports log query and monitoring for many businesses\\, and processes tens of terabytes of data every day. Apache Pulsar is a cloud-native distributed messaging platform with multi-layer and segment-centric architecture and multi-tenancy. With Pulsar\\, we can scale up partitions and merge partitions easily\\, and process millions of topics.\\n</p>\\n\\n<p><em>\\nApache open source community contributor\\, tech lead of Ksyun log service\\n</em></p>","categories":"Pulsar/Bookkeeper","url":"https://apachecon.com/acah2020/tracks/pulsar.html#T1050"},{"uid":"acah2020-pulsar-T1130@apachecon.com","sequence":"1","dtstamp":"20200828T190237Z","dtstart":"20200929T113000Z","dtend":"20200929T121000Z","summary":"The Practice of Apache Pulsar in BIGO (Mandarin)","location":"@home","x-alt-desc":"<strong>\\nHang Chen\\n</strong>\\n<p>\\nPowered by Artificial Intelligence technology\\, BIGO's video-based products and services have gained immense popularity\\, with users in more than 150 countries. These include Bigo Live (live streaming) and Likee (short-form video). Bigo Live is available in more than 150 countries and Likee has more than 100 million users and is popular among the Generation Z. In the past few years\\, we have deployed many Kafka clusters to support real-time ETL and short-form video recommendation. The Apache Pulsar's layered architecture and new features\\, such as Low latency with durability\\, Horizontally scalable\\, Multi-tenancy etc\\, help us solve a lot of problems in production. We have adopted Apache Pulsar to build our Message Processing System\\, especially in Real-Time ETL\\, short-form video recommendation and Real-Time Data report. In this talk\\, I will share our journal of adopting Apache Pulsar in our Real-Time Message Processing System\\, especially in Flink & Flink SQL working with Pulsar. I will also discuss the problems we have encountered in using Pulsar and experience in performance tuning.\\n</p>\\n\\n<p><em>\\nHang Chen is the tech lead of the Messaging Platform team at BIGO. He is responsible for creating a centralized pub-sub messaging Platform\\, which provides a vast number of service/application traffics. He introduced Apache Pulsar into their Messaging Platform and integrated it with upstream and downstream systems\\, such as Flink\\, ClickHouse and other inner systems for Real-Time recommendation and analysis. He focuses on Pulsar performance tuning\\, new features development and Pulsar ecosystem integration.\\n</em></p>","categories":"Pulsar/Bookkeeper","url":"https://apachecon.com/acah2020/tracks/pulsar.html#T1130"},{"uid":"acah2020-pulsar-T1210@apachecon.com","sequence":"1","dtstamp":"20200828T190237Z","dtstart":"20200929T121000Z","dtend":"20200929T125000Z","summary":"Work with Apache Pulsar broker interceptors (Mandarin)","location":"@home","x-alt-desc":"<strong>\\nPenghui Li\\n</strong>\\n<p>\\nBroker interceptor is a new feature that allows users to add the custom interceptor to intercept Pulsar requests. The broker interceptor enables many enterprise features such as audit log\\, reject illegal requests\\, and so on. In this talk\\, I will show how broker interceptor works\\, and how to write a broker interceptor step by step.\\n</p>\\n\\n<p><em>\\nPenghui Li is a PMC member of Apache Pulsar\\, and tech lead in Zhaopin.com\\, where he promotes Apache Pulsar proactively. He focuses on messaging service\\, including messaging system\\, microservice\\, and Apache Pulsar.\\n</em></p>","categories":"Pulsar/Bookkeeper","url":"https://apachecon.com/acah2020/tracks/pulsar.html#T1210"},{"uid":"acah2020-pulsar-T1250@apachecon.com","sequence":"1","dtstamp":"20200828T190237Z","dtstart":"20200929T125000Z","dtend":"20200929T133000Z","summary":"Pulsar adoption in SAAS platform (Mandarin)","location":"@home","x-alt-desc":"<strong>\\nShaohong Pan\\n</strong>\\n<p>\\nIn the past\\, we used AMQP. The service broke down occasionally and had a serious negative impact on our business. AMQP does not support multi-tenancy. I came to know Apache Pulsar last year. After investigation\\, we found it was an ideal streaming data platform and could solve our problems quite well. In this talk\\, I will share how we adopt Pulsar in our parking system. We customize a messaging system with EMQX\\, Pulsar and Sink to deal with our data in our parking system. The following is a general workflow of our business. Upstream: The real-time data at the parking lot is first transmitted to EMQX\\, and then transmitted to Pulsar with a bridge. The business system processes the data and returns the result to Pulsar. Downstream: Sink retrieves the result from Pulsar\\, and sends it to EMQX\\, and then EMQX sends the data to the parking lot. Data analysis: Process data in Hive via pulsar-flink connector. Data query: Develop the features of querying data in Pulsar Manager via Pulsar SQL and sending data via TOPIC.\\n</p>\\n\\n<p><em>\\nShaohong Pan is tech lead of the messaging system(including Pulsar\\, EMQX\\, etc.) at Keytop\\, a leading smart parking solution provider. He introduced Apache Pulsar to Keytop\\, and promote Apache Pulsar in their business proactively.\\n</em></p>","categories":"Pulsar/Bookkeeper","url":"https://apachecon.com/acah2020/tracks/pulsar.html#T1250"},{"uid":"acah2020-pulsar-W0900@apachecon.com","sequence":"1","dtstamp":"20200828T190237Z","dtstart":"20200930T090000Z","dtend":"20200930T094000Z","summary":"Application of Apache Pulsar in Tencent Midas Scenario (Mandarin)","location":"@home","x-alt-desc":"<strong>\\nDezhi Liu\\n</strong>\\n<p>\\nMidas is an Internet billing platform that supports the 100-billion-level revenue in Tencent's internal business. It integrates domestic and international payment channels\\, provides various services such as account management\\, precision marketing\\, security risk control\\, auditing and accounting\\, billing analysis and so on. The platform carries daily revenue of hundreds of millions of dollars. It provides services for 180+ countries (regions)\\, 10\\,000+ businesses and more than 1 million settlers. Working as an all-round one-stop billing platform\\, the total number of its escrow accounts is more than 30 billions. The characteristics of Tencent billing\\, a combination of financial attributes and massive Internet attributes\\, such as tens of billions of account custody and daily tens of billions of transaction requests\\, for such a huge transaction volume and complex business processes\\,Various asynchronous or abnormal situations require the support of distributed message queues. The characteristics of pulsar\\, the cloud-native storage and computing separation design\\, for Tencent's large-scale system\\, on-demand elastic scaling is very necessary\\; millions of topics\\, delayed messages\\, any number of consumers\\, etc.\\, for high concurrency of billing Such scenes are suitable\\; The ability to replicate across regions is also necessary for billing globalization. Combining high consistency\\, high reliability and performance considerations\\, we currently use pulsar as the core part of the system as the standard method of exception handling and communication between services of the consistent transaction engine. It has already carried tens of billions of messages per day and maintains a good stability. In actual operation\\, we also found some problems with pulsar. For example\\, the dependence on zookeeper is still relatively heavy. At present\\, there is no separation of consumption and production of brokers. The cross-region strong consistency is not perfect for node selection. We have tried to solve some of the problems. Submit to the community\\, and others also communicate and discuss with the community. In general\\, pulsar currently meets our needs better. We are also happy to share our experience and problems with you\\, and look forward to more exchanges with each other. Pulsar can be more perfect and have a wider range of applications under the joint efforts of the community.\\n</p>\\n\\n<p><em>\\nFocusing on the development of financial-level distributed components\\, he is mainly engaged in the design and development of distributed distributed message transactions and transaction engines\\, and escorts Tencent's revenue. Pay more attention to the field of distributed messaging\\, participate in the construction of the Apache Pulsar community\\, and introduce the Tencent transaction message bus to the ground.\\n</em></p>","categories":"Pulsar/Bookkeeper","url":"https://apachecon.com/acah2020/tracks/pulsar.html#W0900"},{"uid":"acah2020-pulsar-W0940@apachecon.com","sequence":"1","dtstamp":"20200828T190237Z","dtstart":"20200930T094000Z","dtend":"20200930T102000Z","summary":"AMQP-on-Pulsar  bring native AMQP protocol support to Apache Pulsar\\n(Mandarin)","location":"@home","x-alt-desc":"<strong>\\nHao Zhang\\n</strong>\\n<p>\\nChina Mobile is the Gold Member of OpenStack Foundation and has the largest OpenStack cluster deployment practice in the world. RabbitMQ is the default integration of the message middleware in OpenStack\\, and China Mobile has encountered great challenges in the deployment and maintenance of RabbitMQ. In the OpenStack system\\, RabbitMQ\\, as an RPC communication component\\, has a large number of messages flowing in and out. During the operation process\\, there is often a backlog of messages. This will cause memory exceptions\\, and processes will often be stuck due to memory exceptions. On the other hand\\, RabbitMQ's mirrored queue is used in order to ensure high availability of data. When a node runs into an abnormal state\\, the entire cluster is unavailable regularly. Moreover\\, RabbitMQ's programming language erlang is obscure and difficult to troubleshoot. In summary\\, considering the instability of RabbitMQ cluster\\, the difficulty of operation and maintenance\\, and the difficulty of troubleshooting\\, China Mobile intends to develop a middleware product that can replace RabbitMQ. Then China Mobile's middleware team begins to investigate the self-developed technical route of AMQP message queue. By comparing Qpid\\, RocketMQ and Pulsar\\, China Mobile is attracted by Pulsar's unique architecture which decouples data serving and data storage into separate layers. Apache Pulsar is an event streaming platform designed from the ground up to be cloud-native- deploying a multi-layer and segment-centric architecture. The architecture separates serving and storage into different layers\\, making the system container-friendly. The cloud-native architecture provides scalability\\, availability\\, and resiliency and enables companies to expand their offerings with real-time data-enabled solutions. Pulsar has gained wide adoption since it was open-sourced in 2016 and was designated an Apache Top-Level project in 2018. So we decided to develop AMQP on Pulsar(AoP). By adding the AoP protocol handler in your existing Pulsar cluster\\, you can migrate your existing RabbitMQ applications and services to Pulsar without modifying the code. This enables RabbitMQ applications to leverage Pulsars powerful features\\, such as infinite event stream retention with Apache BookKeeper and tiered storage. I will introduce how we develop AoP\\, the architecture\\, and how to deploy it in container. Then I will present the performance comparison of AoP and RabbitMQ.\\n</p>\\n\\n<p><em>\\nHao Zhang is a senior software engineer at China Mobile\\, where he specializes in message queue and distributed cache with extensive experience in handling high-reliability and high-performance projects. He is also a contributor to Apache Pulsar and Apache RocketMQ.\\n</em></p>","categories":"Pulsar/Bookkeeper","url":"https://apachecon.com/acah2020/tracks/pulsar.html#W0940"},{"uid":"acah2020-pulsar-W1020@apachecon.com","sequence":"2","dtstamp":"20200828T190237Z","dtstart":"20200930T102000Z","dtend":"20200930T110000Z","summary":"Apache Pulsar in AI data service (Mandarin)","location":"@home","x-alt-desc":"<strong>\\nDongliang Jiang\\n</strong>\\n<p>\\nAppen is a leading company in the AI data service area. When serving a large volume of data collection and annotation\\, we faced some challenges on task distribution\\, anti-scamming and AI model training. The traditional task distribution was based on database\\, it has flexibility on messing around data\\, but its not easy to scale horizontally and has performance issues when the dataset grows large. We adopt the Apache Pulsar and NoSQL database solution to resolve those pain points and keep the flexibility. We have also used Apache Pulsar with Apache Flink in our workload reporting\\, anti-scamming and AI model training for both real-time pipeline and batch pipeline. Apache Pulsar plays a key role in our AI data platform as the data lake to connect all the business features and make each component decoupled.\\n</p>\\n\\n<p><em>\\nArchitect in Appen China. Have 20 years experience on high performance computing\\, distributed systems and messaging/streaming architectures.\\n</em></p>","categories":"Pulsar/Bookkeeper","url":"https://apachecon.com/acah2020/tracks/pulsar.html#W1020"},{"uid":"acah2020-pulsar-W1100@apachecon.com","sequence":"2","dtstamp":"20200828T190237Z","dtstart":"20200930T110000Z","dtend":"20200930T114000Z","summary":"Unified data processing with Apache Spark and Apache Pulsar (Mandarin)","location":"@home","x-alt-desc":"<strong>\\nJia Zhai\\, Vincent Xie\\n</strong>\\n<p>\\nLambda is widely used in the industry when people need to process both real-time and historical data to get a result. It is effective\\, and a good balance of speed and reliability. But there are still challenges to use Lambda in the practice. The biggest detraction has been the need to maintain two distinct (and possibly complex) systems to generate both batch and streaming layers. Thus\\, the operational cost of maintaining multiple clusters is nontrivial\\, and in some cases\\, one business logic would have to be split into many segments across different places\\, which is a challenge to maintain as the business grows and it also increases communication overhead. In this session\\, we'd like to present a unique data processing architecture with Apache Spark and Apache Pulsar\\, a solution\\, with the core idea of \\\"One data storage\\, one computing engine\\, and one API\\\"\\, to solve the problems of Lambda architecture.\\n</p>\\n\\n<p><em>\\nJia Zhai is the co-founder of StreamNative\\, as well as PMC member of both Apache Pulsar and Apache BookKeeper\\, and contributes to these two projects continually.\\n<br />\\nVincent (Weisheng) Xie is the chief data scientist and senior Director at Orange Financial. Previously\\, he worked as a tech lead of ML engineering at Intel.\\n\\n\\n</em></p>","categories":"Pulsar/Bookkeeper","url":"https://apachecon.com/acah2020/tracks/pulsar.html#W1100"},{"uid":"acah2020-pulsar-W1140@apachecon.com","sequence":"1","dtstamp":"20200828T190237Z","dtstart":"20200930T114000Z","dtend":"20200930T122000Z","summary":"Serverless Event Streaming with Pulsar Functions (Mandarin)","location":"@home","x-alt-desc":"<strong>\\nXiaolong Ran\\n</strong>\\n<p>\\nApache Pulsar is a cloud-native new generation messaging system and real-time processing platform. The messaging system is closely related to the real-time computing platform\\, and it is often separated and loosely deployed and managed. As the computing component of Pulsar\\, the Pulsar function is a fusion and innovation of the message and computing platform in the serverless direction. The Pulsar function provides multi-language support for Go\\, Python\\, and Java\\; and runtimes for threads\\, processes\\, and Kubernetes. This provides good functionality for users to write\\, run\\, and deploy functions. Let users only care about the logic of the real calculation\\, without complicated configuration or management\\; more convenient built-in message-based streaming platform.\\n</p>\\n\\n<p><em>\\nXiaolong Ran is a Software Engineer at StreamNative and the committer of Apache Pulsar. The main contributor to Go Functions and pulsar-client-go projects.\\n</em></p>","categories":"Pulsar/Bookkeeper","url":"https://apachecon.com/acah2020/tracks/pulsar.html#W1140"},{"uid":"acah2020-pulsar-W1615@apachecon.com","sequence":"0","dtstamp":"20200828T190237Z","dtstart":"20200930T161500Z","dtend":"20200930T165500Z","summary":"Pulsar Function Mesh - Complex Streaming Jobs in a Simple Way","location":"@home","x-alt-desc":"<strong>\\nNeng Lu\\, Sijie Guo\\n</strong>\\n<p>\\nPulsar Function is a succinct computing abstraction Apache Pulsar provides users to express simple ETL and streaming tasks. The simplicity comes in two folds: Simple Interface and Simple Deployment. As it has been adopted\\, we realized that the native support of organizing multiple functions into integrity will be very beneficial. With such support\\, people can express and manage multi-stage jobs easily. In addition\\, this support also provides the possibility of higher-level abstraction DSL to further simplify the job composition. We call this new feature -- Pulsar Function Mesh. This talk aims to provide a thorough walkthrough of this new Pulsar Function Mesh Feature\\, including its design\\, implementation\\, use cases\\, and examples\\, to help people seeking simple streaming solutions understand this newly created powerful tool in Apache Pulsar.\\n</p>\\n\\n<p><em>\\nNeng Lu:<br />\\nNeng Lu is a staff software engineer at StreamNative where he drives the development of Apache Pulsar and the integrations with big data ecosystem. Before that\\, he was a senior software engineer at Twitter. He was the core committer to the Heron project and the leading engineer for Heron development at Twitter. He also worked on Twitters monitoring and key-value storage systems. Before joining Twitter\\, he got his master's degree from UCLA and a bachelor degree from Zhejiang University.<br />\\nSijie Guo:<br />\\nSijie Guo is the co-founder and CEO of StreamNative. StreamNative is a real-time data infrastructure startup offering a cloud-native event streaming platform powered by Apache Pulsar for the enterprises. Before StreamNative\\, he co-founded Streamlio. Before Streamlio\\, he worked for Twitter as the tech lead for the messaging infrastructure group\\, where he co-created DistributedLog and Twitter EventBus. Before Twitter\\, he worked on the push notification infrastructure at Yahoo!. He is also the VP of Apache BookKeeper and PMC member of Apache Pulsar.\\n</em></p>","categories":"Pulsar/Bookkeeper","url":"https://apachecon.com/acah2020/tracks/pulsar.html#W1615"},{"uid":"acah2020-pulsar-W1655@apachecon.com","sequence":"0","dtstamp":"20200828T190237Z","dtstart":"20200930T165500Z","dtend":"20200930T173500Z","summary":"Indestructible storage in the cloud with Apache BookKeeper","location":"@home","x-alt-desc":"<strong>\\nAnup Ghatage\\, Ankit Jain\\, Charan Reddy Guttapalem\\, Karan Mehta\\, Venkateswararao Jujjuri\\n</strong>\\n<p>\\nThis talk highlights how Apache software\\, community\\, and corporate interaction works well together. The Salesforce team goes over how they have implemented a highly durable and available cloud storage service based on Apache Bookkeeper. Specifically\\, they speak about their requirements\\, why they chose Apache BookKeeper and the changes they made in cooperation with the Apache community to make it as cloud-aware. As software is increasingly deployed in public cloud environments\\, foundational platforms such as Apache BookKeeper must also continuously evolve to effectively work in multi availability zone environments and be designed to work around problems unique to such environments. We at Salesforce added to BookKeeper the ability to function effectively in a Multi-AZ public cloud environment. The first step to this was adding awareness in bookies about their location in the cluster. Which then enabled zone aware placement policies and handling of entire zone failures. They also go over how all of these functions without allowing any downtime to upper-level services. All of these changes go hand in hand with the core tenets of Apache BookKeeper's core quorum based storage principles but rethought to work across availability zones in a cloud-native manner. This talk goes over how we manage various challenges such as the creation of ensembles\\, placement\\, and replication of ledgers\\, tolerance to bookie/zone failures\\, upgrade scenarios and backward compatibility all the while satisfying the durability guarantees promised by BookKeeper in a public cloud environment.\\n</p>\\n\\n<p><em>\\nAnup Ghatage<br />\\nAnup works on Salesforce's Infrastructure platform. Previously\\, he has worked on database internals\\, query processing and storage at SAP\\, Cisco Systems and other companies for more than 7 years. Anup holds a BS from the University of Pune and an MS from Carnegie Mellon University. Ask him to perform some close-up magic / read your mind for you.<br />\\nAnkit Jain<br />\\nAnkit has worked in Salesforce big data infrastructure for the past few years after graduating from Carnegie Mellon University. He is passionate about distributed systems and big data.<br />\\nCharan Reddy Guttapalem<br />\\nCharan is a PMTS\\, working on a highly available and durable Storage layer for Database System at Salesforce. He serves as the committer and PMC member for the Apache Bookkeeper project. Previously he worked on Windows Phone client side features and APIs.<br />\\nKaran Mehta<br />\\nKaran has worked in Salesforce big data infrastructure for the past few years after graduating from UC Irvine. Current Apache Phoenix PMC member.<br />\\nVenkateswararao Jujjuri<br />\\nCurrently leading an effort to build a massively scalable\\, highly performant distributed storage service at Salesforce. Previously an Architect and member of the IBM Cloud\\, Open Virtualization. Current Apache Bookkeeper PMC member.\\n</em></p>","categories":"Pulsar/Bookkeeper","url":"https://apachecon.com/acah2020/tracks/pulsar.html#W1655"},{"uid":"acah2020-pulsar-W1815@apachecon.com","sequence":"0","dtstamp":"20200828T190237Z","dtstart":"20200930T181500Z","dtend":"20200930T185500Z","summary":"KoP\\, AoP and MoP - Facilitating interoperability between different messaging protocols in Apache Pulsar","location":"@home","x-alt-desc":"<strong>\\nSijie Guo\\n</strong>\\n<p>\\nPulsar is a cloud-native event streaming platform that provides the ability to connect\\, store\\, and process event streams in real-time. It also provides many different language clients for applications to ingest and consume events and offers connectors for people to connect Pulsar with external systems easily without writing any code. However\\, there are many applications already written in other messaging protocols such as JMS\\, Kafka\\, AMQP\\, and HTTP-based protocols. It is hard for people to rewrite those existing applications. In order to reduce the adoption barrier for the existing world\\, we at StreamNative introduced the protocol handler mechanism in Pulsar 2.5.0 to allow a Pulsar broker to support different message protocols by reusing its core event streaming infrastructure (i.e. multi-layered architecture\\, infinite stream storage\\, multi-tenancy and etc). It facilitates the interoperability between different message protocols in Pulsar. In this talk\\, we will give a deep-dive into the protocol handler mechanism and share the experiences of using this mechanism to support different message protocols (Kafka\\, AMQP\\, REST\\, and etc) and the interoperability in Pulsar.\\n</p>\\n\\n<p><em>\\nSijie Guo is the co-founder and CEO of StreamNative\\, which provides a cloud-native event streaming platform powered by Apache Pulsar. Sijie has worked on messaging and streaming data technologies for more than a decade. Prior to StreamNative\\, Sijie cofounded Streamlio\\, a company focused on real-time solutions. At Twitter\\, Sijie was the tech lead for the messaging infrastructure group\\, where he co-created DistributedLog and Twitter EventBus. Prior to that\\, he worked on the push notification infrastructure at Yahoo!\\, where he was one of the original developers of BookKeeper and Pulsar. He is also the VP of Apache BookKeeper and PMC member of Apache Pulsar. You can follow him on twitter.\\n</em></p>","categories":"Pulsar/Bookkeeper","url":"https://apachecon.com/acah2020/tracks/pulsar.html#W1815"},{"uid":"acah2020-pulsar-W1855@apachecon.com","sequence":"0","dtstamp":"20200828T190237Z","dtstart":"20200930T185500Z","dtend":"20200930T193500Z","summary":"Pulsar Functions Deployment Options","location":"@home","x-alt-desc":"<strong>\\nSanjeev Kulkarni\\n</strong>\\n<p>\\nPulsar functions bring stream processing capabilities to Pulsar topics without needing to setup a different cluster for a processing engine. With its simple API and flexible deployment options\\, it makes it very easy for even novice developers to write stream processing applications that work both on their laptop as well as in the data-center. In this talk\\, I will go over the different deployment models for Pulsar Functions. We will explore the thread-based\\, process-based and Kubernetes based runtime options for running Pulsar Functions. We will also explore different tradeoffs between running functions within the broker vs running them on dedicated function workers.\\n</p>\\n\\n<p><em>\\nSanjeev Kulkarni works on Splunk's Data Stream Processor product\\, focusing on systems and infrastructure layers. Prior to Splunk\\, he was the co-founder of Streamlio that was building next generation real time processing engines based on Apache Pulsar. Before that Sanjeev was the technical lead for real-time analytics at Twitter where he co-created Twitter Heron. Sanjeev also worked in the Adsense team at Google leading several initiatives. He has a MS. in computer science from the University of Wisconsin\\, Madison.\\n</em></p>","categories":"Pulsar/Bookkeeper","url":"https://apachecon.com/acah2020/tracks/pulsar.html#W1855"},{"uid":"acah2020-pulsar-W1935@apachecon.com","sequence":"0","dtstamp":"20200828T190237Z","dtstart":"20200930T193500Z","dtend":"20200930T201500Z","summary":"Streaming Best Practices with Apache Pulsar for Enabling ML","location":"@home","x-alt-desc":"<strong>\\nDevin Bost\\n</strong>\\n<p>\\nIn this presentation\\, we introduce best practices of streaming\\, some of the most important lessons we can teach about how to build sustainable streaming architectures that transform the enterprise. We will cover innovative architectural patterns that combine distributed technologies intended for scale and show how these best practices open the doors for enabling machine learning at an unprecedented level. We will discuss best practices for enabling ML with Kappa architecture. We will also demonstrate how to leverage an innovative approach to streaming validation that builds on existing best practices developed at Overstock and accelerates the productionalization of streaming pipelines.\\n</p>\\n\\n<p><em>\\nWith over 10 years of experience in the software industry\\, Devin has developed software in over 15 different languages. Between his experience of performing data migrations\\, applying vector calculus for ML\\, and building enterprise applications\\, he learned the critical role of data in opening doors of insight into novel market opportunities. He also observed many companies architect their software with the mindset of \\\"well figure out the data later\\,\\\" only to code themselves into life-threatening dead-ends. These observations fueled his interests in context-rich stream-based architectures like Kappa that thrive on live data capture and real-time analysis for instant value-creation.\\n</em></p>","categories":"Pulsar/Bookkeeper","url":"https://apachecon.com/acah2020/tracks/pulsar.html#W1935"},{"uid":"acah2020-royale-T1655@apachecon.com","sequence":"0","dtstamp":"20200828T190237Z","dtstart":"20200929T165500Z","dtend":"20200929T173500Z","summary":"Hello\\, Royale!","location":"@home","x-alt-desc":"<strong>\\nAndrew Wetmore\\n</strong>\\n<p>\\nA high-level\\, task-focused view of Apache Royale\\, including a bit of history\\, how it has evolved\\, its core of AS3 and MXML\\, what it inherits from Flex and how it differs\\, where it is going\\, and what you can do with it.\\n</p>\\n\\n<p><em>\\nDocumentation and QA specialist during 15 years in the software industry with projects ranging from kitchen-table startups to major corporations. Has built several applications using Flex/Royale as the front end technology. Chief editor for the Apache Royale project.\\n</em></p>","categories":"Royale","url":"https://apachecon.com/acah2020/tracks/royale.html#T1655"},{"uid":"acah2020-royale-W1655@apachecon.com","sequence":"1","dtstamp":"20200828T190237Z","dtstart":"20200930T165500Z","dtend":"20200930T173500Z","summary":"Apache Royale - moving a Flex app to Royale","location":"@home","x-alt-desc":"<strong>\\nAndrew Wetmore\\, Alina Kazi\\n</strong>\\n<p>\\nThere are many applications of all sizes that are facing their end-of-life because they were built in Flex presuming the availability of Adobe Flash. Now that browsers are ending their support of Flash\\, we need to either migrate those apps to Royale or throw away a rich code resource. This sessions steps through the process of migrating an existing Flex application to Royale. It explores how the two platforms are similar and where they diverge\\, and the tools that are in place to help you get from the dying app to a one with a new life.\\n</p>\\n\\n<p><em>\\nAndrew has 15 years in QA and documentation in the software industry and experience building many Flex and Royale applications.\\n<br />\\nAlina is a participant in the Apache Royale project\\, and a former staffer at DBIZ Solutions\\, where she was involved in a major migration from Flex to Royale\\n</em></p>","categories":"Royale","url":"https://apachecon.com/acah2020/tracks/royale.html#W1655"},{"uid":"acah2020-royale-W1815@apachecon.com","sequence":"0","dtstamp":"20200828T190237Z","dtstart":"20200930T181500Z","dtend":"20200930T185500Z","summary":"Apache Royale - starting from a blank file","location":"@home","x-alt-desc":"<strong>\\nAndrew Wetmore\\, Carlos Rovira\\n</strong>\\n<p>\\nThis is how-to-do-it session for people who have not worked with Apache Royale\\, and do not have experience in the Adobe/Apache Flex ecosystem. Focusing on a model to-do list application using an MVC structure\\, the session steps through setting up Royale\\, getting the first files compiled and running\\, and what goes into making a robust\\, extensible application that can run on any major browser or as an app on a mobile device.\\n</p>\\n\\n<p><em>\\nAndrew Wetmore:<br />\\nAndrew worked in and led documentation and QA teams in projects ranging from startups to groups in major corporations for 15 years. He is the main editor for the Apache Royale project.<br />\\nCarlos Rovira:<br />\\nCreator of the Jewel component set for Apache Royale. Author of many tutorials and blog entries on different aspects of Royale.\\n</em></p>","categories":"Royale","url":"https://apachecon.com/acah2020/tracks/royale.html#W1815"},{"uid":"acah2020-royale-R1655@apachecon.com","sequence":"0","dtstamp":"20200828T190237Z","dtstart":"20201001T165500Z","dtend":"20201001T173500Z","summary":"Apache Royale: Tour de Jewel","location":"@home","x-alt-desc":"<strong>\\nAndrew Wetmore\\, Carlos Rovira\\n</strong>\\n<p>\\nA walk-through of the Jewel component set\\, using the examples in the Tour de Jewel site\\, showing how Royale code achieves a wide range of effects and functions for a user interface.\\n</p>\\n\\n<p><em>\\nAndrew Wetmore:<br />\\n15 years' experience leading QA and documentation teams for software projects large and small. Experience building Flex/Flash and Royale applications.<br />\\nCarlos Rovira:<br />\\nCreator of the Tour de Jewel feature and author of many tutorials on using Royale.\\n</em></p>","categories":"Royale","url":"https://apachecon.com/acah2020/tracks/royale.html#R1655"},{"uid":"acah2020-royale-R1815@apachecon.com","sequence":"0","dtstamp":"20200828T190237Z","dtstart":"20201001T181500Z","dtend":"20201001T185500Z","summary":"Apache Royale Masterclass","location":"@home","x-alt-desc":"<strong>\\nAndrew Wetmore\\n</strong>\\n<p>\\nHow to get the best out of Royale. Includes discussion and demonstration of emulation\\, strands and beads\\, data binding\\, CSS use\\, components\\, frameworks\\, importing libraries from the JavaScript world\\, IDEs that work with Royale\\, and other cool Royale features.\\n</p>\\n\\n<p><em>\\n15 years running documentation and QA teams. Editor-writer for the Apache Infrastructure team.\\n</em></p>","categories":"Royale","url":"https://apachecon.com/acah2020/tracks/royale.html#R1815"},{"uid":"acah2020-search-T1615@apachecon.com","sequence":"1","dtstamp":"20200810T143451Z","dtstart":"20200929T161500Z","dtend":"20200929T165500Z","summary":"Tales From The Trenches: Solr Operations","location":"@home","x-alt-desc":"<strong>\\nMike Drob\\n</strong>\\n<p>\\nThere are many pitfalls that a team can fall into when designing and implementing a new Solr-based search application. We will draw on stories from the presenter's operational experience and distill the events into easy to understand patterns and anti-patterns. Topics covered would include query patterns\\, indexing patterns\\, and shard design.\\n</p>\\n\\n<p><em>\\nAn engineer with over a decade of distributed systems experience\\, Mike has spent most of his career helping enable others who are using big data platforms. He is a PMC member and committer on several Apache projects\\, and strongly believes that when people develop breadth in their expertise it builds better software all around. When not working\\, he enjoys photography\\, dogs\\, photography of dogs.\\n</em></p>","categories":"Lucene/Solr/Search","url":"https://apachecon.com/acah2020/tracks/search.html#T1615"},{"uid":"acah2020-search-T1655@apachecon.com","sequence":"1","dtstamp":"20200810T143451Z","dtstart":"20200929T165500Z","dtend":"20200929T173500Z","summary":"Improving Search Availability: Striving for more 9s","location":"@home","x-alt-desc":"<strong>\\nShubhro Roy\\n</strong>\\n<p>\\nAvailability is a critical aspect of any distributed system\\, especially when your customer's mission critical applications depend on it. But what does availability really mean for Search and how do we measure it? Once measured how do we ensure a multi-cluster deployment of Apache Solr with terabyte scale sharded inverted index hits the holy grail of 4 9s of availability ? How do we automatically detect failures with such systems and what are our options to handle and recover from such failures without human intervention ? In this talk we will discuss various architectural choices and deployment strategies we have adopted at Box to improve availability of search while supporting high-throughput\\, near real-time indexing\\, low latency and multi-tenancy. We will share our learnings from various issues we have faced running Solr at scale and how we have address them by building additional scaffolding or tweaking Solr itself. Come take a peek under the hood of Box Search.\\n</p>\\n\\n<p><em>\\nShubhro enjoys working with data at scale\\, be it indexing\\, mining or analyzing it. Currently he is part of the Search team at Box\\, building infrastructure components that enable millions of users to find relevant content. Prior to Box\\, Shubhro worked on full text database search at Oracle. He has been working on enterprise search and data discovery for the past 8 years after graduating from Carnegie Mellon University with Masters in Information Systems\\, specializing in Information Retrieval and Machine Learning.\\n</em></p>","categories":"Lucene/Solr/Search","url":"https://apachecon.com/acah2020/tracks/search.html#T1655"},{"uid":"acah2020-search-T1735@apachecon.com","sequence":"1","dtstamp":"20200810T143451Z","dtstart":"20200929T173500Z","dtend":"20200929T181500Z","summary":"Open Source Docs as Code","location":"@home","x-alt-desc":"<strong>\\nCassandra Targett\\n</strong>\\n<p>\\nThis talk will review the Lucene community's experiences maintaining Solr documentation in the same way we maintain code. Prior to 2016\\, the Solr Reference Guide was only in Confluence (cwiki). Despite community agreement that docs are important\\, editing them was a separate process that was easy to put off. That put a burden on a few committers to update the content for each new release\\, and frequently each version's Guide was not complete for 2-3 months after a release was announced. In 2016 we decided to integrate the documentation with our source code. We converted Confluence pages to AsciiDoc files and started generating static HTML pages hosted in our main website. These changes allowed committers to update documentation as they changed the code. In an open source project where everyone is a volunteer and there are possibly only 1-2 people who understand any feature\\, this has been an incredibly empowering change. Today committer maintenance of docs is high enough that the Guide requires very little effort to prepare for publication. All Release Managers can publish it as part of the release process\\, reducing the burden on the few who knew their way around the old system. This engagement means the Guide can evolve quickly as community needs change. In this talk I'll share how we made these choices\\, the content and build tools we use\\, and how other projects can make updating docs a natural part of the code change process.\\n</p>\\n\\n<p><em>\\nCassandra has 20 years experience in search and knowledge management. She has been an Apache Lucene committer since 2013 and a member of the PMC since 2016. As Director of Engineering at Lucidworks\\, she manages the day-to-day work of the Solr development team.\\n</em></p>","categories":"Lucene/Solr/Search","url":"https://apachecon.com/acah2020/tracks/search.html#T1735"},{"uid":"acah2020-search-T1815@apachecon.com","sequence":"1","dtstamp":"20200810T143451Z","dtstart":"20200929T181500Z","dtend":"20200929T185500Z","summary":"An Anatomy of an Answer: Open NLP & Discourse Analysis - based Indexing","location":"@home","x-alt-desc":"<strong>\\nBoris Galitsky\\n</strong>\\n<p>\\nIndexers usually index all text in documents. However\\, once we learn to \\\"understand\\\" the logic of a plain text\\, we will see how bad for a search it is to index the whole thing. Discourse analysis helps to select text fragment which should be matched with a potential query\\, and throw away the rest In this talk we will apply discourse linguistic to practical text search and discover that the majority of indexers which index all text perform very poorly for complex queries. Relying on standard relevance means such as TF*IDF does not alleviate this problem. We will explore how discourse analysis helps search by identifying text fragments which should be indexed and matched with potential queries\\, and those text fragments which would mislead the search and make its precision low. We will demonstrate how a discourse analysis - based indexer can be employed relying on Apache Open NLP project. The audience will learn how discourse analysis formalizes a logic of text to be searched and represents it as a discourse tree\\, a structure to represent a domain-independent logical organization of text essential for finding relevant fragments. We will also discuss how to proceed from search engines like SOLR to chatbots\\, where discourse analysis helps with dialogue management.\\n</p>\\n\\n<p><em>\\nBoris Galitsky has been presenting talks on AI over last two decades and at Apache conferences over last few years. He contributed linguistic and machine learning technologies to Silicon Valley startups for last 25 years\\, as well as eBay and Oracle\\, where he is currently an architect of the Digital Assistant project. An author of three computer science books\\, 150+ publications and 20+ patents related to search\\, he is now working on a book \\\"AI for CRM\\\" to be published by Springer in 2021. Boris is Apache committer to OpenNLP where he created OpenNLP.Similarity component which is a basis for search engine and chatbot development.\\n</em></p>","categories":"Lucene/Solr/Search","url":"https://apachecon.com/acah2020/tracks/search.html#T1815"},{"uid":"acah2020-search-W1615@apachecon.com","sequence":"0","dtstamp":"20200812T151913Z","dtstart":"20200930T161500Z","dtend":"20200930T165500Z","summary":"Concurrent Search In Lucene","location":"@home","x-alt-desc":"<strong>\\nAtri Sharma\\n</strong>\\n<p>\\nConcurrent search is not a new feature in Lucene but has been unexplored. This talk will talk about the basics\\, benefits\\, when to use and when not to use and recent improvements in this area. Concurrent search can provide a massive gain for analytical queries\\, which are becoming more and more popular as data volumes grow. Single threaded queries do not take the complete advantage of available CPU resources -- something that concurrent query fixes. This talk will take audience through a complete know-hows and integrating with existing search platforms built using Lucene.\\n</p>\\n\\n<p><em>\\nDatabase and search guy. Apache Lucene and Apache Solr committer. Major contributor to PostgreSQL.\\n</em></p>","categories":"Lucene/Solr/Search","url":"https://apachecon.com/acah2020/tracks/search.html#W1615"},{"uid":"acah2020-search-W1655@apachecon.com","sequence":"0","dtstamp":"20200812T151913Z","dtstart":"20200930T165500Z","dtend":"20200930T173500Z","summary":"Solr's new Plugin System","location":"@home","x-alt-desc":"<strong>\\nDavid Smiley\\n</strong>\\n<p>\\nSolr 8.4 has a new plugin system that portends of a better future much improved from today: (a) load plugins at runtime\\, (b) find 3rd party plugins in a registry\\, (c) fetch\\, install\\, and even configure plugins from the command line (CLI)\\, (d) a more slimmed down Solr distribution that is more secure. After an overview\\, you will see this system demonstrated\\, after which you should feel comfortable in trying it out for yourself when you leave. Beyond the CLI demonstration\\, we'll look behind the covers a bit to show some of how it works. We'll finish with a discussion of the gaps and thus what the future hopefully holds as this new mechanism blossoms. You'll learn a bit about what it takes to \\\"package\\\" up your plugins too.\\n</p>\\n\\n<p><em>\\nDavid Smiley is a prolific Apache Lucene/Solr committer/PMC member and ASF member. David has written books\\, delivered training\\, and speaks at meetups & conferences on this subject. Ultimately\\, his passion his hacking on Lucene & Solr. He works on search at Salesforce which graciously supports these endeavors.\\n</em></p>","categories":"Lucene/Solr/Search","url":"https://apachecon.com/acah2020/tracks/search.html#W1655"},{"uid":"acah2020-search-W1735@apachecon.com","sequence":"0","dtstamp":"20200812T151913Z","dtstart":"20200930T173500Z","dtend":"20200930T181500Z","summary":"Monitoring Apache Solr Ecosystem on Kubernetes","location":"@home","x-alt-desc":"<strong>\\nAmrit Sarkar\\n</strong>\\n<p>\\nKubernetes is fast becoming the operating system for the Cloud and brings a ubiquity that has the potential for massive benefits for technology organizations. Applications/Microservices are moved to orchestration tools like Kubernetes to leverage features like horizontal autoscaling\\, fault tolerance\\, CICD and more. Apache Solr can be deployed on Kubernetes on a large-scale for a plethora of use cases. For such scale\\, effective metric dashboards\\, log analytics\\, monitoring\\, and alerting system is a requirement to make sure abnormal behaviors are detected\\, error diagnostics are performed and the ability to fine-tune the entire ecosystem to reach the best possible performance. In this talk\\, we discuss and compare various monitoring and analytics tools for the Solr ecosystem running on Kubernetes. From inbuilt features to third-party tools which provide powerful yet easy to use dynamic dashboards and OpenTracing support.\\n</p>\\n\\n<p><em>\\nAmrit Sarkar is Cloud Search Reliability Engineer at Lucidworks Inc\\, California-based enterprise search technology company\\, with 4+ years experience in search domain and big data\\, e-commerce and product. He is working primarily on running search-based applications on Kubernetes\\, and developing and improving core components of Apache Solr.\\n</em></p>","categories":"Lucene/Solr/Search","url":"https://apachecon.com/acah2020/tracks/search.html#W1735"},{"uid":"acah2020-search-W1815@apachecon.com","sequence":"0","dtstamp":"20200812T151913Z","dtstart":"20200930T181500Z","dtend":"20200930T185500Z","summary":"Towards an open source tool stack for e-commerce search","location":"@home","x-alt-desc":"<strong>\\nEric Pugh\\, Ren Kriegler\\n</strong>\\n<p>\\nSearch teams in the e-commerce space want to own their search: they want to understand how exactly the retrieval works and optimise it according to their specific needs\\, both from the user and from the seller perspective. Implementing search using open source search engines\\, such as Solr and Elasticsearch\\, seems like a perfect match. Unfortunately\\, the open source solutions available today arent anywhere near reaching parity with a commercial solution out of the box\\, especially when it comes to optimizing search relevance and managing individual queries as a merchandiser. This leads to a very difficult buy vs build decision\\, especially for smaller teams that dont have deep search expertise already and are faced with developing significant functionality for digital commerce from scratch. In this session we will introduce Chorus: an initiative to combine open source tools and libraries like Querqy (powerful query rewriting library)\\, SMUI (a search management UI to boost and bury products and categories)\\, and the Quepid\\, RRE\\, and Quaerite (search relevance assessment and tuning projects) into a single template to accelerate the development of your own e-commerce search\\, allowing you to shift from setting up basic search functionality to domain specific optimizations much faster.\\n</p>\\n\\n<p><em>\\nEric Pugh:<br />\\nFascinated by the craft of software development\\, Eric Pugh has been involved in the open source world as a developer\\, committer and user for the past fifteen years. He is a member of the Apache Software Foundation and continues to be very active in the Solr and Tika projects\\, as well as avidly reads every commit to the Zeppelin project! In biotech\\, financial services\\, and defense IT\\, he has helped European and American companies develop coherent strategies for embracing open source software. Eric became involved in Solr when he submitted the patch SOLR-284 for extracting text from binary files (such as PDF and MS Office formats)\\, that subsequently became the single most popular patch as measured by votes! He co-authored the book Apache Solr Enterprise Search Server\\, now on its third edition. Today he helps OSCs clients build their own search teams and improve their search maturity\\, both by leading projects and by acting as a trusted advisor.<br />\\nRen Kriegler:<br />\\nRen has been working as a freelance search consultant for clients in Germany and abroad for more than ten years. Although he is interested in all aspects of search and NLP\\, key areas include search relevance and e-commerce search. His technological focus is on Solr/Elasticsearch/Lucene. Ren is founder and co-organiser of MICES (Mix-Camp E-Commerce Search - a Berlin Buzzwords partner event). He maintains Querqy - an open source library for query pre-processing.\\n</em></p>","categories":"Lucene/Solr/Search","url":"https://apachecon.com/acah2020/tracks/search.html#W1815"},{"uid":"acah2020-streaming-T1615@apachecon.com","sequence":"1","dtstamp":"20200810T143451Z","dtstart":"20200929T161500Z","dtend":"20200929T165500Z","summary":"No More Silos: Integrating Databases and Apache Kafka","location":"@home","x-alt-desc":"<strong>\\nRobin Moffatt\\n</strong>\\n<p>\\nCompanies new and old are all recognising the importance of a low-latency\\, scalable\\, fault-tolerant data backbone\\, in the form of the Apache Kafka streaming platform. With Kafka\\, developers can integrate multiple sources and systems\\, which enables low latency analytics\\, event-driven architectures and the population of multiple downstream systems. In this talk\\, well look at one of the most common integration requirements - connecting databases to Kafka. Well consider the concept that all data is a stream of events\\, including that residing within a database. Well look at why wed want to stream data from a database\\, including driving applications in Kafka from events upstream. Well discuss the different methods for connecting databases to Kafka\\, and the pros and cons of each. Techniques including Change-Data-Capture (CDC) and Kafka Connect will be covered. Attendees of this talk will learn: * That all data is event streams\\; databases are just a materialised view of a stream of events. * The best ways to integrate databases with Kafka. * Anti-patterns of which to be aware. * The power of ksqlDB for transforming streams of data in Kafka.\\n</p>\\n\\n<p><em>\\nRobin is a Senior Developer Advocate at Confluent\\, the company founded by the original creators of Apache Kafka\\, as well as an Oracle ACE Director (Alumnus). He has been speaking at conferences since 2009 including QCon\\, Devoxx\\, Strata\\, Kafka Summit\\, and redev. You can find many of his talks online at http://rmoff.net/talks/\\, and his blog articles at http://cnfl.io/rmoff and http://rmoff.net/. Outside of work he enjoys drinking good beer and eating fried breakfasts\\, although generally not at the same time.\\n</em></p>","categories":"Streaming","url":"https://apachecon.com/acah2020/tracks/streaming.html#T1615"},{"uid":"acah2020-streaming-T1655@apachecon.com","sequence":"2","dtstamp":"20200810T143451Z","dtstart":"20200929T165500Z","dtend":"20200929T173500Z","summary":"Achieve the event-driven Nirvana with Apache Druid","location":"@home","x-alt-desc":"<strong>\\nAbdelkrim Hadjidj\\n</strong>\\n<p>\\nAfter two decades of transforming into data-driven organizations\\, companies are moving to the next level: building event-driven organizations. An event-driven organization achieves faster insights\\, a better customer experience and more agility. However\\, this transformation requires advanced skills to make sense of all the events in real-time which put business people on the side. In this presentation\\, we will review the data architectures used by the most advanced event driven organizations today. We will discuss the challenges they face on delivering the promised business value and why stream processing technologies like Apache Kafka and Apache Flink are not enough to achieve the streaming nirvana. Finally\\, we will explain how Apache Druid enables self-service BI on event data and allows business users to ask their own questions leading to real-time insights.\\n</p>\\n\\n<p><em>\\nAbdelkrim is a Data expert with 12 years experience on distributed systems (big data\\, IoT\\, peer to peer and cloud). He helps customers in EMEA using open source streaming technologies such as Apache Kafka\\, NiFi\\, Flink and Druid to pivot into event driven organizations. Abdelkrim is currently working as a Senior Solution Engineer at Imply. Previously\\, He held several positions including Senior Streaming Specialist at Cloudera\\, Solution Engineer at Hortonworks\\, Big Data Lead at Atos and CTO at Artheamis. He published several scientific papers at well-known IEEE and ACM journals. You can find him talking at Meetups or worldwide tech conferences such as Dataworks Summit\\, Strata or Flink Forward. He founded and runs the Future Of Data Meetup in Paris which is a group of 2300+ data and tech enthusiasts.\\n</em></p>","categories":"Streaming","url":"https://apachecon.com/acah2020/tracks/streaming.html#T1655"},{"uid":"acah2020-streaming-T1735@apachecon.com","sequence":"2","dtstamp":"20200810T143451Z","dtstart":"20200929T173500Z","dtend":"20200929T181500Z","summary":"Incrementally Streaming RDBMS Data to Your DataLake Automagically","location":"@home","x-alt-desc":"<strong>\\nTimothy Spann\\n</strong>\\n<p>\\nThere is often data locked in transactional relational systems that you would like to ingest\\, transform\\, parse\\, aggregate\\, and store forever in Hadoop as wide tables. With the new features in Apache NiFi\\, Cloudera Schema Registry\\, HBase 2\\, Phoenix\\, Hive 3\\, Kudu\\, Spark 2\\, Kafka\\, Ranger\\, Atlas\\, Zeppelin and Hue this becomes something you can do at scale without the heavy hand processing of yore. Now with the hybrid cloud\\, you may want to securely ingest to multiple clusters with new tools including Streams Replication Manager. They told me it's not ETL or ELT\\, exactly it is so much more. You now have full control over global data assets with full management\\, full control and smart dashboards to allow a true enterprise open source solution for all your data. With materialized views and the ability to federate queries to JDBC and other data sources your fully ACID Hive 3 tables allow for you to escape the small scale EDW and be reborn in unlimited scale data worlds. References: https://community.cloudera.com/t5/Community-Articles/ETL-With-Lookups-with-Apache-HBase-and-Apache-NiFi/ta-p/248243 https://community.cloudera.com/t5/Community-Articles/Ingesting-RDBMS-Data-As-New-Tables-Arrive-Automagically-into/ta-p/246214 https://community.cloudera.com/t5/Community-Articles/Incrementally-Streaming-RDBMS-Data-to-Your-Hadoop-DataLake/ta-p/247927 https://community.cloudera.com/t5/Community-Articles/Ingesting-Golden-Gate-Records-From-Apache-Kafka-and/ta-p/247557 https://www.datainmotion.dev/2020/05/cloudera-flow-management-101-lets-build.html\\n</p>\\n\\n<p><em>\\nTim Spann is a Principal DataFlow Field Engineer at Cloudera\\, the Big Data Zone leader and blogger at DZone and an experienced data engineer with 15 years of experience. He runs the Future of Data Princeton meetup as well as other events. He has spoken at Philly Open SOurce\\, ApacheCon in Montreal\\, Strata NYC\\, Oracle Code NYC\\, IoT Fusion in Philly\\, meetups in Princeton\\, NYC\\, Philly\\, Berlin and Prague\\, DataWorks Summits in San Jose\\, Berlin and Sydney.\\n</em></p>","categories":"Streaming","url":"https://apachecon.com/acah2020/tracks/streaming.html#T1735"},{"uid":"acah2020-streaming-T1815@apachecon.com","sequence":"2","dtstamp":"20200810T143451Z","dtstart":"20200929T181500Z","dtend":"20200929T185500Z","summary":"Introduction to Event Streams Development with Kafka Streams","location":"@home","x-alt-desc":"<strong>\\nBill Bejeck\\n</strong>\\n<p>\\nDevelopers today work with a lot of data. Much of this data is available near real-time. And it presents the opportunity for businesses and organizations to improve service and deliver more value to users of today's applications. But the question is\\, how to manage this incoming stream of records? Viewing the incoming data as event streams is one way to think about working with data. In recent years\\, Apache Kafka has become a defacto standard for ingesting record streams. To work with the incoming data\\, Apache Kafka provides a Producer and Consumer interface as the basic building blocks for sending to and reading records from Kafka. When building a Kafka-based microservice\\, using the Producer and Consumer clients means handling all the details of communicating yourself. To enable building event-driven applications\\, Apache Kafka provides Kafka Streams. Kafka Streams is the native stream procession library for Apache Kafka In this talk\\, we'll review Kafka and how it can function as a central nervous system for incoming data. From there\\, we'll cover how Kafka Producers and Consumers work and how developers can build a microservice using these building blocks. Finally\\, we'll transition our application to a Kafka Streams application and demonstrate how using Kafka Streams can simplify building a Kafka based microservice. Attendees of this presentation will gain the knowledge needed to understand how Kafka Streams works and how they can get started using it to simplify the development of applications involving Apache Kafka. Additionally\\, developers in attendance that aren't familiar with Apache Kafka itself will gain an understanding of how it can help their business or organization make effective use of available incoming event streams.\\n</p>\\n\\n<p><em>\\nBill Bejeck is working at Confluent as an integration architect on the Developer Relations Team before that Bill was a software engineer on the Kafka Streams team for three years. He has been a software engineer for over 17 years and has regularly contributed to Kafka Streams. Before Confluent\\, he worked on various ingest applications as a U.S. Government contractor using distributed software such as Apache Kafka\\, Apache Spark\\, and Apache Hadoop. Bill is a committer to Apache Kafka and has also written a book about Kafka Streams titled Kafka Streams in Action.\\n</em></p>","categories":"Streaming","url":"https://apachecon.com/acah2020/tracks/streaming.html#T1815"},{"uid":"acah2020-streaming-T1855@apachecon.com","sequence":"1","dtstamp":"20200810T143451Z","dtstart":"20200929T185500Z","dtend":"20200929T193500Z","summary":"Change Data Capture with Flink SQL and Debezium","location":"@home","x-alt-desc":"<strong>\\nMarta Paes\\n</strong>\\n<p>\\nChange Data Capture (CDC) has become the standard to capture and propagate committed changes from a database to downstream consumers\\, for example to keep multiple datastores in sync and avoid common pitfalls such as dual writes (remember? \\\"Friends don't let friends do dual writes\\\"). Consuming these changelogs with Apache Flink used to be a pain\\, but the latest release (Flink 1.11) introduced not only support for CDC\\, but support for CDC from the comfort of your SQL couch. In this talk\\, we'll demo how to use Flink SQL to easily process database changelog data generated with Debezium.\\nAbout the speaker(s):\\n</p>\\n\\n<p><em>\\nMarta is a Developer Advocate at Ververica (formerly data Artisans) and a contributor to Apache Flink. After finding her mojo in open source\\, she is committed to making sense of Data Engineering through the eyes of those using its by-products. Marta holds a Masters in Biomedical Engineering\\, where she developed a particular taste for multi-dimensional data visualization\\, and previously worked as a Data Warehouse Engineer at Zalando and Accenture.\\n</em></p>","categories":"Streaming","url":"https://apachecon.com/acah2020/tracks/streaming.html#T1855"},{"uid":"acah2020-streaming-T1935@apachecon.com","sequence":"1","dtstamp":"20200828T190238Z","dtstart":"20200929T193500Z","dtend":"20200929T201500Z","summary":"Real-Time Stock Processing With Apache NiFi\\, Apache Flink and Apache Kafka","location":"@home","x-alt-desc":"<strong>\\nPierre Villard\\, Timothy Spann\\n</strong>\\n<p>\\nWe will ingest a variety of real-time feeds including stocks with NiFi\\, filter and process and segment it into Kafka topics. Kafka data will be in Apache Avro format with schemas specified in Cloudera Schema Registry. Apache Flink\\, Kafka Connect and NiFi will do additional event processing along with machine learning and deep learning. We will store real-time feed data in Apache Kudu for real-time analytics and summaries. Apache OpenNLP\\, Apache MXNet\\, CoreNLP\\, NLTK and SpaCy will be used to analyse stock trend data in streams as well as stock prices and futures. As part of the stream processing we will also be classifying images and stock data with Apache MXNet and DJL. We will also produce cleaned and aggregated data to subscribers via Apache Kafka\\, Apache Flink SQL and Apache NiFi. We will push to applications\\, message listeners\\, web clients\\, Slack channels and to email\\, To be useful in our enterprise\\, we will have full authorization\\, authentication\\, auditing\\, data encryption and data lineage via Apache Ranger\\, Apache Atlas and Apache NiFi. References: https://community.cloudera.com/t5/Community-Articles/Real-Time-Stock-Processing-With-Apache-NiFi-and-Apache-Kafka/ta-p/249221\\n</p>\\n\\n<p><em>\\nPierre Villard is currently a Senior Product Manager at Cloudera in charge of all the products around Apache NiFi and its subprojects like the NiFi Registry\\, MiNiFi agents\\, etc.. He has been active in the Apache NiFi project for the last 4.5 years and is a committer and PMC member of the project. Before joining Cloudera\\, Pierre worked at Google and Hortonworks where he helped customers develop solutions on-premises and in the cloud by using many technologies including Apache NiFi.<br />\\nTim Spann is a Principal DataFlow Field Engineer at Cloudera\\, the Big Data Zone leader and blogger at DZone and an experienced data engineer with 15 years of experience. He runs the Future of Data Princeton meetup as well as other events. He has spoken at Philly Open SOurce\\, ApacheCon in Montreal\\, Strata NYC\\, Oracle Code NYC\\, IoT Fusion in Philly\\, meetups in Princeton\\, NYC\\, Philly\\, Berlin and Prague\\, DataWorks Summits in San Jose\\, Berlin and Sydney.\\n</em></p>","categories":"Streaming","url":"https://apachecon.com/acah2020/tracks/streaming.html#T1935"},{"uid":"acah2020-streaming-W1615@apachecon.com","sequence":"2","dtstamp":"20200812T151913Z","dtstart":"20200930T161500Z","dtend":"20200930T165500Z","summary":"Interactive Streaming Data Analytics via Flink on Zeppelin","location":"@home","x-alt-desc":"<strong>\\nJeff Zhang\\n</strong>\\n<p>\\nFlink is a powerful distributed streaming engine\\, but it requires lots of programming skills. Even Flink supports sql\\, it is not an easy job for an analyst to use Flink to do streaming data analytics directly. Fortunately\\, another apache project Zeppelin integrates Flink and make streaming data analytics pretty easy for these data analyst without programming skillset. In this talk\\, I would talk about how to use Flink on Zeppelin to do interactive streaming data analytics. And how to build real time dashboard without writting any html/js code.\\n</p>\\n\\n<p><em>\\nJeff has 11 years of experience in big data industry. He is an open source veteran\\, start to use hadoop since 2009 and is PMC of apache project Tez/Livy/Zeppelin and committer of apache Pig. His past experience is not only on big data infrastructure\\, but also on how to leverage these big data tools to get insight. He speaks several times on big data conferences like hadoop summit\\, strata + hadoop world. Now he works in Alibaba Group as a staff engineer. Prior that he works in Hortonworks where he develops these popular big data tools.\\n</em></p>","categories":"Streaming","url":"https://apachecon.com/acah2020/tracks/streaming.html#W1615"},{"uid":"acah2020-streaming-W1655@apachecon.com","sequence":"2","dtstamp":"20200812T151913Z","dtstart":"20200930T165500Z","dtend":"20200930T173500Z","summary":"Fast Samza SQL: Stream Processing Made Easy","location":"@home","x-alt-desc":"<strong>\\nWeiqing Yang\\, Aditya Toomula\\n</strong>\\n<p>\\nApache Samza is a distributed stream processing framework that allows users to process and analyze data in real-time. Fast Samza SQL (FSS) is a managed stream processing service\\, powering hundreds of Samza pipelines in production across LinkedIn. Use cases like stream repartitioning\\, change capture views\\, materialized views\\, data migration\\, and data caching are the popular ones hosted by FSS. Such stream processing pipelines are expressed declaratively\\, with Samza SQL being the predominant DSL that FSS offers. Due to its SQL-like syntax\\, rich authoring and testing environment\\, users can create and deploy their stream processing jobs in a self-serve fashion within a few minutes. FSS also enables creation of stream processing pipelines programmatically. Users just need to focus on their business logic while FSS takes care of the rest\\, such as dependency management\\, resource provisioning\\, auto-scaling\\, job monitoring\\, failure recovery\\, etc. In this talk\\, we will introduce the overall FSS architecture\\, highlight the unique value propositions that FSS brings to stream processing at LinkedIn and share the experiences and lessons we have learned.\\n</p>\\n\\n<p><em>\\nWeiqing Yang<br />\\nWeiqing has been working in big data computation frameworks since 2015 and is an Apache Spark/HBase/Hadoop/Samza contributor. She is currently a software engineer in streaming infrastructure team at LinkedIn\\, working on Samza\\, Kafka\\, etc. Before that\\, she worked in Spark team at Hortonworks. Weiqing obtained a Master Degree in Computational Data Science from Carnegie Mellon University. Weiqing enjoys speaking at conferences. She presented in Spark Summit 2017\\, HBaseCon 2017\\, and KubeCon + CloudNativeCon North America 2019.<br />\\nAditya Toomula<br />\\nAditya has been working at Linkedin in streams infrastructure team since 2016. He has contributed to Apache Samza and Brooklin with latest contributions to Samza Sql and fully managed Samza. He is an Apache Samza committer and has over 15 years of Software Engineering experience. In his earlier life\\, he worked in Storage domain at NetApp\\, building various kinds of replication products and file systems.\\n</em></p>","categories":"Streaming","url":"https://apachecon.com/acah2020/tracks/streaming.html#W1655"},{"uid":"acah2020-streaming-W1735@apachecon.com","sequence":"0","dtstamp":"20200812T151913Z","dtstart":"20200930T173500Z","dtend":"20200930T181500Z","summary":"Fresh updates about the new Beam Spark Structured Streaming runner","location":"@home","x-alt-desc":"<strong>\\nEtienne Chauchot\\n</strong>\\n<p>\\nApache Beam provides a unified programming model to execute batch and streaming pipelines on all the popular big data engines. The translation layer from Beam to the chosen big data engine is called a runner. A little more than one year ago\\, a new Spark runner based on Spark Structured Streaming framework was started and it has been merged to Beam master since. This talk will give updates about this new runner showing some added features\\, some performance improvements and also things that are yet to come.\\n</p>\\n\\n<p><em>\\nEtienne has been working in software engineering and architecture for more than 16 years. He is focused on Big Data subjects. He is an Open Source fan and contributes to Apache projects such as Apache Beam\\, Apache Flink or Apache Spark. He is a Beam committer and PMC member.\\n</em></p>","categories":"Streaming","url":"https://apachecon.com/acah2020/tracks/streaming.html#W1735"},{"uid":"acah2020-streaming-W1815@apachecon.com","sequence":"0","dtstamp":"20200812T151913Z","dtstart":"20200930T181500Z","dtend":"20200930T185500Z","summary":"Pravega: Storage for data streams","location":"@home","x-alt-desc":"<strong>\\nFlavio Junqueira\\n</strong>\\n<p>\\nThere is no shortage of use cases with elements that continuously generate data: end users posting updates and shopping online\\; sensors that periodically emit samples\\; drones that continuously produce aerial video streams\\; connected cars that generate a combination of videos\\, images\\, and telemetry\\; and server fleets that generate an abundance of telemetry data. One common aspect shared by several of these cases is that the sources of data are machines\\, and at scale\\, machines can generate data at extremely high volumes. Machine-generated data creates an important challenge for analytics systems to ingest\\, store and process such high-volumes of machine-generated data in an efficient and effective manner. Pravega is a software system developed from the ground up to enable applications to ingest and store high-volumes of continuously generated data. Pravega exposes the stream as a core storage primitive\\, which enables applications continuously generating data to ingest and store such data permanently. Applications that consume stream data from Pravega are able to access the data through the same API\\, independent of whether it is tailing the stream\\, reprocessing the stream\\, or processing historical data. Pravega has some unique features such as the ability of storing an unbounded amount of data per stream\\, while appending transactionally and scaling according to workload variations. It uses an underlying segment abstraction not only to implement such features\\, but advanced ones to support stream applications such as state synchronization and key-value tables. In this presentation\\, we overview Pravega\\, including its main features and architecture. We show how to use Pravega when building streaming data pipelines along with stream processors such as Apache Flink. We have implemented Pravega connectors for Flink that enable end-to-end exactly-once semantics for data pipelines using Pravega checkpoints and transactions. Pravega is an open-source project\\, licensed under the Apache License Version 2.0\\, and hosted on GitHub (https://github.com/pravega/pravega).\\n</p>\\n\\n<p><em>\\nFlavio Junqueira is a Senior Distinguished Engineer at Dell. He holds a PhD in computer science from the University of California\\, San Diego\\, and he is interested in various aspects of distributed systems\\, including distributed algorithms\\, concurrency\\, and scalability. His recent work at Dell focuses on stream analytics\\, and specifically\\, on the development of a novel storage system for streams called Pravega. Before Dell\\, Flavio held an engineering position with Confluent and research positions with Yahoo! Research and Microsoft Research. Flavio has co-authored a number of scientific publications (over 4\\,000 citations according to Google Scholar) and an OReilly ZooKeeper book on Apache ZooKeeper. Flavio is an Apache Member and has contributed to projects hosted by the ASF\\, including Apache ZooKeeper (as PMC and committer)\\, Apache BookKeeper (as PMC and committer)\\, and Apache Kafka.\\n</em></p>","categories":"Streaming","url":"https://apachecon.com/acah2020/tracks/streaming.html#W1815"},{"uid":"acah2020-streaming-W1855@apachecon.com","sequence":"1","dtstamp":"20200812T151913Z","dtstart":"20200930T185500Z","dtend":"20200930T193500Z","summary":"Story of moving our 4Trillion Event Log Pipeline from Batch to Streaming","location":"@home","x-alt-desc":"<strong>\\nLohit VijayaRenu\\, Zhenzhao Wang\\, Praveen Killamsetti\\n</strong>\\n<p>\\nTwitter's LogPipeline handle more than 4Trillion events per day. This complex pipeline has evolved over the years to support Twitter's scale of data. This pipeline is designed to be resilient\\, support high throughput and use resources efficiently. Because of its legacy architecture\\, it was still batch pipeline at scale. For some time\\, our team has been redesigning this to support streaming use cases and have done significant architecture changes for this pipeline In this talk we deep dive into our old architecture\\, highlight pros and cons of that and describe how we are making changes for it to be more streaming friendly. We talk about various open source projects such as Apache Hadoop\\, Apache Flume\\, Apache Tez\\, Apache Beam and cloud technologies which tie together to form our large scale event LogPipeline.\\n</p>\\n\\n<p><em>\\nLohit VijayaRenu:<br />\\nLohit is part of DataPlatform team at Twitter. He concentrates on projects around storage\\, compute and log pipeline for Twitter scale both on premise and cloud. He has worked at several startups before joining Twitter. He has a Masters degree in Computer Science from Stony Brook University.<br />\\nZhenzhao Wang:<br />\\nZhenzhao works at Twitter as part of Hadoop and Log Management team. He is currently concentrating on Twitter Log Ingestion Pipeline which scales to handle trillions of events per day. Previously he was a member of DFS(Pangu) team in Alibaba Cloud where he focused on feature for random file access file in Pangu used as storage for Virtual Machines. He has Bachelor's degree from Nankai University and Master's degree from Tsinghua University.<br />\\nPraveen Killamsetti<br />\\nPraveen works at Twitter as part of the DataPlatform organization. In his current role\\, he is working on scaling the log ingestion pipeline to trillions of events in the streaming model and building a data set lifecycle management system for analytical data sets. He has a master degree in computer science from IIT Madras.  Before joining Twitter\\, Praveen worked on building distributed storage systems at Nimble Storage\\, NetApp and built various products including Synchronous Replication across multiple data centers with automatic failover\\, Write Optimized KV stores\\, Dedupe and Compression stack\\, Efficient Cloning features\\, Archiving Storage Snapshots to S3 efficiently etc.\\n</em></p>","categories":"Streaming","url":"https://apachecon.com/acah2020/tracks/streaming.html#W1855"},{"uid":"acah2020-streaming-R1615@apachecon.com","sequence":"2","dtstamp":"20200812T151913Z","dtstart":"20201001T161500Z","dtend":"20201001T165500Z","summary":"Google Cloud Pub/Sub vs Apache Kafka for streaming solution at scale","location":"@home","x-alt-desc":"<strong>\\nPrateek Srivastava\\n</strong>\\n<p>\\nEvaluation of various technologies to support High speed\\, Highly scalable REST API to ingest high volume of Analytics payloads from User browsers distributed across the globe. Furthermore\\, we will discuss tech stack choices\\, performance benchmarks\\, costing and best practices for implementing such big data streaming solutions in Google Cloud.\\n</p>\\n\\n<p><em>\\nPrateek Srivastava is Technical Architect at Sigmoid. We help organisations realize the power of open source to manage big data and leverage AI/ML tech to derive actionable insights.\\nHe has more than 13 years of experience in Big data\\, Cloud and Service Oriented architecture and has helped build and sustain several end to end data infrastructures for customers around the world.\\n</em></p>","categories":"Streaming","url":"https://apachecon.com/acah2020/tracks/streaming.html#R1615"},{"uid":"acah2020-streaming-R1655@apachecon.com","sequence":"0","dtstamp":"20200812T151913Z","dtstart":"20201001T165500Z","dtend":"20201001T173500Z","summary":"Building your First Connector for Kafka Connect","location":"@home","x-alt-desc":"<strong>\\nRicardo Ferreira\\n</strong>\\n<p>\\nApache Kafka is rapidly becoming the de-facto standard for distributed streaming architectures\\, and as its adoption grows the need to leverage existing data also grows. When developers need to handle certain technologies that happen to not have an connector available\\; they have no other choice other than write their own. But that can be quite challenging\\, even for experienced developers. This talk will explain in details what it takes to develop a connector\\, how the Kafka Connect framework works\\, and what are the common pitfalls that you should avoid. The code of an existing connector will be used to explain how the implementation should look like so you can develop more confidence when building your own.\\n</p>\\n\\n<p><em>\\nRicardo is a Developer Advocate at Confluent\\, the company founded by the original co-creators of Apache Kafka. He has over 20 years of experience where he specializes in streaming data architectures\\, big data\\, cloud\\, and serverless. Prior to Confluent\\, he worked for other vendors\\, such as Oracle\\, Red Hat\\, and IONA Technologies\\, as well as several consulting firms. When not working\\, he enjoys grilling steaks in his backyard with his family and friends\\, where he gets the chance to talk about anything that is not IT related. Currently\\, he lives in Raleigh\\, North Carolina\\, with his wife and son. Follow Ricardo on Twitter: @riferrei\\n</em></p>","categories":"Streaming","url":"https://apachecon.com/acah2020/tracks/streaming.html#R1655"},{"uid":"acah2020-streaming-R1735@apachecon.com","sequence":"0","dtstamp":"20200812T151913Z","dtstart":"20201001T173500Z","dtend":"20201001T181500Z","summary":"Understanding Data Streaming and Analytics with Apache Kafka","location":"@home","x-alt-desc":"<strong>\\nRicardo Ferreira\\n</strong>\\n<p>\\nThe use of distributed streaming platforms is becoming increasingly popular among developers\\, but have you ever wonder what exactly this is? Part Pub/Sub messaging system\\, partly distributed storage\\, partly event processing engine\\, the usage of this type of technology brings a whole new perspective on how developers capture\\, store\\, and process events. This talk will explain what distributed streaming platforms are and how it can be a game changer for modern data architectures. It will be discussed the road in IT that led to the need of this type of plataform\\, the current state of Apache Kafka\\, as well as scenarios where this technology can be implemented.\\n</p>\\n\\n<p><em>\\nRicardo is a Developer Advocate at Confluent\\, the company founded by the original co-creators of Apache Kafka. He has over 20 years of experience where he specializes in streaming data architectures\\, big data\\, cloud\\, and serverless. Prior to Confluent\\, he worked for other vendors\\, such as Oracle\\, Red Hat\\, and IONA Technologies\\, as well as several consulting firms. When not working\\, he enjoys grilling steaks in his backyard with his family and friends\\, where he gets the chance to talk about anything that is not IT related. Currently\\, he lives in Raleigh\\, North Carolina\\, with his wife and son. Follow Ricardo on Twitter: @riferrei\\n</em></p>","categories":"Streaming","url":"https://apachecon.com/acah2020/tracks/streaming.html#R1735"},{"uid":"acah2020-streaming-R1855@apachecon.com","sequence":"0","dtstamp":"20200812T151913Z","dtstart":"20201001T185500Z","dtend":"20201001T193500Z","summary":"Event Streaming and the Data Communication Layer","location":"@home","x-alt-desc":"<strong>\\nAdam Bellemare\\n</strong>\\n<p>\\nStreaming technologies unlock decoupled\\, near real-time services at scale. The most important part of any streaming platform is the event-broker (eg. Apache Kafka or Pulsar) as it plays the role of the Data Communication Layer (DCL). Many organizations fail to grasp the importance of the DCL and often relegate it to the role of a simple asynchronous message queue\\, leaving their key business domain events locked away in monolithic data stores. This is one of the many pitfalls that will be covered in this presentation\\, along with strategies and tipss for avoiding them. A well constructed DCL decouples both the ownership and production of data from the downstream services that require access to it. Access to clean\\, reliable\\, structured\\, and sorted data streams enables extremely powerful event-driven patterns. Data becomes much easier to access and no longer relies upon the producer's implementation to serve disparate business requirements. Teams and products can organize much more clearly along business bounded contexts\\, and modular\\, disposable\\, and compositional services become extremely easy to build and test. This presentation covers the best practices\\, responsibilities of the various actors\\, recommendations about specific technological implementations\\, and both the organizational changes required and those that will occur as a result of a reliable DCL implementation.\\n</p>\\n\\n<p><em>\\nAdam Bellemare is the author of Building Event-Driven Microservices (O'Reilly\\, 2020). He has been working on event-driven architectures since 2010. His major accomplishments in this time include building an event-driven processing platform at BlackBerry\\, driving the migration to event-driven microservices at Flipp\\, and most recently\\, starting a new role to improve event-driven architectures at Shopify. He has contributed to both Apache Avro and Apache Kafka and is a keen supporter of the open-source community.\\n</em></p>","categories":"Streaming","url":"https://apachecon.com/acah2020/tracks/streaming.html#R1855"},{"uid":"acah2020-tomcat-T1615@apachecon.com","sequence":"88","dtstamp":"20200828T190238Z","dtstart":"20200929T161500Z","dtend":"20200929T165500Z","summary":"State of the Cat","location":"@home","x-alt-desc":"<strong> Mark Thomas </strong> <p> A review of the past year or so for Apache Tomcat and a look forward to what is expected in the coming 12 months.  </p> <p><em> I have been an Apache Tomcat committer since November 2003. I initially worked on Tomcat in my free time but since August 2008 I have been employed by SpringSource (now part of VMware) to work on Apache Tomcat. I spend most of my time working on Tomcat but I also work on tc Server\\, VMware's Servlet & JSP container based on Apache Tomcat. I am the release manager Apache Tomcat 8.5\\, 9.0 and 10.0 where I try to release a new version every month or so. I am currently focused on Tomcat 10 development which supports Jakarta EE 9. I am a committer for Eclipse Servlet\\, Server Pages\\, Expression Language and WebSocket. Elsewhere at the ASF\\, I am a member of the ASF security and infrastructure teams and I am also on the Commons PMC where I focus on Commons Pool and DBCP. I am a member of the ASF and served as a Director from 2016 to 2019. I have held the position of VP\\, Brand Management since February 2018. </em></p> \\n\\n<!-- Lost in the Docs -->","categories":"Tomcat","url":"https://apachecon.com/acah2020/tracks/tomcat.html#T1615"},{"uid":"acah2020-tomcat-T1655@apachecon.com","sequence":"88","dtstamp":"20200828T190238Z","dtstart":"20200929T165500Z","dtend":"20200929T173500Z","summary":"Lost in the Docs","location":"@home","x-alt-desc":"<strong> Felix Schumacher </strong> <p> Tomcat has a lot of documentation and a lot of features. We will look at some of the features that are overlooked or not found but could be handy.  </p> <p><em> Felix started to use open source at university while trying to compile Fortran for his math studies. He stayed with Linux but left Fortran for other languages like Java\\, Perl and Python. He continued in working in IT -- building thin clients and management systems for DHCP and DNS. With time his interests faded into looking after a horde of Tomcat servers and felt responsible to make them run faster and more stable. He contributes to both Apache Tomcat and JMeter projects adapting them to his own needs and helping others for fun. He gladly became a committer on both projects and is a member of the Apache Software Foundation. </em></p> \\n\\n<!-- Deploying a Production Instance -->","categories":"Tomcat","url":"https://apachecon.com/acah2020/tracks/tomcat.html#T1655"},{"uid":"acah2020-tomcat-T1735@apachecon.com","sequence":"87","dtstamp":"20200828T190238Z","dtstart":"20200929T173500Z","dtend":"20200929T181500Z","summary":"Deploying a Production Instance","location":"@home","x-alt-desc":"<strong> Andrew Carr </strong> <p> Deploying Tomcat in a local development environment is a task that most developers are familiar with. Setting up a consistent\\, reliable\\, dependable and hardened Tomcat instance in a production environment is not as difficult as most would assume. Here we will discuss important aspects of a Production deployment\\, along with the configuration of other environments\\, like Staging\\, Integration\\, and Quality Assurance. There are pitfalls to avoid\\, common tasks to accomplish and automation that can assist in these tasks.  </p> <p><em> About: Andrew has been working in the I.T. industry since 1996 developing hardware\\, network and software solutions to suit business needs and requirements. Leveraging open source software\\, he has implemented enterprise software solutions for a number of large corporations while delivering training to staff\\, both entry-level and expert. Currently\\, Andrew works as a Consulting Enterprise Architect at Perforce.  </em></p> \\n<!-- HTTP/2\\, HTTP/3\\, and SSL/TLS State of the Art in our Servers (httpd\\, Traffic Server\\, and Tomcat) -->","categories":"Tomcat","url":"https://apachecon.com/acah2020/tracks/tomcat.html#T1735"},{"uid":"acah2020-tomcat-T1815@apachecon.com","sequence":"58","dtstamp":"20200828T190238Z","dtstart":"20200929T181500Z","dtend":"20200929T185500Z","summary":"HTTP/2\\, HTTP/3\\, and SSL/TLS State of the Art in our Servers (httpd\\, Traffic Server\\, and Tomcat)","location":"@home","x-alt-desc":"<strong> Jean-Frederic Clere </strong> <p> A new protocol is getting ready HTTP/3 we will look to where we are with it in our serves. The \\\"old\\\" HTTP/2 protocol and the corresponding TLS/SSL are common to Traffic Server\\, HTTP Server and Tomcat. The presentation will shortly explain the new protocol and the ALPN extensions and look to the state of the those in our 3 servers and show the common parts and the specifics of each servers. A demo configuration of each server will be run. </p> <p><em> Jean-Frederic has spent more than 20 years writing client/server software. His knowledges range from Cobol to Java\\, BS2000 to Linux and /390 to i386 but with preference to the later \\;). He is committer in Httpd and Tomcat and he likes complex projects where different languages and machines are involved. Borne in France\\, Jean-Frederic lived in Barcelona (Spain) for 14 years. Since May 2006 he lives in Neuchatel (Switzerland) where he works for RedHat in the JBoss division on Tomcat\\, httpd and cloud/cluster related topics. </em></p> \\n<!-- Split your Tomcat Installation for Easier Upgrades -->","categories":"Tomcat","url":"https://apachecon.com/acah2020/tracks/tomcat.html#T1815"},{"uid":"acah2020-tomcat-T1855@apachecon.com","sequence":"1","dtstamp":"20200828T190238Z","dtstart":"20200929T185500Z","dtend":"20200929T193500Z","summary":"Split your Tomcat Installation for Easier Upgrades","location":"@home","x-alt-desc":"<strong> Christopher Schultz </strong> <p> Upgrading Apache Tomcat can seem like a risky process if your team isn't well-versed in the process. Splitting your Tomcat installation into stock install + customized deployment can make the process much less risky and even allow you to quickly downgrade if necessary. We'll explore how to split your Tomcat installation to get you more comfortable upgrading Tomcat\\, reduce deployment times\\, and improve your security. </p> <p><em> Christopher Schultz is the CTO of Total Child Health\\, Inc. where he leads a small team of engineers to build server-side healthcare-related software in Java. Chris is an ASF Member active in the Apache Tomcat and Velocity communities as well as a committer on both projects\\, and Tomcat PMC and security team member. He has attended and spoken at several previous ApacheCon events and helped to organize an Apache BarCamp in the Washington\\, DC area. </em></p> \\n\\n\\n<!-- Tomcat: New and Upcoming -->","categories":"Tomcat","url":"https://apachecon.com/acah2020/tracks/tomcat.html#T1855"},{"uid":"acah2020-tomcat-T1935@apachecon.com","sequence":"1","dtstamp":"20200828T190238Z","dtstart":"20200929T193500Z","dtend":"20200929T201500Z","summary":"Tomcat: New and Upcoming","location":"@home","x-alt-desc":"<strong> Rmy Mucherat </strong> <p> This session presents the new features that were recently introduced in Apache Tomcat with examples and ideas to take advantage of them\\, as well as upcoming development plans. </p> <p><em> Remy is a long time Tomcat committer and ASF member. Lately he's been focusing on various areas such as IO\\, ahead of time compilation and optimizations\\, and various other additions to Tomcat. </em></p> \\n<!-- Reverse-Proxying with nginx -->","categories":"Tomcat","url":"https://apachecon.com/acah2020/tracks/tomcat.html#T1935"},{"uid":"acah2020-tomcat-W1615@apachecon.com","sequence":"0","dtstamp":"20200914T145203Z","dtstart":"20200930T161500Z","dtend":"20200930T165500Z","summary":"Reverse-Proxying with nginx","location":"@home","x-alt-desc":"<strong> Igal Sapir </strong> <p> \\\"nginx\\, pronounced \\\"Engine X\\\"\\, is a high performance Web Server\\, Load Balancer\\, and Reverse Proxy\\, which has been released as free and open source (FreeBSD license) since 2004. I will show how to configure nginx to serve as a reverse proxy and load balancer in front of Apache Tomcat backend servers. </p> <p><em> Igal has been fascinated with software ever since he got his first computer at the age of 12. Based in Los Angeles\\, Igal is an Open Source advocate\\, and in the past two decades he has been developing web applications and helping organizations around the globe to solve issues of scalability\\, security\\, and performance. </em></p> \\n<!-- Tomcat: From a Cluster to a Cloud -->","categories":"Tomcat","url":"https://apachecon.com/acah2020/tracks/tomcat.html#W1615"},{"uid":"acah2020-tomcat-W1655@apachecon.com","sequence":"0","dtstamp":"20200914T145203Z","dtstart":"20200930T165500Z","dtend":"20200930T173500Z","summary":"Tomcat: From a Cluster to a Cloud","location":"@home","x-alt-desc":"<strong> Jean-Frederic Clere </strong> <p> Using Tomcat in a cluster and in a cloud. We start by looking how to configure tomcat to get a cluster and then explore the problems and solutions to have distributed applications running in a cloud. Most cloud providers now have a Kubernetes API. We will look to what we have to add to Tomcat to have a decent cloud support for monitoring\\, tracing and operating on the cloud. We will show how to use all the pieces. A demo of a cluster will be prepared and run during the presentation and the corresponding application will be moved to a Kubernetes cloud. </p> <p><em> Jean-Frederic has spent more than 20 years writing client/server software. His knowledges range from Cobol to Java\\, BS2000 to Linux and /390 to i386 but with preference to the later \\;). He is committer in Httpd and Tomcat and he likes complex projects where different languages and machines are involved. Borne in France\\, Jean-Frederic lived in Barcelona (Spain) for 14 years. Since May 2006 he lives in Neuchatel (Switzerland) where he works for RedHat in the JBoss division on Tomcat\\, httpd and cloud/cluster related topics. </em></p> \\n<!-- Migrating from AJP to HTTP: It's About Time -->","categories":"Tomcat","url":"https://apachecon.com/acah2020/tracks/tomcat.html#W1655"},{"uid":"acah2020-tomcat-W1735@apachecon.com","sequence":"0","dtstamp":"20200914T145203Z","dtstart":"20200930T173500Z","dtend":"20200930T181500Z","summary":"Migrating from AJP to HTTP: It's About Time","location":"@home","x-alt-desc":"<strong> Christopher Schultz </strong> <p> The Apache JServ Protocol was developed in 1997 as a proxying protocol between Apache httpd and Apache Jserv. At the time\\, mod_proxy was not an option for connecting to Apache Jserv\\, so Apache mod_jk was developed and generations of developers have used it to great effect. But AJP has some serious flaws\\, including lack of encryption and the inability to upgrade connections to use Websockets. In the intervening years\\, mod_proxy has become much more fully-featured and can solve all the problems with using AJP. We will cover all of the reasons AJP should be abandoned\\, all the nice things mod_jk does for you\\, and how to achieve the same results using mod_proxy with the http and wstunnel child-mods. </p> <p><em> Christopher Schultz is the CTO of Total Child Health\\, Inc. where he leads a small team of engineers to build server-side healthcare-related software in Java. Chris is an ASF Member active in the Apache Tomcat and Velocity communities as well as a committer on both projects\\, and Tomcat PMC and security team member. He has attended and spoken at several previous ApacheCon events and helped to organize an Apache BarCamp in the Washington\\, DC area. </em></p> \\n<!-- Tomcat 10 and Jakarta EE -->","categories":"Tomcat","url":"https://apachecon.com/acah2020/tracks/tomcat.html#W1735"},{"uid":"acah2020-tomcat-W1815@apachecon.com","sequence":"0","dtstamp":"20200914T145203Z","dtstart":"20200930T181500Z","dtend":"20200930T185500Z","summary":"Tomcat 10 and Jakarta EE","location":"@home","x-alt-desc":"<strong> Mark Thomas </strong> <p> The move of Java EE to the Eclipse Foundation and its transformation to Jakarta EE has resulted in some potentially significant changes for end users. The part of this session will look at what the changes are\\, the impact they have for end users and what the Apache Tomcat project is doing to help mitigate those impacts. In the second part of the session\\, the current progress of Tomcat 10 towards Jakarta EE 9 support will be discussed along with the expected timeline for a stable Tomcat 10.0 release. The final part of the session will look at Jakarta EE 10\\, the likely changes and new features and the road map for Jakarta EE 10 support in Apache Tomcat. </p> <p><em> I have been an Apache Tomcat committer since November 2003. I initially worked on Tomcat in my free time but since August 2008 I have been employed by SpringSource (now part of VMware) to work on Apache Tomcat. I spend most of my time working on Tomcat but I also work on tc Server\\, VMware's Servlet & JSP container based on Apache Tomcat. I am the release manager Apache Tomcat 8.5\\, 9.0 and 10.0 where I try to release a new version every month or so. I am currently focused on Tomcat 10 development which supports Jakarta EE 9. I am a committer for Eclipse Servlet\\, Server Pages\\, Expression Language and WebSocket. Elsewhere at the ASF\\, I am a member of the ASF security and infrastructure teams and I am also on the Commons PMC where I focus on Commons Pool and DBCP. I am a member of the ASF and served as a Director from 2016 to 2019. I have held the position of VP\\, Brand Management since February 2018. </em></p> \\n<!-- Getting Started Hacking Tomcat -->","categories":"Tomcat","url":"https://apachecon.com/acah2020/tracks/tomcat.html#W1815"},{"uid":"acah2020-tomcat-R1615@apachecon.com","sequence":"0","dtstamp":"20200914T145203Z","dtstart":"20201001T161500Z","dtend":"20201001T165500Z","summary":"Getting Started Hacking Tomcat","location":"@home","x-alt-desc":"<strong> Christopher Schultz </strong> <p> Something bugging you in Tomcat? Think you have a great idea for a feature or improvement? Documentation needs improvement? Getting started hacking on Tomcat's code or documentation is easy! We'll cover how to get a copy of Tomcat's source code\\, build it locally\\, communicate with the Tomcat committers\\, and submit a patch or pull-request. </p> <p><em> Christopher Schultz is the CTO of Total Child Health\\, Inc. where he leads a small team of engineers to build server-side healthcare-related software in Java. Chris is an ASF Member active in the Apache Tomcat and Velocity communities as well as a committer on both projects\\, and Tomcat PMC and security team member. He has attended and spoken at several previous ApacheCon events and helped to organize an Apache BarCamp in the Washington\\, DC area. </em></p> \\n<!-- Apache Tomcat and Spring Boot -->","categories":"Tomcat","url":"https://apachecon.com/acah2020/tracks/tomcat.html#R1615"},{"uid":"acah2020-tomcat-R1655@apachecon.com","sequence":"0","dtstamp":"20200914T145203Z","dtstart":"20201001T165500Z","dtend":"20201001T173500Z","summary":"Apache Tomcat and Spring Boot","location":"@home","x-alt-desc":"<strong> Andrew Carr </strong> <p> Discover how Spring leverages code provided by the Apache Tomcat project allowing developers to quickly prototype and deploy advanced Java web applications. A lot of developers use Spring Boot to bootstrap applications and these applications frequently end up in production. How does Spring use Tomcat to deploy your Java application with minimal effort? How much of the Tomcat code is included in the Spring Boot project? What is the best way to leverage features offered by both software packages? Dive deep into the workings of Tomcat and Spring\\, exploring their interaction.  </p> <p><em> About: Andrew has been working in the I.T. industry since 1996 developing hardware\\, network and software solutions to suit business needs and requirements. Leveraging open source software\\, he has implemented enterprise software solutions for a number of large corporations while delivering training to staff\\, both entry-level and expert. Currently\\, Andrew works as a Consulting Enterprise Architect at Perforce.  </em></p> \\n<!-- Openly Handling Security Vulnerabilities (Q&A/Panel) -->","categories":"Tomcat","url":"https://apachecon.com/acah2020/tracks/tomcat.html#R1655"},{"uid":"acah2020-tomcat-R1735@apachecon.com","sequence":"0","dtstamp":"20200914T145203Z","dtstart":"20201001T173500Z","dtend":"20201001T181500Z","summary":"Openly Handling Security Vulnerabilities (Q&A/Panel)","location":"@home","x-alt-desc":"<strong> Mark Thomas\\, Christopher Schultz\\, Coty Sutherland </strong> <p> Apache Tomcat is one of the most popular Java application servers in the world. The Apache Tomcat Security Team handles many vulnerability reports via its private \\\"security\\\" list each year where potential and actual vulnerabilities are discussed in a decidedly non-open way to help keep the public safe. At some point\\, the software needs to change to address any security shortcomings in the product and all of those changes are available immediately\\, and publicly\\, to the whole world. In this Q&A/panel discussion\\, members of the Apache Tomcat Security Team will discuss how the Apache Tomcat Security Team handles those vulnerability reports and manages patches in an open and (mostly) transparent way. Audience participation is highly encouraged\\, so come prepared with any questions you may have about our processes. </p> <p><em>  </em></p>","categories":"Tomcat","url":"https://apachecon.com/acah2020/tracks/tomcat.html#R1735"}]}],"success":true}